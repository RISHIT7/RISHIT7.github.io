[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Quarto Blog",
    "section": "",
    "text": "UCI-HAR Classification\n\n\n\n\n\n\nML\n\n\nTime Series\n\n\n\n\n\n\n\n\n\nMar 7, 2025\n\n\nRishit Jakharia\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rishit Jakharia",
    "section": "",
    "text": "I am currently a 3rd year student pursuing my undergraduate degree in Computer Science at IIT Delhi. A deep-seated passion for Machine Learning and Deep Learning fuels my academic journey.\nBeyond the practical aspects, I am drawn to the mathematical foundations that underpin computer science, finding beauty and inspiration in the elegant logic and structures that form the backbone of our digital world.\nAs an enthusiastic learner, I am not just interested in keeping up with the latest advancements but actively engaging with them. I thrive on challenges and view each one as an opportunity to expand my knowledge and skills."
  },
  {
    "objectID": "posts/uci-har-pytorch.html",
    "href": "posts/uci-har-pytorch.html",
    "title": "UCI-HAR Classification",
    "section": "",
    "text": "::: {#cell-1 .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2024-12-13T05:50:55.402561Z”,“iopub.status.busy”:“2024-12-13T05:50:55.401962Z”,“iopub.status.idle”:“2024-12-13T05:50:55.407609Z”,“shell.execute_reply”:“2024-12-13T05:50:55.406519Z”,“shell.execute_reply.started”:“2024-12-13T05:50:55.402528Z”}’ trusted=‘true’ execution_count=3}\n:::\nimport random\nimport os\n\nseed_value = 42\n\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\n# torch.use_deterministic_algorithms(True)\n\n'cuda'"
  },
  {
    "objectID": "posts/uci-har-pytorch.html#setting-up-the-train-and-test-loaders",
    "href": "posts/uci-har-pytorch.html#setting-up-the-train-and-test-loaders",
    "title": "UCI-HAR Classification",
    "section": "Setting Up the train and test loaders",
    "text": "Setting Up the train and test loaders\n\nclass HARDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n\ntrain_dataset = HARDataset(trainX, trainy)\ntest_dataset = HARDataset(testX, testy)\n\nBATCH = 256\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH, shuffle=False)"
  },
  {
    "objectID": "posts/uci-har-pytorch.html#preprocessing",
    "href": "posts/uci-har-pytorch.html#preprocessing",
    "title": "UCI-HAR Classification",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nmean = torch.zeros(128, 9)  # Shape (128, 9)\nsum_sq = torch.zeros(128, 9)  # Shape (128, 9)\ntotal_elements = 0\n\nfor batch, _ in train_loader:\n    batch_size = batch.size(0)\n    \n    mean += batch.sum(dim=0)\n    sum_sq += (batch ** 2).sum(dim=0)\n    total_elements += batch_size\n\nmean /= total_elements  # Shape (128, 9)\nstd = torch.sqrt(sum_sq / total_elements - mean ** 2)  # Shape (128, 9)\n\n# print(\"Mean:\", mean)\n# print(\"Standard Deviation:\", std)\n\n\nclass Normalize:\n    def __init__(self, mean, std, device=None):\n        self.mean = mean.to(device) if device else mean\n        self.std = std.to(device) if device else std\n    \n    def __call__(self, sample):\n        if not isinstance(sample, torch.Tensor):\n            sample = torch.tensor(sample, dtype=torch.float32)\n        \n        normalized_sample = (sample - self.mean) / self.std\n        return normalized_sample\n\nclass AddNoise:\n    def __init__(self, noise_level=0.01):\n        self.noise_level = noise_level\n\n    def __call__(self, sample):\n        noise = torch.randn_like(sample) * self.noise_level\n        return sample + noise\n\n\nclass DatasetWithAugmentation(Dataset):\n    def __init__(self, data, target, transforms=None):\n        self.data = data\n        self.target = target\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        target = self.target[idx]\n\n        if self.transforms:\n            for transform in self.transforms:\n                sample = transform(sample)\n\n        return sample, target\n\nnormalize_transform = Normalize(torch.Tensor(mean), torch.Tensor(std))\nadd_noise_transform = AddNoise(noise_level=0.02)\n\ntransforms = [normalize_transform, add_noise_transform]\n# transforms = [normalize_transform]\n\ntrain_dataset = DatasetWithAugmentation(trainX, trainy, transforms=transforms)\ntest_dataset = DatasetWithAugmentation(testX, testy, transforms=transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH, shuffle=False)"
  },
  {
    "objectID": "posts/uci-har-pytorch.html#loss-function",
    "href": "posts/uci-har-pytorch.html#loss-function",
    "title": "UCI-HAR Classification",
    "section": "Loss Function",
    "text": "Loss Function\n\ndef kl_divergence_loss(preds, targets):\n    targets = targets.float()\n    \n    preds = F.softmax(preds, dim=-1)\n    targets = F.softmax(targets, dim=-1)\n    loss = F.kl_div(preds.log(), targets, reduction='batchmean')\n    return loss\n\ndef mse_loss(preds, targets):\n    # Ensure that the predictions and targets are one-hot encoded\n    preds = F.softmax(preds, dim=-1)\n    return F.mse_loss(preds, targets)\n\n# Focal Loss\ndef focal_loss(preds, targets, alpha=0.25, gamma=2.0):\n    \"\"\"\n    Focal Loss for multi-class classification.\n    \n    Arguments:\n    preds -- the raw logits from the model (shape: [batch_size, num_classes])\n    targets -- the true labels (shape: [batch_size])\n    alpha -- balancing factor for class imbalances (default is 0.25)\n    gamma -- focusing parameter (default is 2.0)\n    \n    Returns:\n    loss -- the computed focal loss\n    \"\"\"\n    # Apply softmax to get probabilities\n    preds = F.softmax(preds, dim=-1)\n    \n    # Convert targets to one-hot encoding\n    targets_one_hot = targets\n    \n    # Cross-entropy loss\n    ce_loss = F.cross_entropy(preds, targets, reduction='none')\n    \n    # Get the predicted probability for the correct class\n    pt = torch.exp(-ce_loss)\n    \n    # Compute focal loss\n    focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n    \n    return focal_loss.mean()\n\n# criterion = nn.CrossEntropyLoss() # -&gt; 96.89% 93.55%\n# criterion = kl_divergence_loss # -&gt; 99.09% 94.81%\n# criterion = mse_loss # -&gt; 96.27% 92.94%\n# criterion = nn.CrossEntropyLoss(label_smoothing=0.1) # -&gt; 97.43% 94.06%\n# criterion = focal_loss # -&gt; 97.27% 93.32%"
  },
  {
    "objectID": "posts/uci-har-pytorch.html#optimizer-and-learning-rate",
    "href": "posts/uci-har-pytorch.html#optimizer-and-learning-rate",
    "title": "UCI-HAR Classification",
    "section": "Optimizer and Learning Rate",
    "text": "Optimizer and Learning Rate\n\ndef warmup_lr(epoch, step_size, warmup_epochs=5, gamma=0.1):\n    if epoch &lt; warmup_epochs:\n        return (epoch + 1) / warmup_epochs\n    \n    else:\n        epoch_since_warmup = epoch - warmup_epochs\n        return gamma ** (epoch_since_warmup // step_size)\n\nWARMUP_EPOCHS = 14\nGAMMA = 0.1\nSTEP_SIZE = 20"
  },
  {
    "objectID": "posts/uci-har-pytorch.html#plotting-train-and-test-acc-and-loss",
    "href": "posts/uci-har-pytorch.html#plotting-train-and-test-acc-and-loss",
    "title": "UCI-HAR Classification",
    "section": "Plotting Train and Test Acc and Loss",
    "text": "Plotting Train and Test Acc and Loss\n\n# Plot history\ndef plot_history(history):\n    epochs = range(1, len(history['train_loss']) + 1)\n\n    plt.figure(figsize=(12, 4))\n\n    # Loss plot\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, history['train_loss'], label='Train Loss')\n    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Loss History')\n\n    # Accuracy plot\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, history['train_acc'], label='Train Accuracy')\n    plt.plot(epochs, history['val_acc'], label='Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.title('Accuracy History')\n\n    plt.show()"
  },
  {
    "objectID": "posts/uci-har-pytorch.html#confusion-matrix-function",
    "href": "posts/uci-har-pytorch.html#confusion-matrix-function",
    "title": "UCI-HAR Classification",
    "section": "Confusion Matrix Function",
    "text": "Confusion Matrix Function\n\ndef plot_confusion_matrix(model, val_loader, class_names, device=\"cuda\"):\n    \"\"\"\n    Evaluates the model and plots a confusion matrix with a custom color map.\n\n    Arguments:\n    - model: Trained PyTorch model.\n    - val_loader: DataLoader for validation dataset.\n    - class_names: List of class names for the confusion matrix.\n    - device: 'cuda' or 'cpu'.\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():  # Disable gradient computation\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            preds = torch.argmax(outputs, dim=1)  # Predicted classes\n            \n            # If labels are one-hot encoded, convert them to class indices\n            if len(labels.shape) &gt; 1 and labels.size(1) &gt; 1:\n                labels = torch.argmax(labels, dim=1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Compute confusion matrix\n    cm = confusion_matrix(all_labels, all_preds, labels=np.arange(len(class_names)))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n\n    # Plot the confusion matrix with customization\n    fig, ax = plt.subplots(figsize=(10, 8))  # Set larger figure size\n    disp.plot(cmap=\"YlGnBu\", ax=ax, colorbar=True)  # Use \"cividis\" colormap and add colorbar\n    plt.title(\"Confusion Matrix\", fontsize=16)\n    plt.xticks(fontsize=12, rotation=45)\n    plt.yticks(fontsize=12)\n    plt.show()"
  },
  {
    "objectID": "posts/uci-har-pytorch.html#training-function",
    "href": "posts/uci-har-pytorch.html#training-function",
    "title": "UCI-HAR Classification",
    "section": "Training Function",
    "text": "Training Function\n\ndef train_model(model, train_loader, val_loader, criterion=None, optimizer=None, scheduler=None, epochs=20, device='cpu', early_stopping=False, save_path='best_model.pth'):\n    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n\n    patience = 20\n    min_delta = 1e-3\n    best_loss = np.inf\n    best_acc = -np.inf\n    patience_counter = 0\n    best_model_state = None  # Store the best model in memory\n\n    if criterion is None:\n        criterion = F.mse_loss\n    _criterion = criterion\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss, train_correct = 0.0, 0\n        \n        # Training phase\n        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\", unit=\"batch\") as tepoch:\n            for inputs, targets in tepoch:\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n                \n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = _criterion(outputs, targets)\n                loss.backward()\n                optimizer.step()\n                \n                train_loss += loss.item() * inputs.size(0)\n                train_correct += (outputs.argmax(1) == targets.argmax(1)).sum().item()\n\n                tepoch.set_postfix(loss=loss.item())\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_loss /= len(train_loader.dataset)\n        train_acc = train_correct / len(train_loader.dataset)\n\n        # Validation phase\n        model.eval()\n        val_loss, val_correct = 0.0, 0\n        with torch.no_grad():\n            with tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\", unit=\"batch\") as vepoch:\n                for inputs, targets in vepoch:\n                    inputs = inputs.to(device)\n                    targets = targets.to(device)\n                    \n                    outputs = model(inputs)\n                    loss = criterion(outputs, targets)\n                    val_loss += loss.item() * inputs.size(0)\n                    val_correct += (outputs.argmax(1) == targets.argmax(1)).sum().item()\n\n                    vepoch.set_postfix(loss=loss.item())\n\n        val_loss /= len(val_loader.dataset)\n        val_acc = val_correct / len(val_loader.dataset)\n\n        # Save history\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_acc'].append(train_acc)\n        history['val_acc'].append(val_acc)\n\n        print(f\"Epoch {epoch+1}/{epochs} - \"\n              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n        # Update the best model in memory\n        if best_acc &lt; val_acc - min_delta:\n            best_loss = val_loss\n            best_acc = val_acc\n            patience_counter = 0\n            best_model_state = model.state_dict()\n            print(f\"New best model found at epoch {epoch+1} with val_loss: {val_loss:.4f}\")\n        else:\n            patience_counter += 1\n\n        # Early stopping\n        if patience_counter &gt;= patience and early_stopping and epoch &gt; 100:\n            print(\"Early stopping triggered!\")\n            break\n\n    # Save the best model at the end of training\n    if best_model_state is not None:\n        torch.save(best_model_state, save_path)\n        print(f\"Best model saved to {save_path} with val_loss: {best_loss:.4f} and val_acc: {best_acc:.4f}\")\n\n    return history"
  },
  {
    "objectID": "posts/uci-har-pytorch.html#models",
    "href": "posts/uci-har-pytorch.html#models",
    "title": "UCI-HAR Classification",
    "section": "Models",
    "text": "Models\n\nRNN + Attention\n\nclass AdaptiveRNN(nn.Module):\n    def __init__(self, n_timesteps, n_features, n_outputs):\n        super(AdaptiveRNN, self).__init__()\n\n        # Bi-directional LSTM layer\n        self.lstm = nn.LSTM(\n            input_size=n_features,\n            hidden_size=256,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.batch_norm = nn.BatchNorm1d(256 * 2)\n        self.batch_norm1 = nn.BatchNorm1d(128)\n\n        # Attention mechanism\n        self.attention_dense = nn.Linear(256 * 2, 1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(256 * 2, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, n_outputs)\n\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        # LSTM output\n        lstm_out, _ = self.lstm(x)  # lstm_out shape: (batch_size, timesteps, 128*2)\n        \n        batch_size, n_timesteps, n_features = lstm_out.size()\n        lstm_out = lstm_out.reshape(batch_size * n_timesteps, n_features)\n        lstm_out = self.batch_norm(lstm_out)\n        lstm_out = lstm_out.reshape(batch_size, n_timesteps, n_features)\n        \n        # Attention mechanism\n        attention_scores = torch.sigmoid(self.attention_dense(lstm_out))  # (batch_size, timesteps, 1)\n        attention_scores = lstm_out * attention_scores  # Weighted timesteps\n\n        # Global average pooling\n        pooled = attention_scores.mean(dim=1)\n        \n        # Fully connected layers\n        x = F.relu(self.fc1(pooled))\n        x = self.batch_norm1(x)\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n\n        return F.softmax(x, dim=1)\n\n\n# Define the model parameters\nn_timesteps = 32 * 4  # 128 timesteps\nn_features = 9\nn_outputs = 6\n\n# Create the model\nmodel = AdaptiveRNN(n_timesteps=n_timesteps, n_features=n_features, n_outputs=n_outputs).to(device)\nsummary(model)\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nAdaptiveRNN                              --\n├─LSTM: 1-1                              546,816\n├─BatchNorm1d: 1-2                       1,024\n├─BatchNorm1d: 1-3                       256\n├─Linear: 1-4                            513\n├─Linear: 1-5                            65,664\n├─Linear: 1-6                            8,256\n├─Linear: 1-7                            390\n├─Dropout: 1-8                           --\n=================================================================\nTotal params: 622,919\nTrainable params: 622,919\nNon-trainable params: 0\n=================================================================\n\n\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n\n\n# loss_fn = lambda inputs, targets: 0.9*kl_divergence_loss(inputs, targets) + 0.1*nn.CrossEntropyLoss()(inputs.float(), targets.float())\nhistory = train_model(model, train_loader, test_loader, optimizer=optimizer, criterion=kl_divergence_loss, epochs=150, device=device, early_stopping=True)\n\nEpoch 1/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.51batch/s, loss=0.00351] \nEpoch 1/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.06batch/s, loss=-7.78e-8]\n\n\nEpoch 1/150 - Train Loss: 0.0020, Train Acc: 0.9879 - Val Loss: 0.0067, Val Acc: 0.9600\nNew best model found at epoch 1 with val_loss: 0.0067\n\n\nEpoch 2/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.53batch/s, loss=0.00333] \nEpoch 2/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.61batch/s, loss=5.67e-7]\n\n\nEpoch 2/150 - Train Loss: 0.0016, Train Acc: 0.9917 - Val Loss: 0.0069, Val Acc: 0.9617\nNew best model found at epoch 2 with val_loss: 0.0069\n\n\nEpoch 3/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.70batch/s, loss=2.36e-5] \nEpoch 3/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.19batch/s, loss=-5.85e-8]\n\n\nEpoch 3/150 - Train Loss: 0.0014, Train Acc: 0.9921 - Val Loss: 0.0056, Val Acc: 0.9678\nNew best model found at epoch 3 with val_loss: 0.0056\n\n\nEpoch 4/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.92batch/s, loss=0.00196] \nEpoch 4/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.51batch/s, loss=7.7e-7] \n\n\nEpoch 4/150 - Train Loss: 0.0013, Train Acc: 0.9927 - Val Loss: 0.0075, Val Acc: 0.9593\n\n\nEpoch 5/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.81batch/s, loss=0.00115] \nEpoch 5/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.43batch/s, loss=5.62e-8]\n\n\nEpoch 5/150 - Train Loss: 0.0011, Train Acc: 0.9939 - Val Loss: 0.0068, Val Acc: 0.9613\n\n\nEpoch 6/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.65batch/s, loss=0.000549]\nEpoch 6/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.41batch/s, loss=4.15e-5]\n\n\nEpoch 6/150 - Train Loss: 0.0013, Train Acc: 0.9929 - Val Loss: 0.0080, Val Acc: 0.9559\n\n\nEpoch 7/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.51batch/s, loss=0.000857]\nEpoch 7/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.76batch/s, loss=-6.1e-8]\n\n\nEpoch 7/150 - Train Loss: 0.0013, Train Acc: 0.9929 - Val Loss: 0.0072, Val Acc: 0.9596\n\n\nEpoch 8/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.48batch/s, loss=0.000464]\nEpoch 8/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.35batch/s, loss=-3.55e-8]\n\n\nEpoch 8/150 - Train Loss: 0.0016, Train Acc: 0.9908 - Val Loss: 0.0059, Val Acc: 0.9667\n\n\nEpoch 9/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.50batch/s, loss=0.00216] \nEpoch 9/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.94batch/s, loss=-7.62e-8]\n\n\nEpoch 9/150 - Train Loss: 0.0016, Train Acc: 0.9905 - Val Loss: 0.0077, Val Acc: 0.9579\n\n\nEpoch 10/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.65batch/s, loss=0.00201] \nEpoch 10/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.08batch/s, loss=-7.14e-8]\n\n\nEpoch 10/150 - Train Loss: 0.0016, Train Acc: 0.9903 - Val Loss: 0.0068, Val Acc: 0.9627\n\n\nEpoch 11/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.61batch/s, loss=0.0029]  \nEpoch 11/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.43batch/s, loss=-7.94e-8]\n\n\nEpoch 11/150 - Train Loss: 0.0016, Train Acc: 0.9914 - Val Loss: 0.0070, Val Acc: 0.9596\n\n\nEpoch 12/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.60batch/s, loss=0.00197] \nEpoch 12/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.39batch/s, loss=-7.51e-8]\n\n\nEpoch 12/150 - Train Loss: 0.0015, Train Acc: 0.9914 - Val Loss: 0.0073, Val Acc: 0.9576\n\n\nEpoch 13/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.68batch/s, loss=0.00155] \nEpoch 13/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.26batch/s, loss=-5.12e-8]\n\n\nEpoch 13/150 - Train Loss: 0.0015, Train Acc: 0.9909 - Val Loss: 0.0075, Val Acc: 0.9559\n\n\nEpoch 14/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.63batch/s, loss=0.00081] \nEpoch 14/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.66batch/s, loss=-4.35e-8]\n\n\nEpoch 14/150 - Train Loss: 0.0017, Train Acc: 0.9908 - Val Loss: 0.0069, Val Acc: 0.9640\n\n\nEpoch 15/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=0.00115] \nEpoch 15/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.27batch/s, loss=-7.58e-8]\n\n\nEpoch 15/150 - Train Loss: 0.0015, Train Acc: 0.9916 - Val Loss: 0.0069, Val Acc: 0.9603\n\n\nEpoch 16/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.55batch/s, loss=0.00192] \nEpoch 16/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.24batch/s, loss=-6.37e-8]\n\n\nEpoch 16/150 - Train Loss: 0.0011, Train Acc: 0.9931 - Val Loss: 0.0074, Val Acc: 0.9606\n\n\nEpoch 17/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.000373]\nEpoch 17/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.37batch/s, loss=-8.53e-8]\n\n\nEpoch 17/150 - Train Loss: 0.0009, Train Acc: 0.9947 - Val Loss: 0.0076, Val Acc: 0.9589\n\n\nEpoch 18/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.32batch/s, loss=0.000373]\nEpoch 18/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.32batch/s, loss=-7.1e-8]\n\n\nEpoch 18/150 - Train Loss: 0.0006, Train Acc: 0.9966 - Val Loss: 0.0070, Val Acc: 0.9613\n\n\nEpoch 19/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=0.00177] \nEpoch 19/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.62batch/s, loss=-4.96e-8]\n\n\nEpoch 19/150 - Train Loss: 0.0011, Train Acc: 0.9943 - Val Loss: 0.0063, Val Acc: 0.9657\n\n\nEpoch 20/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.61batch/s, loss=0.000316]\nEpoch 20/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.87batch/s, loss=-5.76e-8]\n\n\nEpoch 20/150 - Train Loss: 0.0011, Train Acc: 0.9935 - Val Loss: 0.0074, Val Acc: 0.9586\n\n\nEpoch 21/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.74batch/s, loss=0.00199] \nEpoch 21/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.15batch/s, loss=-4.48e-8]\n\n\nEpoch 21/150 - Train Loss: 0.0014, Train Acc: 0.9918 - Val Loss: 0.0071, Val Acc: 0.9620\n\n\nEpoch 22/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.33batch/s, loss=0.000371]\nEpoch 22/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.22batch/s, loss=5.24e-6]\n\n\nEpoch 22/150 - Train Loss: 0.0012, Train Acc: 0.9940 - Val Loss: 0.0076, Val Acc: 0.9572\n\n\nEpoch 23/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.59batch/s, loss=0.00077] \nEpoch 23/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.96batch/s, loss=3.36e-7]\n\n\nEpoch 23/150 - Train Loss: 0.0007, Train Acc: 0.9963 - Val Loss: 0.0079, Val Acc: 0.9583\n\n\nEpoch 24/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.00174] \nEpoch 24/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.99batch/s, loss=1.71e-7]\n\n\nEpoch 24/150 - Train Loss: 0.0010, Train Acc: 0.9936 - Val Loss: 0.0086, Val Acc: 0.9539\n\n\nEpoch 25/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.46batch/s, loss=0.000475]\nEpoch 25/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.97batch/s, loss=-5.07e-8]\n\n\nEpoch 25/150 - Train Loss: 0.0011, Train Acc: 0.9933 - Val Loss: 0.0076, Val Acc: 0.9606\n\n\nEpoch 26/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.63batch/s, loss=0.00139] \nEpoch 26/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.86batch/s, loss=-6.98e-8]\n\n\nEpoch 26/150 - Train Loss: 0.0009, Train Acc: 0.9942 - Val Loss: 0.0066, Val Acc: 0.9644\n\n\nEpoch 27/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.66batch/s, loss=0.000834]\nEpoch 27/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.40batch/s, loss=3.82e-7]\n\n\nEpoch 27/150 - Train Loss: 0.0006, Train Acc: 0.9961 - Val Loss: 0.0065, Val Acc: 0.9630\n\n\nEpoch 28/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.63batch/s, loss=0.000493]\nEpoch 28/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.29batch/s, loss=3.3e-7] \n\n\nEpoch 28/150 - Train Loss: 0.0005, Train Acc: 0.9969 - Val Loss: 0.0063, Val Acc: 0.9657\n\n\nEpoch 29/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.64batch/s, loss=6.04e-5] \nEpoch 29/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.35batch/s, loss=-5.55e-8]\n\n\nEpoch 29/150 - Train Loss: 0.0008, Train Acc: 0.9955 - Val Loss: 0.0063, Val Acc: 0.9637\n\n\nEpoch 30/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.52batch/s, loss=4.53e-5] \nEpoch 30/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.64batch/s, loss=-7.37e-8]\n\n\nEpoch 30/150 - Train Loss: 0.0006, Train Acc: 0.9963 - Val Loss: 0.0049, Val Acc: 0.9746\nNew best model found at epoch 30 with val_loss: 0.0049\n\n\nEpoch 31/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.68batch/s, loss=0.00561] \nEpoch 31/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.49batch/s, loss=-8.55e-8]\n\n\nEpoch 31/150 - Train Loss: 0.0010, Train Acc: 0.9936 - Val Loss: 0.0064, Val Acc: 0.9640\n\n\nEpoch 32/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.71batch/s, loss=4.76e-5] \nEpoch 32/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.04batch/s, loss=3.81e-6]\n\n\nEpoch 32/150 - Train Loss: 0.0010, Train Acc: 0.9944 - Val Loss: 0.0057, Val Acc: 0.9678\n\n\nEpoch 33/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.60batch/s, loss=0.000509]\nEpoch 33/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.62batch/s, loss=3.32e-6]\n\n\nEpoch 33/150 - Train Loss: 0.0009, Train Acc: 0.9950 - Val Loss: 0.0071, Val Acc: 0.9637\n\n\nEpoch 34/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.66batch/s, loss=0.000317]\nEpoch 34/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 33.93batch/s, loss=2.69e-6]\n\n\nEpoch 34/150 - Train Loss: 0.0007, Train Acc: 0.9961 - Val Loss: 0.0055, Val Acc: 0.9705\n\n\nEpoch 35/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.90batch/s, loss=0.000529]\nEpoch 35/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.84batch/s, loss=-6.92e-8]\n\n\nEpoch 35/150 - Train Loss: 0.0004, Train Acc: 0.9980 - Val Loss: 0.0054, Val Acc: 0.9718\n\n\nEpoch 36/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.98batch/s, loss=0.000713]\nEpoch 36/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.35batch/s, loss=-5.32e-8]\n\n\nEpoch 36/150 - Train Loss: 0.0007, Train Acc: 0.9959 - Val Loss: 0.0067, Val Acc: 0.9640\n\n\nEpoch 37/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.22batch/s, loss=5.37e-6] \nEpoch 37/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.02batch/s, loss=0.000543]\n\n\nEpoch 37/150 - Train Loss: 0.0008, Train Acc: 0.9956 - Val Loss: 0.0078, Val Acc: 0.9589\n\n\nEpoch 38/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.54batch/s, loss=0.000445]\nEpoch 38/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.16batch/s, loss=-6.55e-8]\n\n\nEpoch 38/150 - Train Loss: 0.0008, Train Acc: 0.9955 - Val Loss: 0.0081, Val Acc: 0.9583\n\n\nEpoch 39/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.51batch/s, loss=0.0011]  \nEpoch 39/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.17batch/s, loss=-6.8e-8]\n\n\nEpoch 39/150 - Train Loss: 0.0009, Train Acc: 0.9946 - Val Loss: 0.0069, Val Acc: 0.9650\n\n\nEpoch 40/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.66batch/s, loss=6.38e-5] \nEpoch 40/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.23batch/s, loss=1.47e-7]\n\n\nEpoch 40/150 - Train Loss: 0.0009, Train Acc: 0.9942 - Val Loss: 0.0088, Val Acc: 0.9545\n\n\nEpoch 41/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.69batch/s, loss=4.41e-6] \nEpoch 41/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.26batch/s, loss=-4e-8]  \n\n\nEpoch 41/150 - Train Loss: 0.0009, Train Acc: 0.9951 - Val Loss: 0.0071, Val Acc: 0.9630\n\n\nEpoch 42/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.000219]\nEpoch 42/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.40batch/s, loss=-2.66e-8]\n\n\nEpoch 42/150 - Train Loss: 0.0010, Train Acc: 0.9944 - Val Loss: 0.0082, Val Acc: 0.9586\n\n\nEpoch 43/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.75batch/s, loss=0.00267] \nEpoch 43/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.62batch/s, loss=-5.12e-8]\n\n\nEpoch 43/150 - Train Loss: 0.0006, Train Acc: 0.9969 - Val Loss: 0.0072, Val Acc: 0.9613\n\n\nEpoch 44/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.61batch/s, loss=0.000231]\nEpoch 44/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.59batch/s, loss=-3.55e-8]\n\n\nEpoch 44/150 - Train Loss: 0.0006, Train Acc: 0.9965 - Val Loss: 0.0081, Val Acc: 0.9576\n\n\nEpoch 45/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.71batch/s, loss=0.00116] \nEpoch 45/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.59batch/s, loss=-4.39e-8]\n\n\nEpoch 45/150 - Train Loss: 0.0005, Train Acc: 0.9971 - Val Loss: 0.0079, Val Acc: 0.9593\n\n\nEpoch 46/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.45batch/s, loss=0.000188]\nEpoch 46/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.96batch/s, loss=-4.32e-8]\n\n\nEpoch 46/150 - Train Loss: 0.0005, Train Acc: 0.9970 - Val Loss: 0.0084, Val Acc: 0.9566\n\n\nEpoch 47/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.64batch/s, loss=0.00144] \nEpoch 47/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.53batch/s, loss=-4.32e-8]\n\n\nEpoch 47/150 - Train Loss: 0.0008, Train Acc: 0.9954 - Val Loss: 0.0077, Val Acc: 0.9562\n\n\nEpoch 48/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.66batch/s, loss=8.87e-5] \nEpoch 48/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.12batch/s, loss=-4.64e-8]\n\n\nEpoch 48/150 - Train Loss: 0.0006, Train Acc: 0.9965 - Val Loss: 0.0082, Val Acc: 0.9552\n\n\nEpoch 49/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.51batch/s, loss=4.17e-5] \nEpoch 49/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.93batch/s, loss=-5.14e-8]\n\n\nEpoch 49/150 - Train Loss: 0.0005, Train Acc: 0.9974 - Val Loss: 0.0088, Val Acc: 0.9542\n\n\nEpoch 50/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.71batch/s, loss=0.000807]\nEpoch 50/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.09batch/s, loss=-4.73e-8]\n\n\nEpoch 50/150 - Train Loss: 0.0004, Train Acc: 0.9974 - Val Loss: 0.0084, Val Acc: 0.9562\n\n\nEpoch 51/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.70batch/s, loss=0.000145]\nEpoch 51/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.10batch/s, loss=-5.19e-8]\n\n\nEpoch 51/150 - Train Loss: 0.0004, Train Acc: 0.9976 - Val Loss: 0.0089, Val Acc: 0.9535\n\n\nEpoch 52/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.45batch/s, loss=0.000894]\nEpoch 52/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.81batch/s, loss=-3.78e-8]\n\n\nEpoch 52/150 - Train Loss: 0.0006, Train Acc: 0.9962 - Val Loss: 0.0076, Val Acc: 0.9613\n\n\nEpoch 53/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.000404]\nEpoch 53/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.37batch/s, loss=-3.96e-8]\n\n\nEpoch 53/150 - Train Loss: 0.0004, Train Acc: 0.9976 - Val Loss: 0.0085, Val Acc: 0.9569\n\n\nEpoch 54/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.52batch/s, loss=2.88e-5] \nEpoch 54/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.75batch/s, loss=-3.98e-8]\n\n\nEpoch 54/150 - Train Loss: 0.0005, Train Acc: 0.9974 - Val Loss: 0.0089, Val Acc: 0.9552\n\n\nEpoch 55/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.54batch/s, loss=0.000136]\nEpoch 55/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.03batch/s, loss=-1.84e-8]\n\n\nEpoch 55/150 - Train Loss: 0.0007, Train Acc: 0.9962 - Val Loss: 0.0081, Val Acc: 0.9566\n\n\nEpoch 56/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=0.000108]\nEpoch 56/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.28batch/s, loss=-4.3e-8]\n\n\nEpoch 56/150 - Train Loss: 0.0004, Train Acc: 0.9977 - Val Loss: 0.0086, Val Acc: 0.9569\n\n\nEpoch 57/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.000356]\nEpoch 57/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.51batch/s, loss=-4.87e-8]\n\n\nEpoch 57/150 - Train Loss: 0.0006, Train Acc: 0.9966 - Val Loss: 0.0104, Val Acc: 0.9460\n\n\nEpoch 58/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.58batch/s, loss=0.000594]\nEpoch 58/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.76batch/s, loss=-4.12e-8]\n\n\nEpoch 58/150 - Train Loss: 0.0010, Train Acc: 0.9944 - Val Loss: 0.0099, Val Acc: 0.9498\n\n\nEpoch 59/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.69batch/s, loss=0.000857]\nEpoch 59/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.54batch/s, loss=-3.87e-8]\n\n\nEpoch 59/150 - Train Loss: 0.0008, Train Acc: 0.9952 - Val Loss: 0.0094, Val Acc: 0.9518\n\n\nEpoch 60/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.77batch/s, loss=8.44e-6] \nEpoch 60/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.19batch/s, loss=-5.19e-8]\n\n\nEpoch 60/150 - Train Loss: 0.0006, Train Acc: 0.9965 - Val Loss: 0.0085, Val Acc: 0.9549\n\n\nEpoch 61/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.72batch/s, loss=4.92e-6] \nEpoch 61/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.05batch/s, loss=-5.57e-8]\n\n\nEpoch 61/150 - Train Loss: 0.0004, Train Acc: 0.9977 - Val Loss: 0.0095, Val Acc: 0.9501\n\n\nEpoch 62/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.70batch/s, loss=0.00234] \nEpoch 62/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.37batch/s, loss=-3.62e-8]\n\n\nEpoch 62/150 - Train Loss: 0.0011, Train Acc: 0.9944 - Val Loss: 0.0096, Val Acc: 0.9522\n\n\nEpoch 63/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.61batch/s, loss=2.66e-5] \nEpoch 63/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.70batch/s, loss=-6.48e-8]\n\n\nEpoch 63/150 - Train Loss: 0.0004, Train Acc: 0.9976 - Val Loss: 0.0105, Val Acc: 0.9484\n\n\nEpoch 64/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.68batch/s, loss=0.000539]\nEpoch 64/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.62batch/s, loss=-7.01e-8]\n\n\nEpoch 64/150 - Train Loss: 0.0007, Train Acc: 0.9961 - Val Loss: 0.0080, Val Acc: 0.9600\n\n\nEpoch 65/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.73batch/s, loss=0.000833]\nEpoch 65/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.48batch/s, loss=-7.03e-8]\n\n\nEpoch 65/150 - Train Loss: 0.0003, Train Acc: 0.9984 - Val Loss: 0.0090, Val Acc: 0.9545\n\n\nEpoch 66/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.59batch/s, loss=5.6e-6]  \nEpoch 66/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.36batch/s, loss=-5.26e-8]\n\n\nEpoch 66/150 - Train Loss: 0.0004, Train Acc: 0.9980 - Val Loss: 0.0080, Val Acc: 0.9555\n\n\nEpoch 67/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.63batch/s, loss=1.87e-5] \nEpoch 67/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.40batch/s, loss=-6.05e-8]\n\n\nEpoch 67/150 - Train Loss: 0.0002, Train Acc: 0.9988 - Val Loss: 0.0078, Val Acc: 0.9579\n\n\nEpoch 68/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.59batch/s, loss=0.00122] \nEpoch 68/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.19batch/s, loss=-5.73e-8]\n\n\nEpoch 68/150 - Train Loss: 0.0004, Train Acc: 0.9981 - Val Loss: 0.0092, Val Acc: 0.9528\n\n\nEpoch 69/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=0.00141] \nEpoch 69/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.61batch/s, loss=-5.28e-8]\n\n\nEpoch 69/150 - Train Loss: 0.0010, Train Acc: 0.9940 - Val Loss: 0.0091, Val Acc: 0.9542\n\n\nEpoch 70/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.64batch/s, loss=9.04e-6] \nEpoch 70/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.44batch/s, loss=-5.19e-8]\n\n\nEpoch 70/150 - Train Loss: 0.0008, Train Acc: 0.9956 - Val Loss: 0.0082, Val Acc: 0.9593\n\n\nEpoch 71/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.57batch/s, loss=0.000645]\nEpoch 71/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.02batch/s, loss=-2.66e-8]\n\n\nEpoch 71/150 - Train Loss: 0.0006, Train Acc: 0.9958 - Val Loss: 0.0075, Val Acc: 0.9613\n\n\nEpoch 72/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.59batch/s, loss=0.00379] \nEpoch 72/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.84batch/s, loss=-5.62e-8]\n\n\nEpoch 72/150 - Train Loss: 0.0010, Train Acc: 0.9944 - Val Loss: 0.0075, Val Acc: 0.9593\n\n\nEpoch 73/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.64batch/s, loss=9.18e-5] \nEpoch 73/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.90batch/s, loss=-5.16e-8]\n\n\nEpoch 73/150 - Train Loss: 0.0011, Train Acc: 0.9932 - Val Loss: 0.0077, Val Acc: 0.9600\n\n\nEpoch 74/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.69batch/s, loss=0.00232] \nEpoch 74/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.80batch/s, loss=-3.57e-8]\n\n\nEpoch 74/150 - Train Loss: 0.0010, Train Acc: 0.9936 - Val Loss: 0.0079, Val Acc: 0.9586\n\n\nEpoch 75/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.17batch/s, loss=0.000876]\nEpoch 75/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.91batch/s, loss=-4.05e-8]\n\n\nEpoch 75/150 - Train Loss: 0.0009, Train Acc: 0.9948 - Val Loss: 0.0095, Val Acc: 0.9491\n\n\nEpoch 76/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.69batch/s, loss=6.31e-6] \nEpoch 76/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.40batch/s, loss=-4.46e-8]\n\n\nEpoch 76/150 - Train Loss: 0.0006, Train Acc: 0.9966 - Val Loss: 0.0089, Val Acc: 0.9545\n\n\nEpoch 77/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.45batch/s, loss=0.00198] \nEpoch 77/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.25batch/s, loss=-4.73e-8]\n\n\nEpoch 77/150 - Train Loss: 0.0006, Train Acc: 0.9962 - Val Loss: 0.0088, Val Acc: 0.9566\n\n\nEpoch 78/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.00113] \nEpoch 78/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.11batch/s, loss=-6.37e-8]\n\n\nEpoch 78/150 - Train Loss: 0.0005, Train Acc: 0.9967 - Val Loss: 0.0088, Val Acc: 0.9555\n\n\nEpoch 79/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=0.000288]\nEpoch 79/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.86batch/s, loss=-5.78e-8]\n\n\nEpoch 79/150 - Train Loss: 0.0005, Train Acc: 0.9971 - Val Loss: 0.0080, Val Acc: 0.9576\n\n\nEpoch 80/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.70batch/s, loss=0.00184] \nEpoch 80/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.47batch/s, loss=-7.85e-8]\n\n\nEpoch 80/150 - Train Loss: 0.0005, Train Acc: 0.9974 - Val Loss: 0.0090, Val Acc: 0.9535\n\n\nEpoch 81/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.41batch/s, loss=0.000388]\nEpoch 81/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 29.96batch/s, loss=3.44e-7] \n\n\nEpoch 81/150 - Train Loss: 0.0009, Train Acc: 0.9952 - Val Loss: 0.0096, Val Acc: 0.9491\n\n\nEpoch 82/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.28batch/s, loss=0.000401]\nEpoch 82/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.90batch/s, loss=-6.53e-8]\n\n\nEpoch 82/150 - Train Loss: 0.0005, Train Acc: 0.9974 - Val Loss: 0.0093, Val Acc: 0.9532\n\n\nEpoch 83/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.69batch/s, loss=0.000519]\nEpoch 83/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.48batch/s, loss=6.79e-6]\n\n\nEpoch 83/150 - Train Loss: 0.0007, Train Acc: 0.9954 - Val Loss: 0.0078, Val Acc: 0.9617\n\n\nEpoch 84/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.55batch/s, loss=1.21e-5] \nEpoch 84/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.65batch/s, loss=-4.07e-8]\n\n\nEpoch 84/150 - Train Loss: 0.0005, Train Acc: 0.9974 - Val Loss: 0.0073, Val Acc: 0.9630\n\n\nEpoch 85/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.66batch/s, loss=0.000208]\nEpoch 85/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.97batch/s, loss=-5.41e-8]\n\n\nEpoch 85/150 - Train Loss: 0.0002, Train Acc: 0.9985 - Val Loss: 0.0076, Val Acc: 0.9623\n\n\nEpoch 86/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.56batch/s, loss=0.000375]\nEpoch 86/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.83batch/s, loss=-6.07e-8]\n\n\nEpoch 86/150 - Train Loss: 0.0002, Train Acc: 0.9985 - Val Loss: 0.0071, Val Acc: 0.9634\n\n\nEpoch 87/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.59batch/s, loss=5.34e-5] \nEpoch 87/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.50batch/s, loss=-6.32e-8]\n\n\nEpoch 87/150 - Train Loss: 0.0002, Train Acc: 0.9990 - Val Loss: 0.0071, Val Acc: 0.9650\n\n\nEpoch 88/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.71batch/s, loss=0.00266] \nEpoch 88/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.23batch/s, loss=-6.98e-8]\n\n\nEpoch 88/150 - Train Loss: 0.0004, Train Acc: 0.9973 - Val Loss: 0.0059, Val Acc: 0.9691\n\n\nEpoch 89/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.71batch/s, loss=7.01e-5] \nEpoch 89/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.29batch/s, loss=-7.96e-8]\n\n\nEpoch 89/150 - Train Loss: 0.0005, Train Acc: 0.9974 - Val Loss: 0.0073, Val Acc: 0.9627\n\n\nEpoch 90/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=0.000948]\nEpoch 90/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.57batch/s, loss=-7.62e-8]\n\n\nEpoch 90/150 - Train Loss: 0.0003, Train Acc: 0.9982 - Val Loss: 0.0077, Val Acc: 0.9600\n\n\nEpoch 91/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.63batch/s, loss=5.78e-6] \nEpoch 91/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.20batch/s, loss=-7.87e-8]\n\n\nEpoch 91/150 - Train Loss: 0.0004, Train Acc: 0.9980 - Val Loss: 0.0069, Val Acc: 0.9644\n\n\nEpoch 92/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.64batch/s, loss=5.95e-6] \nEpoch 92/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.32batch/s, loss=-6.23e-8]\n\n\nEpoch 92/150 - Train Loss: 0.0002, Train Acc: 0.9988 - Val Loss: 0.0071, Val Acc: 0.9617\n\n\nEpoch 93/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=5.12e-5] \nEpoch 93/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.94batch/s, loss=-5.91e-8]\n\n\nEpoch 93/150 - Train Loss: 0.0003, Train Acc: 0.9981 - Val Loss: 0.0074, Val Acc: 0.9596\n\n\nEpoch 94/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.000534]\nEpoch 94/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.36batch/s, loss=-4.64e-8]\n\n\nEpoch 94/150 - Train Loss: 0.0003, Train Acc: 0.9982 - Val Loss: 0.0076, Val Acc: 0.9617\n\n\nEpoch 95/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.63batch/s, loss=3.7e-5]  \nEpoch 95/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.41batch/s, loss=-6.3e-8]\n\n\nEpoch 95/150 - Train Loss: 0.0002, Train Acc: 0.9988 - Val Loss: 0.0093, Val Acc: 0.9542\n\n\nEpoch 96/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.31batch/s, loss=3.83e-5] \nEpoch 96/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.22batch/s, loss=-4.66e-8]\n\n\nEpoch 96/150 - Train Loss: 0.0004, Train Acc: 0.9981 - Val Loss: 0.0083, Val Acc: 0.9572\n\n\nEpoch 97/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=9.67e-5] \nEpoch 97/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.03batch/s, loss=-6.76e-8]\n\n\nEpoch 97/150 - Train Loss: 0.0006, Train Acc: 0.9963 - Val Loss: 0.0078, Val Acc: 0.9593\n\n\nEpoch 98/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.70batch/s, loss=9.24e-7] \nEpoch 98/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.52batch/s, loss=-5.53e-8]\n\n\nEpoch 98/150 - Train Loss: 0.0003, Train Acc: 0.9984 - Val Loss: 0.0081, Val Acc: 0.9589\n\n\nEpoch 99/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.59batch/s, loss=5.79e-5] \nEpoch 99/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.43batch/s, loss=-6.73e-8]\n\n\nEpoch 99/150 - Train Loss: 0.0002, Train Acc: 0.9990 - Val Loss: 0.0080, Val Acc: 0.9610\n\n\nEpoch 100/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.56batch/s, loss=1.08e-7] \nEpoch 100/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.99batch/s, loss=-6.69e-8]\n\n\nEpoch 100/150 - Train Loss: 0.0002, Train Acc: 0.9992 - Val Loss: 0.0073, Val Acc: 0.9647\n\n\nEpoch 101/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=9.35e-6] \nEpoch 101/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.78batch/s, loss=-5.69e-8]\n\n\nEpoch 101/150 - Train Loss: 0.0001, Train Acc: 0.9995 - Val Loss: 0.0074, Val Acc: 0.9613\n\n\nEpoch 102/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.66batch/s, loss=0.00114] \nEpoch 102/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.79batch/s, loss=2.98e-7]\n\n\nEpoch 102/150 - Train Loss: 0.0003, Train Acc: 0.9986 - Val Loss: 0.0074, Val Acc: 0.9627\nEarly stopping triggered!\nBest model saved to best_model.pth with val_loss: 0.0049 and val_acc: 0.9746\n\n\n\n\n\n\nplot_history(history)\n\n\n\n\n\n\n\n\n\nbest_model = AdaptiveRNN(n_timesteps=n_timesteps, n_features=n_features, n_outputs=n_outputs).to(device)\nbest_model.load_state_dict(torch.load('best_model.pth', weights_only = True))\n\nval_correct = 0.0\nwith torch.no_grad():\n    for input, target in test_loader:\n        input = input.to(device)\n        target = target.to(device)\n    \n        outputs = best_model(input)\n        val_correct += (outputs.argmax(1) == target.argmax(1)).sum().item()\n    print(f\"val_acc: {val_correct / len(test_loader.dataset)}\")\n\nval_acc: 0.9613165931455717\n\n\n\nplot_confusion_matrix(best_model, test_loader, class_names=[\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\", \"SITTING\", \"STANDING\", \"LAYING\"])\n\n\n\n\n\n\n\n\n\n\nTCN Model\n\nclass TCNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dilation_rate):\n        super(TCNBlock, self).__init__()\n\n        self.conv1 = nn.Conv1d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            padding='same',\n            dilation=dilation_rate\n        )\n        self.batch_norm = nn.BatchNorm1d(out_channels)\n\n        # Residual connection\n        if in_channels != out_channels:\n            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n        else:\n            self.shortcut = None\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = self.batch_norm(out)\n\n        shortcut = self.shortcut(x) if self.shortcut is not None else x\n        return F.relu(out + shortcut)\n\nclass TCNModel(nn.Module):\n    def __init__(self, n_length, n_features, n_outputs):\n        super(TCNModel, self).__init__()\n\n        self.tcn_blocks = nn.Sequential(\n            TCNBlock(n_features, 64, kernel_size=3, dilation_rate=1),\n            TCNBlock(64, 128, kernel_size=3, dilation_rate=2),\n            TCNBlock(128, 256, kernel_size=3, dilation_rate=4),\n            TCNBlock(256, 64, kernel_size=3, dilation_rate=8)\n        )\n\n        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Linear(64, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, n_outputs)\n\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)  # Convert to (batch_size, n_features, n_length)\n        x = self.tcn_blocks(x)\n\n        x = self.global_avg_pool(x).squeeze(-1)  # Global average pooling\n\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n\n        return F.softmax(x, dim=1)\n\n# Define model parameters\nn_length = 128  \nn_features = 9\nn_outputs = 6\n\n\n# Instantiate model\nmodel = TCNModel(n_length, n_features, n_outputs).to(device)\nsummary(model)\n\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n\n\nloss_fn = lambda inputs, targets: 0.9*kl_divergence_loss(inputs, targets) + 0.1*nn.CrossEntropyLoss()(inputs.float(), targets.float())\nhistory = train_model(model, train_loader, test_loader, optimizer=optimizer, criterion= loss_fn, epochs=150, device=device, early_stopping=True)\n\n\nplot_history(history)\n\n\nplot_confusion_matrix(model, test_loader, class_names=[\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\", \"SITTING\", \"STANDING\", \"LAYING\"])\n\n\n\nTCN + CBAM Model\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_channels, reduction_ratio=8):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.max_pool = nn.AdaptiveMaxPool1d(1)\n        self.fc1 = nn.Conv1d(in_channels, in_channels // reduction_ratio, kernel_size=1)\n        self.fc2 = nn.Conv1d(in_channels // reduction_ratio, in_channels, kernel_size=1)\n\n    def forward(self, x):\n        avg_pool = self.avg_pool(x)\n        max_pool = self.max_pool(x)\n\n        avg_out = F.relu(self.fc1(avg_pool))\n        avg_out = self.fc2(avg_out)\n\n        max_out = F.relu(self.fc1(max_pool))\n        max_out = self.fc2(max_out)\n\n        out = avg_out + max_out\n        return torch.sigmoid(out) * x  # Channel attention applied\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        self.conv = nn.Conv1d(2, 1, kernel_size=kernel_size, padding=kernel_size//2)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_pool = torch.mean(x, dim=1, keepdim=True)\n        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n        x_cat = torch.cat([avg_pool, max_pool], dim=1)  # Concatenate along channel dimension\n        out = self.conv(x_cat)\n        return self.sigmoid(out) * x  # Spatial attention applied\n\nclass CBAMBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(CBAMBlock, self).__init__()\n        self.channel_attention = ChannelAttention(in_channels)\n        self.spatial_attention = SpatialAttention()\n\n    def forward(self, x):\n        x = self.channel_attention(x)\n        x = self.spatial_attention(x)\n        return x\n\nclass TCNWithCBAM(nn.Module):\n    def __init__(self, n_length, n_features, n_outputs):\n        super(TCNWithCBAM, self).__init__()\n\n        self.tcn_blocks = nn.Sequential(\n            TCNBlock(n_features, 64, kernel_size=3, dilation_rate=1),\n            TCNBlock(64, 128, kernel_size=3, dilation_rate=2),\n            TCNBlock(128, 256, kernel_size=3, dilation_rate=4),\n            TCNBlock(256, 256, kernel_size=3, dilation_rate=8)\n        ) # (batch_size, out_channels, n_length)\n\n        self.cbam_block = CBAMBlock(256)\n\n        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Linear(256, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, n_outputs)\n\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)  # Convert to (batch_size, n_features, n_length)\n        x = self.tcn_blocks(x)\n\n        # Apply CBAM\n        x = self.cbam_block(x)\n\n        x = self.global_avg_pool(x).squeeze(-1)  # Global average pooling\n\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n\n        return F.softmax(x, dim=1)\n\n# Define model parameters\nn_length = 128  \nn_features = 9\nn_outputs = 6\n\n\nmodel = TCNWithCBAM(n_length, n_features, n_outputs).to(device)\nsummary(model)\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nTCNWithCBAM                              --\n├─Sequential: 1-1                        --\n│    └─TCNBlock: 2-1                     --\n│    │    └─Conv1d: 3-1                  1,792\n│    │    └─BatchNorm1d: 3-2             128\n│    │    └─Conv1d: 3-3                  640\n│    └─TCNBlock: 2-2                     --\n│    │    └─Conv1d: 3-4                  24,704\n│    │    └─BatchNorm1d: 3-5             256\n│    │    └─Conv1d: 3-6                  8,320\n│    └─TCNBlock: 2-3                     --\n│    │    └─Conv1d: 3-7                  98,560\n│    │    └─BatchNorm1d: 3-8             512\n│    │    └─Conv1d: 3-9                  33,024\n│    └─TCNBlock: 2-4                     --\n│    │    └─Conv1d: 3-10                 196,864\n│    │    └─BatchNorm1d: 3-11            512\n├─CBAMBlock: 1-2                         --\n│    └─ChannelAttention: 2-5             --\n│    │    └─AdaptiveAvgPool1d: 3-12      --\n│    │    └─AdaptiveMaxPool1d: 3-13      --\n│    │    └─Conv1d: 3-14                 8,224\n│    │    └─Conv1d: 3-15                 8,448\n│    └─SpatialAttention: 2-6             --\n│    │    └─Conv1d: 3-16                 15\n│    │    └─Sigmoid: 3-17                --\n├─AdaptiveAvgPool1d: 1-3                 --\n├─Linear: 1-4                            32,896\n├─Linear: 1-5                            8,256\n├─Linear: 1-6                            390\n├─Dropout: 1-7                           --\n=================================================================\nTotal params: 423,541\nTrainable params: 423,541\nNon-trainable params: 0\n=================================================================\n\n\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n\n\nloss_fn = lambda inputs, targets: 0.9*kl_divergence_loss(inputs, targets) + 0.1*nn.CrossEntropyLoss()(inputs.float(), targets.float())\nhistory = train_model(model, train_loader, test_loader, optimizer=optimizer, criterion= loss_fn, epochs=150, device=device, early_stopping=True)\n\nEpoch 1/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 14.79batch/s, loss=0.21] \nEpoch 1/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 40.84batch/s, loss=0.196]\n\n\nEpoch 1/150 - Train Loss: 0.2444, Train Acc: 0.5257 - Val Loss: 0.2193, Val Acc: 0.6895\nNew best model found at epoch 1 with val_loss: 0.2193\n\n\nEpoch 2/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.42batch/s, loss=0.12] \nEpoch 2/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.88batch/s, loss=0.105]\n\n\nEpoch 2/150 - Train Loss: 0.1591, Train Acc: 0.8183 - Val Loss: 0.1289, Val Acc: 0.9074\nNew best model found at epoch 2 with val_loss: 0.1289\n\n\nEpoch 3/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.121]\nEpoch 3/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.27batch/s, loss=0.104]\n\n\nEpoch 3/150 - Train Loss: 0.1181, Train Acc: 0.9529 - Val Loss: 0.1284, Val Acc: 0.9141\nNew best model found at epoch 3 with val_loss: 0.1284\n\n\nEpoch 4/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.61batch/s, loss=0.111]\nEpoch 4/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 39.80batch/s, loss=0.104]\n\n\nEpoch 4/150 - Train Loss: 0.1168, Train Acc: 0.9573 - Val Loss: 0.1287, Val Acc: 0.9138\n\n\nEpoch 5/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.32batch/s, loss=0.116]\nEpoch 5/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.77batch/s, loss=0.104]\n\n\nEpoch 5/150 - Train Loss: 0.1157, Train Acc: 0.9600 - Val Loss: 0.1277, Val Acc: 0.9175\nNew best model found at epoch 5 with val_loss: 0.1277\n\n\nEpoch 6/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.59batch/s, loss=0.114]\nEpoch 6/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.53batch/s, loss=0.104]\n\n\nEpoch 6/150 - Train Loss: 0.1148, Train Acc: 0.9623 - Val Loss: 0.1267, Val Acc: 0.9226\nNew best model found at epoch 6 with val_loss: 0.1267\n\n\nEpoch 7/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.65batch/s, loss=0.118]\nEpoch 7/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.28batch/s, loss=0.104]\n\n\nEpoch 7/150 - Train Loss: 0.1148, Train Acc: 0.9633 - Val Loss: 0.1240, Val Acc: 0.9291\nNew best model found at epoch 7 with val_loss: 0.1240\n\n\nEpoch 8/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.45batch/s, loss=0.111]\nEpoch 8/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.46batch/s, loss=0.104]\n\n\nEpoch 8/150 - Train Loss: 0.1143, Train Acc: 0.9642 - Val Loss: 0.1269, Val Acc: 0.9158\n\n\nEpoch 9/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.71batch/s, loss=0.113]\nEpoch 9/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.51batch/s, loss=0.104]\n\n\nEpoch 9/150 - Train Loss: 0.1135, Train Acc: 0.9676 - Val Loss: 0.1267, Val Acc: 0.9182\n\n\nEpoch 10/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.45batch/s, loss=0.111]\nEpoch 10/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.84batch/s, loss=0.104]\n\n\nEpoch 10/150 - Train Loss: 0.1134, Train Acc: 0.9683 - Val Loss: 0.1244, Val Acc: 0.9253\n\n\nEpoch 11/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.62batch/s, loss=0.11] \nEpoch 11/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.93batch/s, loss=0.104]\n\n\nEpoch 11/150 - Train Loss: 0.1127, Train Acc: 0.9703 - Val Loss: 0.1244, Val Acc: 0.9257\n\n\nEpoch 12/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.71batch/s, loss=0.112]\nEpoch 12/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.90batch/s, loss=0.104]\n\n\nEpoch 12/150 - Train Loss: 0.1128, Train Acc: 0.9690 - Val Loss: 0.1231, Val Acc: 0.9311\nNew best model found at epoch 12 with val_loss: 0.1231\n\n\nEpoch 13/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.70batch/s, loss=0.115]\nEpoch 13/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.99batch/s, loss=0.104]\n\n\nEpoch 13/150 - Train Loss: 0.1121, Train Acc: 0.9728 - Val Loss: 0.1233, Val Acc: 0.9301\n\n\nEpoch 14/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.44batch/s, loss=0.11] \nEpoch 14/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.62batch/s, loss=0.104]\n\n\nEpoch 14/150 - Train Loss: 0.1122, Train Acc: 0.9714 - Val Loss: 0.1213, Val Acc: 0.9328\nNew best model found at epoch 14 with val_loss: 0.1213\n\n\nEpoch 15/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.07batch/s, loss=0.12] \nEpoch 15/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.12batch/s, loss=0.104]\n\n\nEpoch 15/150 - Train Loss: 0.1116, Train Acc: 0.9725 - Val Loss: 0.1223, Val Acc: 0.9325\n\n\nEpoch 16/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.62batch/s, loss=0.113]\nEpoch 16/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.27batch/s, loss=0.104]\n\n\nEpoch 16/150 - Train Loss: 0.1111, Train Acc: 0.9761 - Val Loss: 0.1219, Val Acc: 0.9328\n\n\nEpoch 17/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.80batch/s, loss=0.105]\nEpoch 17/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.56batch/s, loss=0.104]\n\n\nEpoch 17/150 - Train Loss: 0.1114, Train Acc: 0.9752 - Val Loss: 0.1243, Val Acc: 0.9253\n\n\nEpoch 18/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.87batch/s, loss=0.115]\nEpoch 18/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.74batch/s, loss=0.104]\n\n\nEpoch 18/150 - Train Loss: 0.1127, Train Acc: 0.9710 - Val Loss: 0.1213, Val Acc: 0.9386\nNew best model found at epoch 18 with val_loss: 0.1213\n\n\nEpoch 19/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 24.21batch/s, loss=0.112]\nEpoch 19/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.57batch/s, loss=0.104]\n\n\nEpoch 19/150 - Train Loss: 0.1110, Train Acc: 0.9762 - Val Loss: 0.1199, Val Acc: 0.9433\nNew best model found at epoch 19 with val_loss: 0.1199\n\n\nEpoch 20/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.50batch/s, loss=0.109]\nEpoch 20/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.23batch/s, loss=0.104]\n\n\nEpoch 20/150 - Train Loss: 0.1109, Train Acc: 0.9757 - Val Loss: 0.1193, Val Acc: 0.9447\nNew best model found at epoch 20 with val_loss: 0.1193\n\n\nEpoch 21/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.44batch/s, loss=0.112]\nEpoch 21/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 39.01batch/s, loss=0.104]\n\n\nEpoch 21/150 - Train Loss: 0.1112, Train Acc: 0.9754 - Val Loss: 0.1195, Val Acc: 0.9433\n\n\nEpoch 22/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.106]\nEpoch 22/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.77batch/s, loss=0.104]\n\n\nEpoch 22/150 - Train Loss: 0.1109, Train Acc: 0.9767 - Val Loss: 0.1193, Val Acc: 0.9457\nNew best model found at epoch 22 with val_loss: 0.1193\n\n\nEpoch 23/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.58batch/s, loss=0.106]\nEpoch 23/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.03batch/s, loss=0.108]\n\n\nEpoch 23/150 - Train Loss: 0.1105, Train Acc: 0.9780 - Val Loss: 0.1230, Val Acc: 0.9304\n\n\nEpoch 24/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.61batch/s, loss=0.111]\nEpoch 24/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.72batch/s, loss=0.104]\n\n\nEpoch 24/150 - Train Loss: 0.1111, Train Acc: 0.9762 - Val Loss: 0.1226, Val Acc: 0.9348\n\n\nEpoch 25/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.116]\nEpoch 25/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.43batch/s, loss=0.104]\n\n\nEpoch 25/150 - Train Loss: 0.1103, Train Acc: 0.9776 - Val Loss: 0.1226, Val Acc: 0.9325\n\n\nEpoch 26/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.57batch/s, loss=0.11] \nEpoch 26/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 41.54batch/s, loss=0.104]\n\n\nEpoch 26/150 - Train Loss: 0.1104, Train Acc: 0.9784 - Val Loss: 0.1212, Val Acc: 0.9420\n\n\nEpoch 27/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.112]\nEpoch 27/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.58batch/s, loss=0.104]\n\n\nEpoch 27/150 - Train Loss: 0.1106, Train Acc: 0.9773 - Val Loss: 0.1214, Val Acc: 0.9382\n\n\nEpoch 28/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.44batch/s, loss=0.111]\nEpoch 28/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.04batch/s, loss=0.104]\n\n\nEpoch 28/150 - Train Loss: 0.1097, Train Acc: 0.9797 - Val Loss: 0.1203, Val Acc: 0.9413\n\n\nEpoch 29/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.51batch/s, loss=0.112]\nEpoch 29/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.16batch/s, loss=0.104]\n\n\nEpoch 29/150 - Train Loss: 0.1100, Train Acc: 0.9793 - Val Loss: 0.1214, Val Acc: 0.9372\n\n\nEpoch 30/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.72batch/s, loss=0.107]\nEpoch 30/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.91batch/s, loss=0.104]\n\n\nEpoch 30/150 - Train Loss: 0.1093, Train Acc: 0.9822 - Val Loss: 0.1213, Val Acc: 0.9406\n\n\nEpoch 31/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.63batch/s, loss=0.113]\nEpoch 31/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.96batch/s, loss=0.104]\n\n\nEpoch 31/150 - Train Loss: 0.1100, Train Acc: 0.9791 - Val Loss: 0.1238, Val Acc: 0.9311\n\n\nEpoch 32/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.68batch/s, loss=0.108]\nEpoch 32/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.42batch/s, loss=0.104]\n\n\nEpoch 32/150 - Train Loss: 0.1109, Train Acc: 0.9761 - Val Loss: 0.1218, Val Acc: 0.9386\n\n\nEpoch 33/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.74batch/s, loss=0.111]\nEpoch 33/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.45batch/s, loss=0.104]\n\n\nEpoch 33/150 - Train Loss: 0.1110, Train Acc: 0.9743 - Val Loss: 0.1201, Val Acc: 0.9444\n\n\nEpoch 34/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.56batch/s, loss=0.11] \nEpoch 34/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 41.81batch/s, loss=0.104]\n\n\nEpoch 34/150 - Train Loss: 0.1100, Train Acc: 0.9793 - Val Loss: 0.1199, Val Acc: 0.9450\n\n\nEpoch 35/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.21batch/s, loss=0.109]\nEpoch 35/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.67batch/s, loss=0.104]\n\n\nEpoch 35/150 - Train Loss: 0.1102, Train Acc: 0.9785 - Val Loss: 0.1245, Val Acc: 0.9304\n\n\nEpoch 36/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.58batch/s, loss=0.111]\nEpoch 36/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.97batch/s, loss=0.104]\n\n\nEpoch 36/150 - Train Loss: 0.1101, Train Acc: 0.9789 - Val Loss: 0.1255, Val Acc: 0.9237\n\n\nEpoch 37/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.60batch/s, loss=0.108]\nEpoch 37/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.92batch/s, loss=0.104]\n\n\nEpoch 37/150 - Train Loss: 0.1093, Train Acc: 0.9818 - Val Loss: 0.1248, Val Acc: 0.9247\n\n\nEpoch 38/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.32batch/s, loss=0.113]\nEpoch 38/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 38.88batch/s, loss=0.104]\n\n\nEpoch 38/150 - Train Loss: 0.1085, Train Acc: 0.9856 - Val Loss: 0.1245, Val Acc: 0.9277\n\n\nEpoch 39/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.32batch/s, loss=0.111]\nEpoch 39/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.09batch/s, loss=0.104]\n\n\nEpoch 39/150 - Train Loss: 0.1096, Train Acc: 0.9808 - Val Loss: 0.1239, Val Acc: 0.9325\n\n\nEpoch 40/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.62batch/s, loss=0.107]\nEpoch 40/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.12batch/s, loss=0.104]\n\n\nEpoch 40/150 - Train Loss: 0.1095, Train Acc: 0.9816 - Val Loss: 0.1215, Val Acc: 0.9379\n\n\nEpoch 41/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.41batch/s, loss=0.11] \nEpoch 41/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.98batch/s, loss=0.104]\n\n\nEpoch 41/150 - Train Loss: 0.1097, Train Acc: 0.9808 - Val Loss: 0.1224, Val Acc: 0.9355\n\n\nEpoch 42/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.60batch/s, loss=0.108]\nEpoch 42/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.04batch/s, loss=0.104]\n\n\nEpoch 42/150 - Train Loss: 0.1087, Train Acc: 0.9842 - Val Loss: 0.1219, Val Acc: 0.9372\n\n\nEpoch 43/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.66batch/s, loss=0.11] \nEpoch 43/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.47batch/s, loss=0.104]\n\n\nEpoch 43/150 - Train Loss: 0.1094, Train Acc: 0.9810 - Val Loss: 0.1191, Val Acc: 0.9460\n\n\nEpoch 44/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.41batch/s, loss=0.112]\nEpoch 44/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 40.24batch/s, loss=0.104]\n\n\nEpoch 44/150 - Train Loss: 0.1091, Train Acc: 0.9830 - Val Loss: 0.1230, Val Acc: 0.9318\n\n\nEpoch 45/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.59batch/s, loss=0.108]\nEpoch 45/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.91batch/s, loss=0.104]\n\n\nEpoch 45/150 - Train Loss: 0.1101, Train Acc: 0.9795 - Val Loss: 0.1261, Val Acc: 0.9226\n\n\nEpoch 46/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.51batch/s, loss=0.111]\nEpoch 46/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.66batch/s, loss=0.104]\n\n\nEpoch 46/150 - Train Loss: 0.1105, Train Acc: 0.9786 - Val Loss: 0.1220, Val Acc: 0.9369\n\n\nEpoch 47/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.43batch/s, loss=0.107]\nEpoch 47/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.64batch/s, loss=0.104]\n\n\nEpoch 47/150 - Train Loss: 0.1098, Train Acc: 0.9808 - Val Loss: 0.1231, Val Acc: 0.9328\n\n\nEpoch 48/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.05batch/s, loss=0.113]\nEpoch 48/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 41.23batch/s, loss=0.104]\n\n\nEpoch 48/150 - Train Loss: 0.1086, Train Acc: 0.9852 - Val Loss: 0.1226, Val Acc: 0.9345\n\n\nEpoch 49/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.111]\nEpoch 49/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.23batch/s, loss=0.104]\n\n\nEpoch 49/150 - Train Loss: 0.1094, Train Acc: 0.9819 - Val Loss: 0.1254, Val Acc: 0.9243\n\n\nEpoch 50/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.27batch/s, loss=0.105]\nEpoch 50/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.58batch/s, loss=0.104]\n\n\nEpoch 50/150 - Train Loss: 0.1088, Train Acc: 0.9839 - Val Loss: 0.1264, Val Acc: 0.9223\n\n\nEpoch 51/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.35batch/s, loss=0.106]\nEpoch 51/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.91batch/s, loss=0.104]\n\n\nEpoch 51/150 - Train Loss: 0.1082, Train Acc: 0.9867 - Val Loss: 0.1242, Val Acc: 0.9321\n\n\nEpoch 52/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.65batch/s, loss=0.112]\nEpoch 52/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.80batch/s, loss=0.104]\n\n\nEpoch 52/150 - Train Loss: 0.1078, Train Acc: 0.9876 - Val Loss: 0.1242, Val Acc: 0.9301\n\n\nEpoch 53/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.66batch/s, loss=0.106]\nEpoch 53/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.47batch/s, loss=0.104]\n\n\nEpoch 53/150 - Train Loss: 0.1083, Train Acc: 0.9861 - Val Loss: 0.1246, Val Acc: 0.9287\n\n\nEpoch 54/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.75batch/s, loss=0.109]\nEpoch 54/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.11batch/s, loss=0.104]\n\n\nEpoch 54/150 - Train Loss: 0.1080, Train Acc: 0.9869 - Val Loss: 0.1289, Val Acc: 0.9155\n\n\nEpoch 55/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.52batch/s, loss=0.108]\nEpoch 55/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.83batch/s, loss=0.104]\n\n\nEpoch 55/150 - Train Loss: 0.1088, Train Acc: 0.9839 - Val Loss: 0.1245, Val Acc: 0.9298\n\n\nEpoch 56/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.71batch/s, loss=0.104]\nEpoch 56/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.46batch/s, loss=0.104]\n\n\nEpoch 56/150 - Train Loss: 0.1102, Train Acc: 0.9799 - Val Loss: 0.1203, Val Acc: 0.9447\n\n\nEpoch 57/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.64batch/s, loss=0.106]\nEpoch 57/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.05batch/s, loss=0.104]\n\n\nEpoch 57/150 - Train Loss: 0.1086, Train Acc: 0.9856 - Val Loss: 0.1222, Val Acc: 0.9386\n\n\nEpoch 58/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.64batch/s, loss=0.111]\nEpoch 58/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.76batch/s, loss=0.104]\n\n\nEpoch 58/150 - Train Loss: 0.1087, Train Acc: 0.9849 - Val Loss: 0.1192, Val Acc: 0.9484\nNew best model found at epoch 58 with val_loss: 0.1192\n\n\nEpoch 59/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.67batch/s, loss=0.107]\nEpoch 59/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 40.18batch/s, loss=0.104]\n\n\nEpoch 59/150 - Train Loss: 0.1088, Train Acc: 0.9844 - Val Loss: 0.1195, Val Acc: 0.9474\n\n\nEpoch 60/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.47batch/s, loss=0.107]\nEpoch 60/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.09batch/s, loss=0.104]\n\n\nEpoch 60/150 - Train Loss: 0.1085, Train Acc: 0.9859 - Val Loss: 0.1204, Val Acc: 0.9454\n\n\nEpoch 61/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.25batch/s, loss=0.107]\nEpoch 61/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.16batch/s, loss=0.104]\n\n\nEpoch 61/150 - Train Loss: 0.1090, Train Acc: 0.9825 - Val Loss: 0.1201, Val Acc: 0.9454\n\n\nEpoch 62/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 24.37batch/s, loss=0.108]\nEpoch 62/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 45.81batch/s, loss=0.104]\n\n\nEpoch 62/150 - Train Loss: 0.1089, Train Acc: 0.9838 - Val Loss: 0.1199, Val Acc: 0.9450\n\n\nEpoch 63/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 24.73batch/s, loss=0.107]\nEpoch 63/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 46.01batch/s, loss=0.104]\n\n\nEpoch 63/150 - Train Loss: 0.1078, Train Acc: 0.9880 - Val Loss: 0.1202, Val Acc: 0.9454\n\n\nEpoch 64/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 24.46batch/s, loss=0.105]\nEpoch 64/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 45.42batch/s, loss=0.104]\n\n\nEpoch 64/150 - Train Loss: 0.1077, Train Acc: 0.9882 - Val Loss: 0.1213, Val Acc: 0.9403\n\n\nEpoch 65/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.78batch/s, loss=0.106]\nEpoch 65/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.19batch/s, loss=0.104]\n\n\nEpoch 65/150 - Train Loss: 0.1078, Train Acc: 0.9879 - Val Loss: 0.1196, Val Acc: 0.9477\n\n\nEpoch 66/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.72batch/s, loss=0.106]\nEpoch 66/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.66batch/s, loss=0.104]\n\n\nEpoch 66/150 - Train Loss: 0.1080, Train Acc: 0.9869 - Val Loss: 0.1207, Val Acc: 0.9437\n\n\nEpoch 67/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.60batch/s, loss=0.109]\nEpoch 67/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 36.15batch/s, loss=0.104]\n\n\nEpoch 67/150 - Train Loss: 0.1088, Train Acc: 0.9838 - Val Loss: 0.1213, Val Acc: 0.9427\n\n\nEpoch 68/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.37batch/s, loss=0.113]\nEpoch 68/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.27batch/s, loss=0.104]\n\n\nEpoch 68/150 - Train Loss: 0.1086, Train Acc: 0.9849 - Val Loss: 0.1215, Val Acc: 0.9413\n\n\nEpoch 69/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.60batch/s, loss=0.111]\nEpoch 69/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.16batch/s, loss=0.104]\n\n\nEpoch 69/150 - Train Loss: 0.1087, Train Acc: 0.9846 - Val Loss: 0.1202, Val Acc: 0.9454\n\n\nEpoch 70/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.62batch/s, loss=0.106]\nEpoch 70/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.65batch/s, loss=0.104]\n\n\nEpoch 70/150 - Train Loss: 0.1082, Train Acc: 0.9864 - Val Loss: 0.1212, Val Acc: 0.9406\n\n\nEpoch 71/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.43batch/s, loss=0.111]\nEpoch 71/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.99batch/s, loss=0.104]\n\n\nEpoch 71/150 - Train Loss: 0.1082, Train Acc: 0.9865 - Val Loss: 0.1198, Val Acc: 0.9464\n\n\nEpoch 72/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.73batch/s, loss=0.108]\nEpoch 72/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.62batch/s, loss=0.104]\n\n\nEpoch 72/150 - Train Loss: 0.1085, Train Acc: 0.9854 - Val Loss: 0.1201, Val Acc: 0.9454\n\n\nEpoch 73/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.65batch/s, loss=0.111]\nEpoch 73/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.35batch/s, loss=0.104]\n\n\nEpoch 73/150 - Train Loss: 0.1082, Train Acc: 0.9863 - Val Loss: 0.1203, Val Acc: 0.9454\n\n\nEpoch 74/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.22batch/s, loss=0.109]\nEpoch 74/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.24batch/s, loss=0.104]\n\n\nEpoch 74/150 - Train Loss: 0.1079, Train Acc: 0.9878 - Val Loss: 0.1194, Val Acc: 0.9488\n\n\nEpoch 75/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.51batch/s, loss=0.118]\nEpoch 75/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.95batch/s, loss=0.104]\n\n\nEpoch 75/150 - Train Loss: 0.1076, Train Acc: 0.9887 - Val Loss: 0.1206, Val Acc: 0.9437\n\n\nEpoch 76/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.72batch/s, loss=0.107]\nEpoch 76/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.78batch/s, loss=0.104]\n\n\nEpoch 76/150 - Train Loss: 0.1074, Train Acc: 0.9893 - Val Loss: 0.1187, Val Acc: 0.9511\nNew best model found at epoch 76 with val_loss: 0.1187\n\n\nEpoch 77/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.58batch/s, loss=0.11] \nEpoch 77/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.15batch/s, loss=0.104]\n\n\nEpoch 77/150 - Train Loss: 0.1078, Train Acc: 0.9878 - Val Loss: 0.1221, Val Acc: 0.9393\n\n\nEpoch 78/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.34batch/s, loss=0.104]\nEpoch 78/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 38.20batch/s, loss=0.104]\n\n\nEpoch 78/150 - Train Loss: 0.1082, Train Acc: 0.9865 - Val Loss: 0.1189, Val Acc: 0.9511\n\n\nEpoch 79/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.105]\nEpoch 79/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.62batch/s, loss=0.104]\n\n\nEpoch 79/150 - Train Loss: 0.1081, Train Acc: 0.9865 - Val Loss: 0.1198, Val Acc: 0.9477\n\n\nEpoch 80/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.77batch/s, loss=0.105]\nEpoch 80/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.91batch/s, loss=0.104]\n\n\nEpoch 80/150 - Train Loss: 0.1076, Train Acc: 0.9879 - Val Loss: 0.1207, Val Acc: 0.9433\n\n\nEpoch 81/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.29batch/s, loss=0.11] \nEpoch 81/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.38batch/s, loss=0.104]\n\n\nEpoch 81/150 - Train Loss: 0.1074, Train Acc: 0.9895 - Val Loss: 0.1196, Val Acc: 0.9477\n\n\nEpoch 82/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.75batch/s, loss=0.108]\nEpoch 82/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.99batch/s, loss=0.104]\n\n\nEpoch 82/150 - Train Loss: 0.1071, Train Acc: 0.9906 - Val Loss: 0.1197, Val Acc: 0.9474\n\n\nEpoch 83/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.74batch/s, loss=0.109]\nEpoch 83/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 41.70batch/s, loss=0.104]\n\n\nEpoch 83/150 - Train Loss: 0.1074, Train Acc: 0.9897 - Val Loss: 0.1193, Val Acc: 0.9481\n\n\nEpoch 84/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.53batch/s, loss=0.109]\nEpoch 84/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 40.30batch/s, loss=0.104]\n\n\nEpoch 84/150 - Train Loss: 0.1079, Train Acc: 0.9878 - Val Loss: 0.1209, Val Acc: 0.9427\n\n\nEpoch 85/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.52batch/s, loss=0.109]\nEpoch 85/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.87batch/s, loss=0.104]\n\n\nEpoch 85/150 - Train Loss: 0.1075, Train Acc: 0.9897 - Val Loss: 0.1178, Val Acc: 0.9542\nNew best model found at epoch 85 with val_loss: 0.1178\n\n\nEpoch 86/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.61batch/s, loss=0.109]\nEpoch 86/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.40batch/s, loss=0.104]\n\n\nEpoch 86/150 - Train Loss: 0.1078, Train Acc: 0.9878 - Val Loss: 0.1193, Val Acc: 0.9488\n\n\nEpoch 87/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.10batch/s, loss=0.109]\nEpoch 87/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.46batch/s, loss=0.104]\n\n\nEpoch 87/150 - Train Loss: 0.1079, Train Acc: 0.9875 - Val Loss: 0.1188, Val Acc: 0.9501\n\n\nEpoch 88/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.38batch/s, loss=0.114]\nEpoch 88/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.03batch/s, loss=0.104]\n\n\nEpoch 88/150 - Train Loss: 0.1076, Train Acc: 0.9884 - Val Loss: 0.1197, Val Acc: 0.9471\n\n\nEpoch 89/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.54batch/s, loss=0.109]\nEpoch 89/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.72batch/s, loss=0.105]\n\n\nEpoch 89/150 - Train Loss: 0.1072, Train Acc: 0.9903 - Val Loss: 0.1250, Val Acc: 0.9291\n\n\nEpoch 90/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.59batch/s, loss=0.107]\nEpoch 90/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.44batch/s, loss=0.104]\n\n\nEpoch 90/150 - Train Loss: 0.1075, Train Acc: 0.9894 - Val Loss: 0.1204, Val Acc: 0.9454\n\n\nEpoch 91/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.56batch/s, loss=0.106]\nEpoch 91/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.44batch/s, loss=0.104]\n\n\nEpoch 91/150 - Train Loss: 0.1075, Train Acc: 0.9891 - Val Loss: 0.1256, Val Acc: 0.9270\n\n\nEpoch 92/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.74batch/s, loss=0.106]\nEpoch 92/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.47batch/s, loss=0.104]\n\n\nEpoch 92/150 - Train Loss: 0.1080, Train Acc: 0.9875 - Val Loss: 0.1274, Val Acc: 0.9216\n\n\nEpoch 93/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.33batch/s, loss=0.113]\nEpoch 93/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 44.04batch/s, loss=0.104]\n\n\nEpoch 93/150 - Train Loss: 0.1091, Train Acc: 0.9830 - Val Loss: 0.1240, Val Acc: 0.9332\n\n\nEpoch 94/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.94batch/s, loss=0.109]\nEpoch 94/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.23batch/s, loss=0.105]\n\n\nEpoch 94/150 - Train Loss: 0.1091, Train Acc: 0.9846 - Val Loss: 0.1217, Val Acc: 0.9396\n\n\nEpoch 95/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.63batch/s, loss=0.104]\nEpoch 95/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.31batch/s, loss=0.104]\n\n\nEpoch 95/150 - Train Loss: 0.1090, Train Acc: 0.9837 - Val Loss: 0.1198, Val Acc: 0.9464\n\n\nEpoch 96/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.64batch/s, loss=0.109]\nEpoch 96/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.85batch/s, loss=0.104]\n\n\nEpoch 96/150 - Train Loss: 0.1083, Train Acc: 0.9868 - Val Loss: 0.1199, Val Acc: 0.9457\n\n\nEpoch 97/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.55batch/s, loss=0.111]\nEpoch 97/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.26batch/s, loss=0.104]\n\n\nEpoch 97/150 - Train Loss: 0.1086, Train Acc: 0.9852 - Val Loss: 0.1198, Val Acc: 0.9464\n\n\nEpoch 98/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.57batch/s, loss=0.11] \nEpoch 98/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.76batch/s, loss=0.104]\n\n\nEpoch 98/150 - Train Loss: 0.1086, Train Acc: 0.9854 - Val Loss: 0.1202, Val Acc: 0.9460\n\n\nEpoch 99/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.70batch/s, loss=0.111]\nEpoch 99/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.31batch/s, loss=0.104]\n\n\nEpoch 99/150 - Train Loss: 0.1080, Train Acc: 0.9876 - Val Loss: 0.1186, Val Acc: 0.9515\n\n\nEpoch 100/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.80batch/s, loss=0.106]\nEpoch 100/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 37.88batch/s, loss=0.104]\n\n\nEpoch 100/150 - Train Loss: 0.1080, Train Acc: 0.9872 - Val Loss: 0.1193, Val Acc: 0.9484\n\n\nEpoch 101/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.40batch/s, loss=0.111]\nEpoch 101/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.39batch/s, loss=0.104]\n\n\nEpoch 101/150 - Train Loss: 0.1077, Train Acc: 0.9882 - Val Loss: 0.1179, Val Acc: 0.9542\n\n\nEpoch 102/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.77batch/s, loss=0.108]\nEpoch 102/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 44.20batch/s, loss=0.104]\n\n\nEpoch 102/150 - Train Loss: 0.1082, Train Acc: 0.9863 - Val Loss: 0.1170, Val Acc: 0.9562\nNew best model found at epoch 102 with val_loss: 0.1170\n\n\nEpoch 103/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.81batch/s, loss=0.108]\nEpoch 103/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.44batch/s, loss=0.104]\n\n\nEpoch 103/150 - Train Loss: 0.1073, Train Acc: 0.9894 - Val Loss: 0.1186, Val Acc: 0.9522\n\n\nEpoch 104/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.82batch/s, loss=0.105]\nEpoch 104/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.29batch/s, loss=0.104]\n\n\nEpoch 104/150 - Train Loss: 0.1071, Train Acc: 0.9901 - Val Loss: 0.1190, Val Acc: 0.9511\n\n\nEpoch 105/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.50batch/s, loss=0.106]\nEpoch 105/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.52batch/s, loss=0.104]\n\n\nEpoch 105/150 - Train Loss: 0.1069, Train Acc: 0.9916 - Val Loss: 0.1182, Val Acc: 0.9511\n\n\nEpoch 106/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.72batch/s, loss=0.109]\nEpoch 106/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.80batch/s, loss=0.104]\n\n\nEpoch 106/150 - Train Loss: 0.1072, Train Acc: 0.9902 - Val Loss: 0.1189, Val Acc: 0.9488\n\n\nEpoch 107/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.12batch/s, loss=0.105]\nEpoch 107/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.48batch/s, loss=0.104]\n\n\nEpoch 107/150 - Train Loss: 0.1069, Train Acc: 0.9910 - Val Loss: 0.1207, Val Acc: 0.9440\n\n\nEpoch 108/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.37batch/s, loss=0.106]\nEpoch 108/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.41batch/s, loss=0.104]\n\n\nEpoch 108/150 - Train Loss: 0.1072, Train Acc: 0.9895 - Val Loss: 0.1193, Val Acc: 0.9494\n\n\nEpoch 109/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.73batch/s, loss=0.106]\nEpoch 109/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.81batch/s, loss=0.104]\n\n\nEpoch 109/150 - Train Loss: 0.1072, Train Acc: 0.9903 - Val Loss: 0.1198, Val Acc: 0.9477\n\n\nEpoch 110/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.62batch/s, loss=0.104]\nEpoch 110/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.58batch/s, loss=0.104]\n\n\nEpoch 110/150 - Train Loss: 0.1072, Train Acc: 0.9899 - Val Loss: 0.1187, Val Acc: 0.9508\n\n\nEpoch 111/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.58batch/s, loss=0.106]\nEpoch 111/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.02batch/s, loss=0.104]\n\n\nEpoch 111/150 - Train Loss: 0.1071, Train Acc: 0.9901 - Val Loss: 0.1186, Val Acc: 0.9508\n\n\nEpoch 112/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.70batch/s, loss=0.105]\nEpoch 112/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.25batch/s, loss=0.104]\n\n\nEpoch 112/150 - Train Loss: 0.1076, Train Acc: 0.9880 - Val Loss: 0.1194, Val Acc: 0.9488\n\n\nEpoch 113/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.63batch/s, loss=0.108]\nEpoch 113/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 39.53batch/s, loss=0.104]\n\n\nEpoch 113/150 - Train Loss: 0.1074, Train Acc: 0.9891 - Val Loss: 0.1191, Val Acc: 0.9494\n\n\nEpoch 114/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.65batch/s, loss=0.106]\nEpoch 114/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.95batch/s, loss=0.104]\n\n\nEpoch 114/150 - Train Loss: 0.1070, Train Acc: 0.9910 - Val Loss: 0.1200, Val Acc: 0.9467\n\n\nEpoch 115/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.108]\nEpoch 115/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.32batch/s, loss=0.104]\n\n\nEpoch 115/150 - Train Loss: 0.1067, Train Acc: 0.9917 - Val Loss: 0.1192, Val Acc: 0.9501\n\n\nEpoch 116/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.78batch/s, loss=0.111]\nEpoch 116/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.87batch/s, loss=0.104]\n\n\nEpoch 116/150 - Train Loss: 0.1068, Train Acc: 0.9914 - Val Loss: 0.1189, Val Acc: 0.9494\n\n\nEpoch 117/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 24.25batch/s, loss=0.108]\nEpoch 117/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 41.40batch/s, loss=0.104]\n\n\nEpoch 117/150 - Train Loss: 0.1068, Train Acc: 0.9917 - Val Loss: 0.1196, Val Acc: 0.9474\n\n\nEpoch 118/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.60batch/s, loss=0.106]\nEpoch 118/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.16batch/s, loss=0.104]\n\n\nEpoch 118/150 - Train Loss: 0.1067, Train Acc: 0.9914 - Val Loss: 0.1188, Val Acc: 0.9505\n\n\nEpoch 119/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.51batch/s, loss=0.104]\nEpoch 119/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.77batch/s, loss=0.104]\n\n\nEpoch 119/150 - Train Loss: 0.1072, Train Acc: 0.9905 - Val Loss: 0.1201, Val Acc: 0.9450\n\n\nEpoch 120/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.36batch/s, loss=0.109]\nEpoch 120/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.21batch/s, loss=0.104]\n\n\nEpoch 120/150 - Train Loss: 0.1075, Train Acc: 0.9887 - Val Loss: 0.1210, Val Acc: 0.9433\n\n\nEpoch 121/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.58batch/s, loss=0.106]\nEpoch 121/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.15batch/s, loss=0.104]\n\n\nEpoch 121/150 - Train Loss: 0.1070, Train Acc: 0.9908 - Val Loss: 0.1193, Val Acc: 0.9488\n\n\nEpoch 122/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.64batch/s, loss=0.11] \nEpoch 122/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.32batch/s, loss=0.104]\n\n\nEpoch 122/150 - Train Loss: 0.1069, Train Acc: 0.9914 - Val Loss: 0.1193, Val Acc: 0.9481\nEarly stopping triggered!\nBest model saved to best_model.pth with val_loss: 0.1170 and val_acc: 0.9562\n\n\n\n\n\n\nplot_history(history)\n\n\n\n\n\n\n\n\n\nplot_confusion_matrix(model, test_loader, class_names=[\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\", \"SITTING\", \"STANDING\", \"LAYING\"])\n\n\n\n\n\n\n\n\n\n\nTCN + Self Attention\n\nclass SelfAttention(nn.Module):\n    def __init__(self, in_channels, d_k=64, d_v=64):\n        super(SelfAttention, self).__init__()\n        \n        self.query = nn.Conv1d(in_channels, d_k, kernel_size=1)\n        self.key = nn.Conv1d(in_channels, d_k, kernel_size=1)\n        self.value = nn.Conv1d(in_channels, d_v, kernel_size=1)\n        \n        self.attn_fc = nn.Linear(d_v, in_channels)\n\n    def forward(self, x):\n        # x: (batch_size, channels, seq_length)\n        \n        Q = self.query(x)  # Query: (batch_size, d_k, seq_length)\n        K = self.key(x)    # Key: (batch_size, d_k, seq_length)\n        V = self.value(x)  # Value: (batch_size, d_v, seq_length)\n\n        # Scaled dot-product attention\n        attn_scores = torch.bmm(Q.transpose(1, 2), K)  # (batch_size, seq_length, seq_length)\n        attn_scores = attn_scores / (Q.size(-1) ** 0.5)  # Scaling\n        \n        attn_weights = F.softmax(attn_scores, dim=-1)  # (batch_size, seq_length, seq_length)\n        \n        # Attention output\n        attn_output = torch.bmm(attn_weights, V.transpose(1, 2))  # (batch_size, seq_length, d_v)\n        attn_output = attn_output.transpose(1, 2)  # (batch_size, d_v, seq_length)\n\n        # Combine with input\n        output = self.attn_fc(attn_output)\n        output = output.permute(0, 2, 1)\n        return output + x  # Add residual connection\n\n\n# TCN + Self-Attention Model\nclass TCNWithAttention(nn.Module):\n    def __init__(self, n_length, n_features, n_outputs):\n        super(TCNWithAttention, self).__init__()\n\n        # TCN Blocks\n        self.tcn_blocks = nn.Sequential(\n            TCNBlock(n_features, 64, kernel_size=3, dilation_rate=1),\n            TCNBlock(64, 128, kernel_size=3, dilation_rate=2),\n            TCNBlock(128, 256, kernel_size=3, dilation_rate=4),\n            TCNBlock(256, 256, kernel_size=3, dilation_rate=8)\n        ) # (batch_size, out_channels, n_length)\n\n        # Self-Attention Layer\n        self.self_attention = SelfAttention(in_channels=256, d_k=128, d_v=128)\n\n        # Fully connected layers after TCN and Attention\n        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Linear(256, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, n_outputs)\n\n        self.dropout = nn.Dropout(0.5)\n        self.batch_norm1 = nn.BatchNorm1d(128)\n        self.batch_norm2 = nn.BatchNorm1d(64)\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)  # Convert to (batch_size, n_features, n_length)\n        \n        # TCN blocks for feature extraction\n        x = self.tcn_blocks(x)\n        x = self.dropout(x)\n\n        # Apply Self-Attention\n        x = self.self_attention(x)\n\n        # Global average pooling\n        x = self.global_avg_pool(x).squeeze(-1)  # (batch_size, channels)\n\n        # Fully connected layers for classification\n        x = F.relu(self.fc1(x))\n        x = self.batch_norm1(x)\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.batch_norm2(x)\n        x = self.dropout(x)\n        x = self.fc3(x)\n\n        return F.softmax(x, dim=1)\n\n# Model parameters\nn_length = 128  \nn_features = 9\nn_outputs = 6\n\n\nmodel = TCNWithAttention(n_length, n_features, n_outputs).to(device)\nsummary(model)\n\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n\n\nloss_fn = lambda inputs, targets: 0.9*kl_divergence_loss(inputs, targets) + 0.1*nn.CrossEntropyLoss()(inputs.float(), targets.float())\nhistory = train_model(model, train_loader, test_loader, optimizer=optimizer, criterion= loss_fn, epochs=150, device=device, early_stopping=True)\n\n\nplot_history(history)\n\n\nbest_model = TCNWithAttention(n_length, n_features, n_outputs).to(device)\nbest_model.load_state_dict(torch.load('best_model.pth', weights_only = True))\n\nval_correct = 0.0\nwith torch.no_grad():\n    for input, target in test_loader:\n        input = input.to(device)\n        target = target.to(device)\n    \n        outputs = best_model(input)\n        val_correct += (outputs.argmax(1) == target.argmax(1)).sum().item()\n    print(f\"val_acc: {val_correct / len(test_loader.dataset)}\")\n\n\nplot_confusion_matrix(best_model, test_loader, class_names=[\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\", \"SITTING\", \"STANDING\", \"LAYING\"])\n\n\n\nEnsemble of Models\n\nbest_model, model\n\n(AdaptiveRNN(\n   (lstm): LSTM(9, 256, batch_first=True, bidirectional=True)\n   (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n   (batch_norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n   (attention_dense): Linear(in_features=512, out_features=1, bias=True)\n   (fc1): Linear(in_features=512, out_features=128, bias=True)\n   (fc2): Linear(in_features=128, out_features=64, bias=True)\n   (fc3): Linear(in_features=64, out_features=6, bias=True)\n   (dropout): Dropout(p=0.5, inplace=False)\n ),\n TCNWithCBAM(\n   (tcn_blocks): Sequential(\n     (0): TCNBlock(\n       (conv1): Conv1d(9, 64, kernel_size=(3,), stride=(1,), padding=same)\n       (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (shortcut): Conv1d(9, 64, kernel_size=(1,), stride=(1,))\n     )\n     (1): TCNBlock(\n       (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))\n       (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (shortcut): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n     )\n     (2): TCNBlock(\n       (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=same, dilation=(4,))\n       (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (shortcut): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n     )\n     (3): TCNBlock(\n       (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same, dilation=(8,))\n       (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     )\n   )\n   (cbam_block): CBAMBlock(\n     (channel_attention): ChannelAttention(\n       (avg_pool): AdaptiveAvgPool1d(output_size=1)\n       (max_pool): AdaptiveMaxPool1d(output_size=1)\n       (fc1): Conv1d(256, 32, kernel_size=(1,), stride=(1,))\n       (fc2): Conv1d(32, 256, kernel_size=(1,), stride=(1,))\n     )\n     (spatial_attention): SpatialAttention(\n       (conv): Conv1d(2, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n       (sigmoid): Sigmoid()\n     )\n   )\n   (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n   (fc1): Linear(in_features=256, out_features=128, bias=True)\n   (fc2): Linear(in_features=128, out_features=64, bias=True)\n   (fc3): Linear(in_features=64, out_features=6, bias=True)\n   (dropout): Dropout(p=0.5, inplace=False)\n ))\n\n\n\ndef calculate_ensemble_accuracy(models, test_loader, num_classes, device='cuda'):\n    correct = 0\n    total = 0\n\n    models = [model.to(device).eval() for model in models]  # Ensure all models are in eval mode\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            # Move data to the appropriate device\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Get ensemble predictions\n            avg_probs = torch.zeros((inputs.size(0), num_classes)).to(device)\n            for model in models:\n                outputs = model(inputs)\n                \n                avg_probs += outputs\n            avg_probs /= len(models)\n            preds = avg_probs.argmax(dim=1)\n\n            # Update accuracy\n            correct += (preds == targets.argmax(1)).sum().item()\n            total += targets.size(0)\n\n    # Calculate overall accuracy\n    accuracy = correct / total\n    print(f\"Ensemble Test Accuracy: {accuracy * 100:.2f}%\")\n    return accuracy\n\n\ncalculate_ensemble_accuracy([best_model], test_loader, num_classes=6, device=device)\n\nEnsemble Test Accuracy: 96.06%\n\n\n0.9606379368849678\n\n\n\ncalculate_ensemble_accuracy([model], test_loader, num_classes=6, device=device)\n\nEnsemble Test Accuracy: 94.77%\n\n\n0.9477434679334917"
  },
  {
    "objectID": "posts/uci-har-pytorch.html#log",
    "href": "posts/uci-har-pytorch.html#log",
    "title": "UCI-HAR Classification",
    "section": "Log",
    "text": "Log\n\n\n\n\n\n\nRNN (180k) + Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9606; Val Acc: 0.9192\nTCN (56k) + KL Div + Adam + StepLR -&gt; Train Acc: 0.9698; Val Acc: 0.9355\nTCN (56k) + KL Div + Adam + ConsineAnnealineLR -&gt; Train Acc: 0.9921; Val Acc: 0.9382\nTCN (250k) + KL Div + Adam + ConsineAnnealingLR -&gt; Train Acc: 0.9871; Val Acc: 0.9484\nTCN (50k) + CBAM + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9891; Val Acc: 0.9450\nTCN (400k) + CBAM + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9970; Val Acc: 0.9545\n\n\n\nAll below this use Early Stopping + Retraining\n\n\n\nNormalized + TCN (400k) + CBAM + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9931; Val Acc: 0.9498\nNormalized + Noise + TCN (400k) + CBAM + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9962; Val Acc: 0.9511\nNormalized + Noise + TCN (284k) + Self Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9977; Val Acc: 0.9427\nNormalized + Noise + TCN (539k) + Self Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9944; Val Acc: 0.9569\nNormalized + Noise + TCN (Dropouts(p=0.5) + BatchNorm) (539k) + Self Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9932; Val Acc: 0.9623\nNormalized + Noise + TCN (Dropouts(p=0.7) + BatchNorm) (539k) + Self Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9950; Val Acc: 0.9545\nModel model with loss func 0.8*kl_div + 0.2*label_smoothing(0.1) -&gt; Train Acc: 0.9875; Val Acc: 0.9559\nNormalized + Noise + RNN (180k) + Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9929; Val Acc: 0.9545\nNormalized + Noise + RNN (622k) + Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9963; Val Acc: 0.9746\n\ntorch.cuda.memory_summary()\n\n'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  22754 KiB |    783 MiB |  12921 GiB |  12921 GiB |\\n|       from large pool |  17792 KiB |    778 MiB |  12853 GiB |  12853 GiB |\\n|       from small pool |   4962 KiB |      8 MiB |     68 GiB |     68 GiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  22754 KiB |    783 MiB |  12921 GiB |  12921 GiB |\\n|       from large pool |  17792 KiB |    778 MiB |  12853 GiB |  12853 GiB |\\n|       from small pool |   4962 KiB |      8 MiB |     68 GiB |     68 GiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  22743 KiB |    782 MiB |  12907 GiB |  12907 GiB |\\n|       from large pool |  17792 KiB |    776 MiB |  12839 GiB |  12839 GiB |\\n|       from small pool |   4951 KiB |      8 MiB |     68 GiB |     68 GiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   1224 MiB |   1224 MiB |   1224 MiB |      0 B   |\\n|       from large pool |   1214 MiB |   1214 MiB |   1214 MiB |      0 B   |\\n|       from small pool |     10 MiB |     10 MiB |     10 MiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   5918 KiB | 552547 KiB |   5176 GiB |   5176 GiB |\\n|       from large pool |   2688 KiB | 548479 KiB |   5103 GiB |   5103 GiB |\\n|       from small pool |   3230 KiB |   5388 KiB |     73 GiB |     73 GiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     112    |     136    |     817 K  |     816 K  |\\n|       from large pool |       3    |      11    |     128 K  |     128 K  |\\n|       from small pool |     109    |     133    |     688 K  |     688 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     112    |     136    |     817 K  |     816 K  |\\n|       from large pool |       3    |      11    |     128 K  |     128 K  |\\n|       from small pool |     109    |     133    |     688 K  |     688 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      10    |      10    |      10    |       0    |\\n|       from large pool |       5    |       5    |       5    |       0    |\\n|       from small pool |       5    |       5    |       5    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      21    |      25    |  360638    |  360617    |\\n|       from large pool |       2    |       8    |   74884    |   74882    |\\n|       from small pool |      19    |      21    |  285754    |  285735    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'\n\n\n\ntorch.cuda.empty_cache()"
  }
]