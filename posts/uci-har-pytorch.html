<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rishit Jakharia">
<meta name="dcterms.date" content="2025-03-07">

<title>UCI-HAR Classification – Rishit Jakharia</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-74dcb147988b4ae4a0ad54b8cb899171.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-d7160f61fa1e3a6a36e7fb36d47de69e.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Rishit Jakharia</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../files/CV.pdf"> 
<span class="menu-text">CV</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#loading-dataset" id="toc-loading-dataset" class="nav-link active" data-scroll-target="#loading-dataset">Loading Dataset</a>
  <ul class="collapse">
  <li><a href="#setting-up-the-train-and-test-loaders" id="toc-setting-up-the-train-and-test-loaders" class="nav-link" data-scroll-target="#setting-up-the-train-and-test-loaders">Setting Up the train and test loaders</a></li>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a></li>
  </ul></li>
  <li><a href="#modelling" id="toc-modelling" class="nav-link" data-scroll-target="#modelling">Modelling</a>
  <ul class="collapse">
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">Loss Function</a></li>
  <li><a href="#optimizer-and-learning-rate" id="toc-optimizer-and-learning-rate" class="nav-link" data-scroll-target="#optimizer-and-learning-rate">Optimizer and Learning Rate</a></li>
  <li><a href="#plotting-train-and-test-acc-and-loss" id="toc-plotting-train-and-test-acc-and-loss" class="nav-link" data-scroll-target="#plotting-train-and-test-acc-and-loss">Plotting Train and Test Acc and Loss</a></li>
  <li><a href="#confusion-matrix-function" id="toc-confusion-matrix-function" class="nav-link" data-scroll-target="#confusion-matrix-function">Confusion Matrix Function</a></li>
  <li><a href="#training-function" id="toc-training-function" class="nav-link" data-scroll-target="#training-function">Training Function</a></li>
  <li><a href="#models" id="toc-models" class="nav-link" data-scroll-target="#models">Models</a>
  <ul class="collapse">
  <li><a href="#rnn-attention" id="toc-rnn-attention" class="nav-link" data-scroll-target="#rnn-attention">RNN + Attention</a></li>
  <li><a href="#tcn-model" id="toc-tcn-model" class="nav-link" data-scroll-target="#tcn-model">TCN Model</a></li>
  <li><a href="#tcn-cbam-model" id="toc-tcn-cbam-model" class="nav-link" data-scroll-target="#tcn-cbam-model">TCN + CBAM Model</a></li>
  <li><a href="#tcn-self-attention" id="toc-tcn-self-attention" class="nav-link" data-scroll-target="#tcn-self-attention">TCN + Self Attention</a></li>
  <li><a href="#ensemble-of-models" id="toc-ensemble-of-models" class="nav-link" data-scroll-target="#ensemble-of-models">Ensemble of Models</a></li>
  </ul></li>
  <li><a href="#log" id="toc-log" class="nav-link" data-scroll-target="#log">Log</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">UCI-HAR Classification</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ML</div>
    <div class="quarto-category">Time Series</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Rishit Jakharia </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 7, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>::: {#cell-1 .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2024-12-13T05:50:55.402561Z”,“iopub.status.busy”:“2024-12-13T05:50:55.401962Z”,“iopub.status.idle”:“2024-12-13T05:50:55.407609Z”,“shell.execute_reply”:“2024-12-13T05:50:55.406519Z”,“shell.execute_reply.started”:“2024-12-13T05:50:55.402528Z”}’ trusted=‘true’ execution_count=3}</p>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset, Dataset</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, ConfusionMatrixDisplay</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> read_csv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<div id="cell-2" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:55.429584Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:55.428981Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:55.436781Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:55.436069Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:55.429555Z&quot;}" data-trusted="true" data-execution_count="4">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>seed_value <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>random.seed(seed_value)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_value)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed_value)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>device</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># torch.use_deterministic_algorithms(True)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>'cuda'</code></pre>
</div>
</div>
<section id="loading-dataset" class="level1">
<h1>Loading Dataset</h1>
<div id="cell-4" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:55.748600Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:55.747906Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:57.130042Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:57.129254Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:55.748568Z&quot;}" data-trusted="true" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> pd.read_csv(<span class="st">'/kaggle/input/har-using-deep-nn/train.csv'</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>train.sample()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">tBodyAcc_mean_X</th>
<th data-quarto-table-cell-role="th">tBodyAcc_mean_Y</th>
<th data-quarto-table-cell-role="th">tBodyAcc_mean_Z</th>
<th data-quarto-table-cell-role="th">tBodyAcc_std_X</th>
<th data-quarto-table-cell-role="th">tBodyAcc_std_Y</th>
<th data-quarto-table-cell-role="th">tBodyAcc_std_Z</th>
<th data-quarto-table-cell-role="th">tBodyAcc_mad_X</th>
<th data-quarto-table-cell-role="th">tBodyAcc_mad_Y</th>
<th data-quarto-table-cell-role="th">tBodyAcc_mad_Z</th>
<th data-quarto-table-cell-role="th">tBodyAcc_max_X</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">angletBodyAccMeangravity</th>
<th data-quarto-table-cell-role="th">angletBodyAccJerkMeangravityMean</th>
<th data-quarto-table-cell-role="th">angletBodyGyroMeangravityMean</th>
<th data-quarto-table-cell-role="th">angletBodyGyroJerkMeangravityMean</th>
<th data-quarto-table-cell-role="th">angleXgravityMean</th>
<th data-quarto-table-cell-role="th">angleYgravityMean</th>
<th data-quarto-table-cell-role="th">angleZgravityMean</th>
<th data-quarto-table-cell-role="th">subject</th>
<th data-quarto-table-cell-role="th">Activity</th>
<th data-quarto-table-cell-role="th">ActivityName</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">4525</td>
<td>0.283203</td>
<td>-0.047024</td>
<td>-0.168986</td>
<td>0.384949</td>
<td>0.176898</td>
<td>-0.310332</td>
<td>0.381757</td>
<td>0.122611</td>
<td>-0.332984</td>
<td>0.465563</td>
<td>...</td>
<td>-0.034924</td>
<td>0.558036</td>
<td>0.258975</td>
<td>-0.854858</td>
<td>-0.78433</td>
<td>0.22296</td>
<td>-0.066506</td>
<td>22</td>
<td>3</td>
<td>WALKING_DOWNSTAIRS</td>
</tr>
</tbody>
</table>

<p>1 rows × 564 columns</p>
</div>
</div>
</div>
<div id="cell-5" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:57.132327Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:57.131958Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:57.137902Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:57.136939Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:57.132288Z&quot;}" data-trusted="true" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>train.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>(7352, 564)</code></pre>
</div>
</div>
<div id="cell-6" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:57.139293Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:57.139026Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:57.662236Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:57.661226Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:57.139269Z&quot;}" data-trusted="true" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> pd.read_csv(<span class="st">'/kaggle/input/har-using-deep-nn/test.csv'</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>test.sample()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">tBodyAcc_mean_X</th>
<th data-quarto-table-cell-role="th">tBodyAcc_mean_Y</th>
<th data-quarto-table-cell-role="th">tBodyAcc_mean_Z</th>
<th data-quarto-table-cell-role="th">tBodyAcc_std_X</th>
<th data-quarto-table-cell-role="th">tBodyAcc_std_Y</th>
<th data-quarto-table-cell-role="th">tBodyAcc_std_Z</th>
<th data-quarto-table-cell-role="th">tBodyAcc_mad_X</th>
<th data-quarto-table-cell-role="th">tBodyAcc_mad_Y</th>
<th data-quarto-table-cell-role="th">tBodyAcc_mad_Z</th>
<th data-quarto-table-cell-role="th">tBodyAcc_max_X</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">angletBodyAccMeangravity</th>
<th data-quarto-table-cell-role="th">angletBodyAccJerkMeangravityMean</th>
<th data-quarto-table-cell-role="th">angletBodyGyroMeangravityMean</th>
<th data-quarto-table-cell-role="th">angletBodyGyroJerkMeangravityMean</th>
<th data-quarto-table-cell-role="th">angleXgravityMean</th>
<th data-quarto-table-cell-role="th">angleYgravityMean</th>
<th data-quarto-table-cell-role="th">angleZgravityMean</th>
<th data-quarto-table-cell-role="th">subject</th>
<th data-quarto-table-cell-role="th">Activity</th>
<th data-quarto-table-cell-role="th">ActivityName</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">718</td>
<td>0.3659</td>
<td>-0.031332</td>
<td>-0.024079</td>
<td>0.183421</td>
<td>-0.344568</td>
<td>-0.403268</td>
<td>0.09989</td>
<td>-0.3422</td>
<td>-0.412337</td>
<td>0.4803</td>
<td>...</td>
<td>-0.37837</td>
<td>-0.840289</td>
<td>-0.864326</td>
<td>0.848585</td>
<td>-0.960939</td>
<td>0.116422</td>
<td>-0.003858</td>
<td>9</td>
<td>3</td>
<td>WALKING_DOWNSTAIRS</td>
</tr>
</tbody>
</table>

<p>1 rows × 564 columns</p>
</div>
</div>
</div>
<div id="cell-7" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:57.664361Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:57.664075Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:57.670099Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:57.669282Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:57.664334Z&quot;}" data-trusted="true" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>test.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(2947, 564)</code></pre>
</div>
</div>
<div id="cell-8" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:57.671355Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:57.671129Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:58.110765Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:58.109900Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:57.671334Z&quot;}" data-trusted="true" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Duplicates in train = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="bu">sum</span>(train.duplicated())))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Duplicates in test = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="bu">sum</span>(test.duplicated())))</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Invalid values in train = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(train.isnull().values.<span class="bu">sum</span>()))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Invalid values in test = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(test.isnull().values.<span class="bu">sum</span>()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Duplicates in train = 0
Duplicates in test = 0
Invalid values in train = 0
Invalid values in test = 0</code></pre>
</div>
</div>
<div id="cell-9" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:58.112210Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:58.111939Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:58.782039Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:58.781095Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:58.112179Z&quot;}" data-trusted="true" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">30</span>,<span class="dv">10</span>))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>sns.countplot(x<span class="op">=</span><span class="st">'subject'</span>,hue<span class="op">=</span><span class="st">'ActivityName'</span>,palette<span class="op">=</span> [<span class="st">"#7fcdbb"</span>,<span class="st">"#fdf824"</span>,<span class="st">"#30c6f0"</span>,<span class="st">"#fc9264"</span>,<span class="st">"#755d6b"</span>,<span class="st">"#9a4ad4"</span>], data <span class="op">=</span> train)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Data Provided by Users in Train Set'</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="uci-har-pytorch_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-10" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:58.783988Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:58.783304Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:59.268161Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:59.267162Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:58.783948Z&quot;}" data-trusted="true" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">30</span>,<span class="dv">10</span>))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>sns.countplot(x<span class="op">=</span><span class="st">'subject'</span>,hue<span class="op">=</span><span class="st">'ActivityName'</span>,palette<span class="op">=</span> [<span class="st">"#7fcdbb"</span>,<span class="st">"#fdf824"</span>,<span class="st">"#30c6f0"</span>,<span class="st">"#fc9264"</span>,<span class="st">"#755d6b"</span>,<span class="st">"#9a4ad4"</span>], data <span class="op">=</span> test)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Data Provided by Users in Test Set'</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="uci-har-pytorch_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-11" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:59.270488Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:59.270199Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:59.454284Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:59.453380Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:59.270461Z&quot;}" data-trusted="true" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Count of Activities in Total (Train)'</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>sns.countplot(x<span class="op">=</span><span class="st">"ActivityName"</span>,data<span class="op">=</span>train)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="uci-har-pytorch_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-12" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:59.456016Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:59.455747Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:59.614869Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:59.614046Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:59.455991Z&quot;}" data-trusted="true" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Count of Activities in Total (Test)'</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>sns.countplot(x<span class="op">=</span><span class="st">"ActivityName"</span>,data<span class="op">=</span>test)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="uci-har-pytorch_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-13" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:59.616254Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:59.615884Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:59.621570Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:59.620492Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:59.616214Z&quot;}" data-trusted="true" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> file_load(filepath):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> read_csv(filepath, header<span class="op">=</span><span class="va">None</span>, sep<span class="op">=</span><span class="st">'\s+'</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df.values</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-14" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:59.622842Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:59.622495Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:59.631962Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:59.631074Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:59.622805Z&quot;}" data-trusted="true" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_test_append(filenames, append_before<span class="op">=</span><span class="st">''</span>):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    datalist <span class="op">=</span> <span class="bu">list</span>()</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name <span class="kw">in</span> filenames:</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> file_load(append_before <span class="op">+</span> name)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        datalist.append(data)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    datalist <span class="op">=</span> np.dstack(datalist)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> datalist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-15" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:59.634881Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:59.634152Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:59.643799Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:59.642856Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:59.634834Z&quot;}" data-trusted="true" data-execution_count="16">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inertial_signals_load(group, append_before<span class="op">=</span><span class="st">''</span>):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    filepath <span class="op">=</span> append_before <span class="op">+</span> group <span class="op">+</span> <span class="st">'/Inertial Signals/'</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    filenames <span class="op">=</span> <span class="bu">list</span>()</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    filenames <span class="op">+=</span> [<span class="st">'total_acc_x_'</span><span class="op">+</span>group<span class="op">+</span><span class="st">'.txt'</span>, <span class="st">'total_acc_y_'</span><span class="op">+</span>group<span class="op">+</span><span class="st">'.txt'</span>, <span class="st">'total_acc_z_'</span><span class="op">+</span>group<span class="op">+</span><span class="st">'.txt'</span>]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    filenames <span class="op">+=</span> [<span class="st">'body_acc_x_'</span><span class="op">+</span>group<span class="op">+</span><span class="st">'.txt'</span>, <span class="st">'body_acc_y_'</span><span class="op">+</span>group<span class="op">+</span><span class="st">'.txt'</span>, <span class="st">'body_acc_z_'</span><span class="op">+</span>group<span class="op">+</span><span class="st">'.txt'</span>]</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    filenames <span class="op">+=</span> [<span class="st">'body_gyro_x_'</span><span class="op">+</span>group<span class="op">+</span><span class="st">'.txt'</span>, <span class="st">'body_gyro_y_'</span><span class="op">+</span>group<span class="op">+</span><span class="st">'.txt'</span>, <span class="st">'body_gyro_z_'</span><span class="op">+</span>group<span class="op">+</span><span class="st">'.txt'</span>]</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> train_test_append(filenames, filepath)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> file_load(append_before <span class="op">+</span> group <span class="op">+</span> <span class="st">'/y_'</span><span class="op">+</span>group<span class="op">+</span><span class="st">'.txt'</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-16" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:50:59.645996Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:50:59.645505Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:50:59.655132Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:50:59.654291Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:50:59.645949Z&quot;}" data-trusted="true" data-execution_count="17">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_dataset(append_before<span class="op">=</span><span class="st">''</span>):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    trainX, trainy <span class="op">=</span> inertial_signals_load(<span class="st">'train'</span>, append_before <span class="op">+</span> <span class="st">'/kaggle/input/ucihar-dataset/UCI-HAR Dataset/'</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    testX, testy <span class="op">=</span> inertial_signals_load(<span class="st">'test'</span>, append_before <span class="op">+</span> <span class="st">'/kaggle/input/ucihar-dataset/UCI-HAR Dataset/'</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    trainy <span class="op">=</span> trainy <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    testy <span class="op">=</span> testy <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    trainy <span class="op">=</span> pd.get_dummies(trainy[:,<span class="dv">0</span>], dtype<span class="op">=</span><span class="bu">int</span>).values</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    testy <span class="op">=</span> pd.get_dummies(testy[:,<span class="dv">0</span>], dtype<span class="op">=</span><span class="bu">int</span>).values</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(trainX.shape, trainy.shape, testX.shape, testy.shape)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trainX, trainy, testX, testy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-17" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:51:00.014061Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:51:00.013387Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:51:05.011449Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:51:05.010608Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:51:00.014030Z&quot;}" data-trusted="true" data-execution_count="18">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>trainX, trainy, testX, testy <span class="op">=</span> load_dataset()</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>verbose, epochs, batch_size <span class="op">=</span> <span class="dv">0</span>, <span class="dv">100</span>, <span class="dv">200</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>n_timesteps <span class="op">=</span> trainX.shape[<span class="dv">1</span>]</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>n_features <span class="op">=</span> trainX.shape[<span class="dv">2</span>]</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>n_outputs <span class="op">=</span> trainy.shape[<span class="dv">1</span>]</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co"># n_steps = 4</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># n_length = 32</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features)) </span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(trainX.shape,testX.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)
(7352, 128, 9) (2947, 128, 9)</code></pre>
</div>
</div>
<section id="setting-up-the-train-and-test-loaders" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-the-train-and-test-loaders">Setting Up the train and test loaders</h2>
<div id="cell-19" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:51:57.441264Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:51:57.440804Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:51:57.447766Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:51:57.446925Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:51:57.441224Z&quot;}" data-trusted="true" data-execution_count="26">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HARDataset(Dataset):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, X, y):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.X <span class="op">=</span> torch.tensor(X, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y <span class="op">=</span> torch.tensor(y, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.X)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.X[idx], <span class="va">self</span>.y[idx]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-20" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:51:58.587015Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:51:58.586314Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:51:58.615411Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:51:58.614609Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:51:58.586982Z&quot;}" data-trusted="true" data-execution_count="27">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> HARDataset(trainX, trainy)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> HARDataset(testX, testy)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>BATCH <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span>BATCH, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span>BATCH, shuffle<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="preprocessing">Preprocessing</h2>
<div id="cell-22" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:52:00.016627Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:52:00.015933Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:52:00.105428Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:52:00.104627Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:52:00.016595Z&quot;}" data-trusted="true" data-execution_count="28">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> torch.zeros(<span class="dv">128</span>, <span class="dv">9</span>)  <span class="co"># Shape (128, 9)</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>sum_sq <span class="op">=</span> torch.zeros(<span class="dv">128</span>, <span class="dv">9</span>)  <span class="co"># Shape (128, 9)</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>total_elements <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch, _ <span class="kw">in</span> train_loader:</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> batch.size(<span class="dv">0</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">+=</span> batch.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    sum_sq <span class="op">+=</span> (batch <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    total_elements <span class="op">+=</span> batch_size</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>mean <span class="op">/=</span> total_elements  <span class="co"># Shape (128, 9)</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> torch.sqrt(sum_sq <span class="op">/</span> total_elements <span class="op">-</span> mean <span class="op">**</span> <span class="dv">2</span>)  <span class="co"># Shape (128, 9)</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="co"># print("Mean:", mean)</span></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="co"># print("Standard Deviation:", std)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-23" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:52:00.425346Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:52:00.424513Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:52:00.431192Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:52:00.430295Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:52:00.425311Z&quot;}" data-trusted="true" data-execution_count="29">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Normalize:</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, mean, std, device<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mean <span class="op">=</span> mean.to(device) <span class="cf">if</span> device <span class="cf">else</span> mean</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.std <span class="op">=</span> std.to(device) <span class="cf">if</span> device <span class="cf">else</span> std</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, sample):</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(sample, torch.Tensor):</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>            sample <span class="op">=</span> torch.tensor(sample, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        normalized_sample <span class="op">=</span> (sample <span class="op">-</span> <span class="va">self</span>.mean) <span class="op">/</span> <span class="va">self</span>.std</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> normalized_sample</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AddNoise:</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, noise_level<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.noise_level <span class="op">=</span> noise_level</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, sample):</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        noise <span class="op">=</span> torch.randn_like(sample) <span class="op">*</span> <span class="va">self</span>.noise_level</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sample <span class="op">+</span> noise</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-24" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:52:00.778372Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:52:00.778057Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:52:00.788934Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:52:00.788175Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:52:00.778348Z&quot;}" data-trusted="true" data-execution_count="30">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DatasetWithAugmentation(Dataset):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data, target, transforms<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.data <span class="op">=</span> data</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target <span class="op">=</span> target</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transforms <span class="op">=</span> transforms</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> <span class="va">self</span>.data[idx]</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> <span class="va">self</span>.target[idx]</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transforms:</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> transform <span class="kw">in</span> <span class="va">self</span>.transforms:</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>                sample <span class="op">=</span> transform(sample)</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sample, target</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>normalize_transform <span class="op">=</span> Normalize(torch.Tensor(mean), torch.Tensor(std))</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>add_noise_transform <span class="op">=</span> AddNoise(noise_level<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>transforms <span class="op">=</span> [normalize_transform, add_noise_transform]</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a><span class="co"># transforms = [normalize_transform]</span></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> DatasetWithAugmentation(trainX, trainy, transforms<span class="op">=</span>transforms)</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> DatasetWithAugmentation(testX, testy, transforms<span class="op">=</span>transforms)</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span>BATCH, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span>BATCH, shuffle<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="modelling" class="level1">
<h1>Modelling</h1>
<section id="loss-function" class="level2">
<h2 class="anchored" data-anchor-id="loss-function">Loss Function</h2>
<div id="cell-26" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:52:02.918057Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:52:02.917245Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:52:02.924851Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:52:02.923908Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:52:02.918023Z&quot;}" data-trusted="true" data-execution_count="31">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kl_divergence_loss(preds, targets):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> targets.<span class="bu">float</span>()</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> F.softmax(preds, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> F.softmax(targets, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.kl_div(preds.log(), targets, reduction<span class="op">=</span><span class="st">'batchmean'</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss(preds, targets):</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ensure that the predictions and targets are one-hot encoded</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> F.softmax(preds, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F.mse_loss(preds, targets)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Focal Loss</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> focal_loss(preds, targets, alpha<span class="op">=</span><span class="fl">0.25</span>, gamma<span class="op">=</span><span class="fl">2.0</span>):</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Focal Loss for multi-class classification.</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a><span class="co">    preds -- the raw logits from the model (shape: [batch_size, num_classes])</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a><span class="co">    targets -- the true labels (shape: [batch_size])</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a><span class="co">    alpha -- balancing factor for class imbalances (default is 0.25)</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a><span class="co">    gamma -- focusing parameter (default is 2.0)</span></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a><span class="co">    loss -- the computed focal loss</span></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply softmax to get probabilities</span></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> F.softmax(preds, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert targets to one-hot encoding</span></span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>    targets_one_hot <span class="op">=</span> targets</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cross-entropy loss</span></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>    ce_loss <span class="op">=</span> F.cross_entropy(preds, targets, reduction<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the predicted probability for the correct class</span></span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>    pt <span class="op">=</span> torch.exp(<span class="op">-</span>ce_loss)</span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute focal loss</span></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>    focal_loss <span class="op">=</span> alpha <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> pt) <span class="op">**</span> gamma <span class="op">*</span> ce_loss</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> focal_loss.mean()</span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a><span class="co"># criterion = nn.CrossEntropyLoss() # -&gt; 96.89% 93.55%</span></span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a><span class="co"># criterion = kl_divergence_loss # -&gt; 99.09% 94.81%</span></span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a><span class="co"># criterion = mse_loss # -&gt; 96.27% 92.94%</span></span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a><span class="co"># criterion = nn.CrossEntropyLoss(label_smoothing=0.1) # -&gt; 97.43% 94.06%</span></span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a><span class="co"># criterion = focal_loss # -&gt; 97.27% 93.32%</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="optimizer-and-learning-rate" class="level2">
<h2 class="anchored" data-anchor-id="optimizer-and-learning-rate">Optimizer and Learning Rate</h2>
<div id="cell-28" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:52:03.932353Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:52:03.931999Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:52:03.937360Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:52:03.936437Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:52:03.932323Z&quot;}" data-trusted="true" data-execution_count="32">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> warmup_lr(epoch, step_size, warmup_epochs<span class="op">=</span><span class="dv">5</span>, gamma<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">&lt;</span> warmup_epochs:</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (epoch <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> warmup_epochs</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        epoch_since_warmup <span class="op">=</span> epoch <span class="op">-</span> warmup_epochs</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> gamma <span class="op">**</span> (epoch_since_warmup <span class="op">//</span> step_size)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>WARMUP_EPOCHS <span class="op">=</span> <span class="dv">14</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>GAMMA <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>STEP_SIZE <span class="op">=</span> <span class="dv">20</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="plotting-train-and-test-acc-and-loss" class="level2">
<h2 class="anchored" data-anchor-id="plotting-train-and-test-acc-and-loss">Plotting Train and Test Acc and Loss</h2>
<div id="cell-30" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:52:04.433086Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:52:04.432305Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:52:04.439041Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:52:04.438105Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:52:04.433052Z&quot;}" data-trusted="true" data-execution_count="33">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot history</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_history(history):</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    epochs <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(history[<span class="st">'train_loss'</span>]) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loss plot</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    plt.plot(epochs, history[<span class="st">'train_loss'</span>], label<span class="op">=</span><span class="st">'Train Loss'</span>)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    plt.plot(epochs, history[<span class="st">'val_loss'</span>], label<span class="op">=</span><span class="st">'Validation Loss'</span>)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Loss History'</span>)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Accuracy plot</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>    plt.plot(epochs, history[<span class="st">'train_acc'</span>], label<span class="op">=</span><span class="st">'Train Accuracy'</span>)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>    plt.plot(epochs, history[<span class="st">'val_acc'</span>], label<span class="op">=</span><span class="st">'Validation Accuracy'</span>)</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Accuracy History'</span>)</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="confusion-matrix-function" class="level2">
<h2 class="anchored" data-anchor-id="confusion-matrix-function">Confusion Matrix Function</h2>
<div id="cell-32" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:52:04.885520Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:52:04.884657Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:52:04.892762Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:52:04.891804Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:52:04.885485Z&quot;}" data-trusted="true" data-execution_count="34">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_confusion_matrix(model, val_loader, class_names, device<span class="op">=</span><span class="st">"cuda"</span>):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Evaluates the model and plots a confusion matrix with a custom color map.</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co">    - model: Trained PyTorch model.</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co">    - val_loader: DataLoader for validation dataset.</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co">    - class_names: List of class names for the confusion matrix.</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co">    - device: 'cuda' or 'cpu'.</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()  <span class="co"># Set model to evaluation mode</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>    all_preds <span class="op">=</span> []</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>    all_labels <span class="op">=</span> []</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():  <span class="co"># Disable gradient computation</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> inputs, labels <span class="kw">in</span> val_loader:</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>            inputs, labels <span class="op">=</span> inputs.to(device), labels.to(device)</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(inputs)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> torch.argmax(outputs, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># Predicted classes</span></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If labels are one-hot encoded, convert them to class indices</span></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(labels.shape) <span class="op">&gt;</span> <span class="dv">1</span> <span class="kw">and</span> labels.size(<span class="dv">1</span>) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>                labels <span class="op">=</span> torch.argmax(labels, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>            all_preds.extend(preds.cpu().numpy())</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>            all_labels.extend(labels.cpu().numpy())</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute confusion matrix</span></span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>    cm <span class="op">=</span> confusion_matrix(all_labels, all_preds, labels<span class="op">=</span>np.arange(<span class="bu">len</span>(class_names)))</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>    disp <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm, display_labels<span class="op">=</span>class_names)</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the confusion matrix with customization</span></span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))  <span class="co"># Set larger figure size</span></span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>    disp.plot(cmap<span class="op">=</span><span class="st">"YlGnBu"</span>, ax<span class="op">=</span>ax, colorbar<span class="op">=</span><span class="va">True</span>)  <span class="co"># Use "cividis" colormap and add colorbar</span></span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Confusion Matrix"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>    plt.xticks(fontsize<span class="op">=</span><span class="dv">12</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a>    plt.yticks(fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-function" class="level2">
<h2 class="anchored" data-anchor-id="training-function">Training Function</h2>
<div id="cell-34" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:52:06.692549Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:52:06.692164Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:52:06.706036Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:52:06.705044Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:52:06.692521Z&quot;}" data-trusted="true" data-execution_count="35">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(model, train_loader, val_loader, criterion<span class="op">=</span><span class="va">None</span>, optimizer<span class="op">=</span><span class="va">None</span>, scheduler<span class="op">=</span><span class="va">None</span>, epochs<span class="op">=</span><span class="dv">20</span>, device<span class="op">=</span><span class="st">'cpu'</span>, early_stopping<span class="op">=</span><span class="va">False</span>, save_path<span class="op">=</span><span class="st">'best_model.pth'</span>):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> {<span class="st">'train_loss'</span>: [], <span class="st">'val_loss'</span>: [], <span class="st">'train_acc'</span>: [], <span class="st">'val_acc'</span>: []}</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    patience <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    min_delta <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    best_loss <span class="op">=</span> np.inf</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    best_acc <span class="op">=</span> <span class="op">-</span>np.inf</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    patience_counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    best_model_state <span class="op">=</span> <span class="va">None</span>  <span class="co"># Store the best model in memory</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> criterion <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>        criterion <span class="op">=</span> F.mse_loss</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    _criterion <span class="op">=</span> criterion</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>        train_loss, train_correct <span class="op">=</span> <span class="fl">0.0</span>, <span class="dv">0</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training phase</span></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tqdm(train_loader, desc<span class="op">=</span><span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss"> - Training"</span>, unit<span class="op">=</span><span class="st">"batch"</span>) <span class="im">as</span> tepoch:</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> inputs, targets <span class="kw">in</span> tepoch:</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>                inputs <span class="op">=</span> inputs.to(device)</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>                targets <span class="op">=</span> targets.to(device)</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>                optimizer.zero_grad()</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> model(inputs)</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> _criterion(outputs, targets)</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>                loss.backward()</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>                optimizer.step()</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>                train_loss <span class="op">+=</span> loss.item() <span class="op">*</span> inputs.size(<span class="dv">0</span>)</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>                train_correct <span class="op">+=</span> (outputs.argmax(<span class="dv">1</span>) <span class="op">==</span> targets.argmax(<span class="dv">1</span>)).<span class="bu">sum</span>().item()</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>                tepoch.set_postfix(loss<span class="op">=</span>loss.item())</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scheduler <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>            scheduler.step()</span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">/=</span> <span class="bu">len</span>(train_loader.dataset)</span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">=</span> train_correct <span class="op">/</span> <span class="bu">len</span>(train_loader.dataset)</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validation phase</span></span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a>        val_loss, val_correct <span class="op">=</span> <span class="fl">0.0</span>, <span class="dv">0</span></span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> tqdm(val_loader, desc<span class="op">=</span><span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss"> - Validation"</span>, unit<span class="op">=</span><span class="st">"batch"</span>) <span class="im">as</span> vepoch:</span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> inputs, targets <span class="kw">in</span> vepoch:</span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a>                    inputs <span class="op">=</span> inputs.to(device)</span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a>                    targets <span class="op">=</span> targets.to(device)</span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a>                    outputs <span class="op">=</span> model(inputs)</span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">=</span> criterion(outputs, targets)</span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a>                    val_loss <span class="op">+=</span> loss.item() <span class="op">*</span> inputs.size(<span class="dv">0</span>)</span>
<span id="cb31-54"><a href="#cb31-54" aria-hidden="true" tabindex="-1"></a>                    val_correct <span class="op">+=</span> (outputs.argmax(<span class="dv">1</span>) <span class="op">==</span> targets.argmax(<span class="dv">1</span>)).<span class="bu">sum</span>().item()</span>
<span id="cb31-55"><a href="#cb31-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-56"><a href="#cb31-56" aria-hidden="true" tabindex="-1"></a>                    vepoch.set_postfix(loss<span class="op">=</span>loss.item())</span>
<span id="cb31-57"><a href="#cb31-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-58"><a href="#cb31-58" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">/=</span> <span class="bu">len</span>(val_loader.dataset)</span>
<span id="cb31-59"><a href="#cb31-59" aria-hidden="true" tabindex="-1"></a>        val_acc <span class="op">=</span> val_correct <span class="op">/</span> <span class="bu">len</span>(val_loader.dataset)</span>
<span id="cb31-60"><a href="#cb31-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-61"><a href="#cb31-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save history</span></span>
<span id="cb31-62"><a href="#cb31-62" aria-hidden="true" tabindex="-1"></a>        history[<span class="st">'train_loss'</span>].append(train_loss)</span>
<span id="cb31-63"><a href="#cb31-63" aria-hidden="true" tabindex="-1"></a>        history[<span class="st">'val_loss'</span>].append(val_loss)</span>
<span id="cb31-64"><a href="#cb31-64" aria-hidden="true" tabindex="-1"></a>        history[<span class="st">'train_acc'</span>].append(train_acc)</span>
<span id="cb31-65"><a href="#cb31-65" aria-hidden="true" tabindex="-1"></a>        history[<span class="st">'val_acc'</span>].append(val_acc)</span>
<span id="cb31-66"><a href="#cb31-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-67"><a href="#cb31-67" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss"> - "</span></span>
<span id="cb31-68"><a href="#cb31-68" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f"Train Loss: </span><span class="sc">{</span>train_loss<span class="sc">:.4f}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_acc<span class="sc">:.4f}</span><span class="ss"> - "</span></span>
<span id="cb31-69"><a href="#cb31-69" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f"Val Loss: </span><span class="sc">{</span>val_loss<span class="sc">:.4f}</span><span class="ss">, Val Acc: </span><span class="sc">{</span>val_acc<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb31-70"><a href="#cb31-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-71"><a href="#cb31-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the best model in memory</span></span>
<span id="cb31-72"><a href="#cb31-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> best_acc <span class="op">&lt;</span> val_acc <span class="op">-</span> min_delta:</span>
<span id="cb31-73"><a href="#cb31-73" aria-hidden="true" tabindex="-1"></a>            best_loss <span class="op">=</span> val_loss</span>
<span id="cb31-74"><a href="#cb31-74" aria-hidden="true" tabindex="-1"></a>            best_acc <span class="op">=</span> val_acc</span>
<span id="cb31-75"><a href="#cb31-75" aria-hidden="true" tabindex="-1"></a>            patience_counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb31-76"><a href="#cb31-76" aria-hidden="true" tabindex="-1"></a>            best_model_state <span class="op">=</span> model.state_dict()</span>
<span id="cb31-77"><a href="#cb31-77" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"New best model found at epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> with val_loss: </span><span class="sc">{</span>val_loss<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb31-78"><a href="#cb31-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb31-79"><a href="#cb31-79" aria-hidden="true" tabindex="-1"></a>            patience_counter <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb31-80"><a href="#cb31-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-81"><a href="#cb31-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Early stopping</span></span>
<span id="cb31-82"><a href="#cb31-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> patience_counter <span class="op">&gt;=</span> patience <span class="kw">and</span> early_stopping <span class="kw">and</span> epoch <span class="op">&gt;</span> <span class="dv">100</span>:</span>
<span id="cb31-83"><a href="#cb31-83" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Early stopping triggered!"</span>)</span>
<span id="cb31-84"><a href="#cb31-84" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb31-85"><a href="#cb31-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-86"><a href="#cb31-86" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save the best model at the end of training</span></span>
<span id="cb31-87"><a href="#cb31-87" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> best_model_state <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb31-88"><a href="#cb31-88" aria-hidden="true" tabindex="-1"></a>        torch.save(best_model_state, save_path)</span>
<span id="cb31-89"><a href="#cb31-89" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Best model saved to </span><span class="sc">{</span>save_path<span class="sc">}</span><span class="ss"> with val_loss: </span><span class="sc">{</span>best_loss<span class="sc">:.4f}</span><span class="ss"> and val_acc: </span><span class="sc">{</span>best_acc<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb31-90"><a href="#cb31-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-91"><a href="#cb31-91" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> history</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="models" class="level2">
<h2 class="anchored" data-anchor-id="models">Models</h2>
<section id="rnn-attention" class="level3">
<h3 class="anchored" data-anchor-id="rnn-attention">RNN + Attention</h3>
<div id="cell-36" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:55:42.379385Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:55:42.379032Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:55:42.389976Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:55:42.388990Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:55:42.379355Z&quot;}" data-trusted="true" data-execution_count="45">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AdaptiveRNN(nn.Module):</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_timesteps, n_features, n_outputs):</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(AdaptiveRNN, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Bi-directional LSTM layer</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lstm <span class="op">=</span> nn.LSTM(</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>            input_size<span class="op">=</span>n_features,</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>            hidden_size<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>            bidirectional<span class="op">=</span><span class="va">True</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_norm <span class="op">=</span> nn.BatchNorm1d(<span class="dv">256</span> <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_norm1 <span class="op">=</span> nn.BatchNorm1d(<span class="dv">128</span>)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attention mechanism</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_dense <span class="op">=</span> nn.Linear(<span class="dv">256</span> <span class="op">*</span> <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fully connected layers</span></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">256</span> <span class="op">*</span> <span class="dv">2</span>, <span class="dv">128</span>)</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, n_outputs)</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.5</span>)</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># LSTM output</span></span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>        lstm_out, _ <span class="op">=</span> <span class="va">self</span>.lstm(x)  <span class="co"># lstm_out shape: (batch_size, timesteps, 128*2)</span></span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>        batch_size, n_timesteps, n_features <span class="op">=</span> lstm_out.size()</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>        lstm_out <span class="op">=</span> lstm_out.reshape(batch_size <span class="op">*</span> n_timesteps, n_features)</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>        lstm_out <span class="op">=</span> <span class="va">self</span>.batch_norm(lstm_out)</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>        lstm_out <span class="op">=</span> lstm_out.reshape(batch_size, n_timesteps, n_features)</span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attention mechanism</span></span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>        attention_scores <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.attention_dense(lstm_out))  <span class="co"># (batch_size, timesteps, 1)</span></span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>        attention_scores <span class="op">=</span> lstm_out <span class="op">*</span> attention_scores  <span class="co"># Weighted timesteps</span></span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global average pooling</span></span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>        pooled <span class="op">=</span> attention_scores.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fully connected layers</span></span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(pooled))</span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.batch_norm1(x)</span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.softmax(x, dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-37" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:55:43.818446Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:55:43.817605Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:55:43.835083Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:55:43.834243Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:55:43.818411Z&quot;}" data-trusted="true" data-execution_count="46">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model parameters</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>n_timesteps <span class="op">=</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">4</span>  <span class="co"># 128 timesteps</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>n_features <span class="op">=</span> <span class="dv">9</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>n_outputs <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AdaptiveRNN(n_timesteps<span class="op">=</span>n_timesteps, n_features<span class="op">=</span>n_features, n_outputs<span class="op">=</span>n_outputs).to(device)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>summary(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
AdaptiveRNN                              --
├─LSTM: 1-1                              546,816
├─BatchNorm1d: 1-2                       1,024
├─BatchNorm1d: 1-3                       256
├─Linear: 1-4                            513
├─Linear: 1-5                            65,664
├─Linear: 1-6                            8,256
├─Linear: 1-7                            390
├─Dropout: 1-8                           --
=================================================================
Total params: 622,919
Trainable params: 622,919
Non-trainable params: 0
=================================================================</code></pre>
</div>
</div>
<div id="cell-38" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:00:24.911545Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:00:24.911198Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:04:01.792011Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:04:01.791056Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:00:24.911514Z&quot;}" data-trusted="true" data-execution_count="51">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max<span class="op">=</span><span class="dv">50</span>, eta_min<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="co"># loss_fn = lambda inputs, targets: 0.9*kl_divergence_loss(inputs, targets) + 0.1*nn.CrossEntropyLoss()(inputs.float(), targets.float())</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> train_model(model, train_loader, test_loader, optimizer<span class="op">=</span>optimizer, criterion<span class="op">=</span>kl_divergence_loss, epochs<span class="op">=</span><span class="dv">150</span>, device<span class="op">=</span>device, early_stopping<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.51batch/s, loss=0.00351] 
Epoch 1/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.06batch/s, loss=-7.78e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/150 - Train Loss: 0.0020, Train Acc: 0.9879 - Val Loss: 0.0067, Val Acc: 0.9600
New best model found at epoch 1 with val_loss: 0.0067</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.53batch/s, loss=0.00333] 
Epoch 2/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.61batch/s, loss=5.67e-7]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/150 - Train Loss: 0.0016, Train Acc: 0.9917 - Val Loss: 0.0069, Val Acc: 0.9617
New best model found at epoch 2 with val_loss: 0.0069</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.70batch/s, loss=2.36e-5] 
Epoch 3/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.19batch/s, loss=-5.85e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/150 - Train Loss: 0.0014, Train Acc: 0.9921 - Val Loss: 0.0056, Val Acc: 0.9678
New best model found at epoch 3 with val_loss: 0.0056</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 4/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.92batch/s, loss=0.00196] 
Epoch 4/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.51batch/s, loss=7.7e-7] </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 4/150 - Train Loss: 0.0013, Train Acc: 0.9927 - Val Loss: 0.0075, Val Acc: 0.9593</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 5/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.81batch/s, loss=0.00115] 
Epoch 5/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.43batch/s, loss=5.62e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 5/150 - Train Loss: 0.0011, Train Acc: 0.9939 - Val Loss: 0.0068, Val Acc: 0.9613</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 6/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.65batch/s, loss=0.000549]
Epoch 6/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.41batch/s, loss=4.15e-5]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 6/150 - Train Loss: 0.0013, Train Acc: 0.9929 - Val Loss: 0.0080, Val Acc: 0.9559</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 7/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.51batch/s, loss=0.000857]
Epoch 7/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.76batch/s, loss=-6.1e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 7/150 - Train Loss: 0.0013, Train Acc: 0.9929 - Val Loss: 0.0072, Val Acc: 0.9596</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 8/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.48batch/s, loss=0.000464]
Epoch 8/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.35batch/s, loss=-3.55e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 8/150 - Train Loss: 0.0016, Train Acc: 0.9908 - Val Loss: 0.0059, Val Acc: 0.9667</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 9/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.50batch/s, loss=0.00216] 
Epoch 9/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.94batch/s, loss=-7.62e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9/150 - Train Loss: 0.0016, Train Acc: 0.9905 - Val Loss: 0.0077, Val Acc: 0.9579</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 10/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.65batch/s, loss=0.00201] 
Epoch 10/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.08batch/s, loss=-7.14e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 10/150 - Train Loss: 0.0016, Train Acc: 0.9903 - Val Loss: 0.0068, Val Acc: 0.9627</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 11/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.61batch/s, loss=0.0029]  
Epoch 11/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.43batch/s, loss=-7.94e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 11/150 - Train Loss: 0.0016, Train Acc: 0.9914 - Val Loss: 0.0070, Val Acc: 0.9596</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 12/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.60batch/s, loss=0.00197] 
Epoch 12/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.39batch/s, loss=-7.51e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 12/150 - Train Loss: 0.0015, Train Acc: 0.9914 - Val Loss: 0.0073, Val Acc: 0.9576</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 13/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.68batch/s, loss=0.00155] 
Epoch 13/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.26batch/s, loss=-5.12e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 13/150 - Train Loss: 0.0015, Train Acc: 0.9909 - Val Loss: 0.0075, Val Acc: 0.9559</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 14/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.63batch/s, loss=0.00081] 
Epoch 14/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.66batch/s, loss=-4.35e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 14/150 - Train Loss: 0.0017, Train Acc: 0.9908 - Val Loss: 0.0069, Val Acc: 0.9640</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 15/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=0.00115] 
Epoch 15/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.27batch/s, loss=-7.58e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 15/150 - Train Loss: 0.0015, Train Acc: 0.9916 - Val Loss: 0.0069, Val Acc: 0.9603</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 16/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.55batch/s, loss=0.00192] 
Epoch 16/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.24batch/s, loss=-6.37e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 16/150 - Train Loss: 0.0011, Train Acc: 0.9931 - Val Loss: 0.0074, Val Acc: 0.9606</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 17/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.000373]
Epoch 17/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.37batch/s, loss=-8.53e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 17/150 - Train Loss: 0.0009, Train Acc: 0.9947 - Val Loss: 0.0076, Val Acc: 0.9589</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 18/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.32batch/s, loss=0.000373]
Epoch 18/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.32batch/s, loss=-7.1e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 18/150 - Train Loss: 0.0006, Train Acc: 0.9966 - Val Loss: 0.0070, Val Acc: 0.9613</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 19/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=0.00177] 
Epoch 19/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.62batch/s, loss=-4.96e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 19/150 - Train Loss: 0.0011, Train Acc: 0.9943 - Val Loss: 0.0063, Val Acc: 0.9657</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 20/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.61batch/s, loss=0.000316]
Epoch 20/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.87batch/s, loss=-5.76e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 20/150 - Train Loss: 0.0011, Train Acc: 0.9935 - Val Loss: 0.0074, Val Acc: 0.9586</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 21/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.74batch/s, loss=0.00199] 
Epoch 21/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.15batch/s, loss=-4.48e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 21/150 - Train Loss: 0.0014, Train Acc: 0.9918 - Val Loss: 0.0071, Val Acc: 0.9620</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 22/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.33batch/s, loss=0.000371]
Epoch 22/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.22batch/s, loss=5.24e-6]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 22/150 - Train Loss: 0.0012, Train Acc: 0.9940 - Val Loss: 0.0076, Val Acc: 0.9572</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 23/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.59batch/s, loss=0.00077] 
Epoch 23/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.96batch/s, loss=3.36e-7]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 23/150 - Train Loss: 0.0007, Train Acc: 0.9963 - Val Loss: 0.0079, Val Acc: 0.9583</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 24/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.00174] 
Epoch 24/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.99batch/s, loss=1.71e-7]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 24/150 - Train Loss: 0.0010, Train Acc: 0.9936 - Val Loss: 0.0086, Val Acc: 0.9539</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 25/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.46batch/s, loss=0.000475]
Epoch 25/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.97batch/s, loss=-5.07e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 25/150 - Train Loss: 0.0011, Train Acc: 0.9933 - Val Loss: 0.0076, Val Acc: 0.9606</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 26/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.63batch/s, loss=0.00139] 
Epoch 26/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.86batch/s, loss=-6.98e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 26/150 - Train Loss: 0.0009, Train Acc: 0.9942 - Val Loss: 0.0066, Val Acc: 0.9644</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 27/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.66batch/s, loss=0.000834]
Epoch 27/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.40batch/s, loss=3.82e-7]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 27/150 - Train Loss: 0.0006, Train Acc: 0.9961 - Val Loss: 0.0065, Val Acc: 0.9630</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 28/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.63batch/s, loss=0.000493]
Epoch 28/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.29batch/s, loss=3.3e-7] </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 28/150 - Train Loss: 0.0005, Train Acc: 0.9969 - Val Loss: 0.0063, Val Acc: 0.9657</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 29/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.64batch/s, loss=6.04e-5] 
Epoch 29/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.35batch/s, loss=-5.55e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 29/150 - Train Loss: 0.0008, Train Acc: 0.9955 - Val Loss: 0.0063, Val Acc: 0.9637</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 30/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.52batch/s, loss=4.53e-5] 
Epoch 30/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.64batch/s, loss=-7.37e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 30/150 - Train Loss: 0.0006, Train Acc: 0.9963 - Val Loss: 0.0049, Val Acc: 0.9746
New best model found at epoch 30 with val_loss: 0.0049</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 31/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.68batch/s, loss=0.00561] 
Epoch 31/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.49batch/s, loss=-8.55e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 31/150 - Train Loss: 0.0010, Train Acc: 0.9936 - Val Loss: 0.0064, Val Acc: 0.9640</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 32/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.71batch/s, loss=4.76e-5] 
Epoch 32/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.04batch/s, loss=3.81e-6]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 32/150 - Train Loss: 0.0010, Train Acc: 0.9944 - Val Loss: 0.0057, Val Acc: 0.9678</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 33/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.60batch/s, loss=0.000509]
Epoch 33/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.62batch/s, loss=3.32e-6]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 33/150 - Train Loss: 0.0009, Train Acc: 0.9950 - Val Loss: 0.0071, Val Acc: 0.9637</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 34/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.66batch/s, loss=0.000317]
Epoch 34/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 33.93batch/s, loss=2.69e-6]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 34/150 - Train Loss: 0.0007, Train Acc: 0.9961 - Val Loss: 0.0055, Val Acc: 0.9705</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 35/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.90batch/s, loss=0.000529]
Epoch 35/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.84batch/s, loss=-6.92e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 35/150 - Train Loss: 0.0004, Train Acc: 0.9980 - Val Loss: 0.0054, Val Acc: 0.9718</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 36/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.98batch/s, loss=0.000713]
Epoch 36/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.35batch/s, loss=-5.32e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 36/150 - Train Loss: 0.0007, Train Acc: 0.9959 - Val Loss: 0.0067, Val Acc: 0.9640</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 37/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.22batch/s, loss=5.37e-6] 
Epoch 37/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.02batch/s, loss=0.000543]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 37/150 - Train Loss: 0.0008, Train Acc: 0.9956 - Val Loss: 0.0078, Val Acc: 0.9589</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 38/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.54batch/s, loss=0.000445]
Epoch 38/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.16batch/s, loss=-6.55e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 38/150 - Train Loss: 0.0008, Train Acc: 0.9955 - Val Loss: 0.0081, Val Acc: 0.9583</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 39/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.51batch/s, loss=0.0011]  
Epoch 39/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.17batch/s, loss=-6.8e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 39/150 - Train Loss: 0.0009, Train Acc: 0.9946 - Val Loss: 0.0069, Val Acc: 0.9650</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 40/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.66batch/s, loss=6.38e-5] 
Epoch 40/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.23batch/s, loss=1.47e-7]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 40/150 - Train Loss: 0.0009, Train Acc: 0.9942 - Val Loss: 0.0088, Val Acc: 0.9545</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 41/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.69batch/s, loss=4.41e-6] 
Epoch 41/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.26batch/s, loss=-4e-8]  </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 41/150 - Train Loss: 0.0009, Train Acc: 0.9951 - Val Loss: 0.0071, Val Acc: 0.9630</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 42/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.000219]
Epoch 42/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.40batch/s, loss=-2.66e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 42/150 - Train Loss: 0.0010, Train Acc: 0.9944 - Val Loss: 0.0082, Val Acc: 0.9586</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 43/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.75batch/s, loss=0.00267] 
Epoch 43/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.62batch/s, loss=-5.12e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 43/150 - Train Loss: 0.0006, Train Acc: 0.9969 - Val Loss: 0.0072, Val Acc: 0.9613</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 44/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.61batch/s, loss=0.000231]
Epoch 44/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.59batch/s, loss=-3.55e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 44/150 - Train Loss: 0.0006, Train Acc: 0.9965 - Val Loss: 0.0081, Val Acc: 0.9576</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 45/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.71batch/s, loss=0.00116] 
Epoch 45/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.59batch/s, loss=-4.39e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 45/150 - Train Loss: 0.0005, Train Acc: 0.9971 - Val Loss: 0.0079, Val Acc: 0.9593</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 46/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.45batch/s, loss=0.000188]
Epoch 46/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.96batch/s, loss=-4.32e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 46/150 - Train Loss: 0.0005, Train Acc: 0.9970 - Val Loss: 0.0084, Val Acc: 0.9566</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 47/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.64batch/s, loss=0.00144] 
Epoch 47/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.53batch/s, loss=-4.32e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 47/150 - Train Loss: 0.0008, Train Acc: 0.9954 - Val Loss: 0.0077, Val Acc: 0.9562</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 48/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.66batch/s, loss=8.87e-5] 
Epoch 48/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.12batch/s, loss=-4.64e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 48/150 - Train Loss: 0.0006, Train Acc: 0.9965 - Val Loss: 0.0082, Val Acc: 0.9552</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 49/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.51batch/s, loss=4.17e-5] 
Epoch 49/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.93batch/s, loss=-5.14e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 49/150 - Train Loss: 0.0005, Train Acc: 0.9974 - Val Loss: 0.0088, Val Acc: 0.9542</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 50/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.71batch/s, loss=0.000807]
Epoch 50/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.09batch/s, loss=-4.73e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 50/150 - Train Loss: 0.0004, Train Acc: 0.9974 - Val Loss: 0.0084, Val Acc: 0.9562</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 51/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.70batch/s, loss=0.000145]
Epoch 51/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.10batch/s, loss=-5.19e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 51/150 - Train Loss: 0.0004, Train Acc: 0.9976 - Val Loss: 0.0089, Val Acc: 0.9535</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 52/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.45batch/s, loss=0.000894]
Epoch 52/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.81batch/s, loss=-3.78e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 52/150 - Train Loss: 0.0006, Train Acc: 0.9962 - Val Loss: 0.0076, Val Acc: 0.9613</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 53/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.000404]
Epoch 53/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.37batch/s, loss=-3.96e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 53/150 - Train Loss: 0.0004, Train Acc: 0.9976 - Val Loss: 0.0085, Val Acc: 0.9569</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 54/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.52batch/s, loss=2.88e-5] 
Epoch 54/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.75batch/s, loss=-3.98e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 54/150 - Train Loss: 0.0005, Train Acc: 0.9974 - Val Loss: 0.0089, Val Acc: 0.9552</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 55/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.54batch/s, loss=0.000136]
Epoch 55/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.03batch/s, loss=-1.84e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 55/150 - Train Loss: 0.0007, Train Acc: 0.9962 - Val Loss: 0.0081, Val Acc: 0.9566</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 56/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=0.000108]
Epoch 56/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.28batch/s, loss=-4.3e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 56/150 - Train Loss: 0.0004, Train Acc: 0.9977 - Val Loss: 0.0086, Val Acc: 0.9569</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 57/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.000356]
Epoch 57/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.51batch/s, loss=-4.87e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 57/150 - Train Loss: 0.0006, Train Acc: 0.9966 - Val Loss: 0.0104, Val Acc: 0.9460</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 58/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.58batch/s, loss=0.000594]
Epoch 58/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.76batch/s, loss=-4.12e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 58/150 - Train Loss: 0.0010, Train Acc: 0.9944 - Val Loss: 0.0099, Val Acc: 0.9498</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 59/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.69batch/s, loss=0.000857]
Epoch 59/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.54batch/s, loss=-3.87e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 59/150 - Train Loss: 0.0008, Train Acc: 0.9952 - Val Loss: 0.0094, Val Acc: 0.9518</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 60/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.77batch/s, loss=8.44e-6] 
Epoch 60/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.19batch/s, loss=-5.19e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 60/150 - Train Loss: 0.0006, Train Acc: 0.9965 - Val Loss: 0.0085, Val Acc: 0.9549</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 61/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.72batch/s, loss=4.92e-6] 
Epoch 61/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.05batch/s, loss=-5.57e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 61/150 - Train Loss: 0.0004, Train Acc: 0.9977 - Val Loss: 0.0095, Val Acc: 0.9501</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 62/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.70batch/s, loss=0.00234] 
Epoch 62/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.37batch/s, loss=-3.62e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 62/150 - Train Loss: 0.0011, Train Acc: 0.9944 - Val Loss: 0.0096, Val Acc: 0.9522</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 63/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.61batch/s, loss=2.66e-5] 
Epoch 63/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.70batch/s, loss=-6.48e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 63/150 - Train Loss: 0.0004, Train Acc: 0.9976 - Val Loss: 0.0105, Val Acc: 0.9484</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 64/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.68batch/s, loss=0.000539]
Epoch 64/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.62batch/s, loss=-7.01e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 64/150 - Train Loss: 0.0007, Train Acc: 0.9961 - Val Loss: 0.0080, Val Acc: 0.9600</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 65/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.73batch/s, loss=0.000833]
Epoch 65/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.48batch/s, loss=-7.03e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 65/150 - Train Loss: 0.0003, Train Acc: 0.9984 - Val Loss: 0.0090, Val Acc: 0.9545</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 66/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.59batch/s, loss=5.6e-6]  
Epoch 66/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.36batch/s, loss=-5.26e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 66/150 - Train Loss: 0.0004, Train Acc: 0.9980 - Val Loss: 0.0080, Val Acc: 0.9555</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 67/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.63batch/s, loss=1.87e-5] 
Epoch 67/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.40batch/s, loss=-6.05e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 67/150 - Train Loss: 0.0002, Train Acc: 0.9988 - Val Loss: 0.0078, Val Acc: 0.9579</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 68/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.59batch/s, loss=0.00122] 
Epoch 68/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.19batch/s, loss=-5.73e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 68/150 - Train Loss: 0.0004, Train Acc: 0.9981 - Val Loss: 0.0092, Val Acc: 0.9528</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 69/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=0.00141] 
Epoch 69/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.61batch/s, loss=-5.28e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 69/150 - Train Loss: 0.0010, Train Acc: 0.9940 - Val Loss: 0.0091, Val Acc: 0.9542</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 70/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.64batch/s, loss=9.04e-6] 
Epoch 70/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.44batch/s, loss=-5.19e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 70/150 - Train Loss: 0.0008, Train Acc: 0.9956 - Val Loss: 0.0082, Val Acc: 0.9593</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 71/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.57batch/s, loss=0.000645]
Epoch 71/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.02batch/s, loss=-2.66e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 71/150 - Train Loss: 0.0006, Train Acc: 0.9958 - Val Loss: 0.0075, Val Acc: 0.9613</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 72/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.59batch/s, loss=0.00379] 
Epoch 72/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.84batch/s, loss=-5.62e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 72/150 - Train Loss: 0.0010, Train Acc: 0.9944 - Val Loss: 0.0075, Val Acc: 0.9593</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 73/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.64batch/s, loss=9.18e-5] 
Epoch 73/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.90batch/s, loss=-5.16e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 73/150 - Train Loss: 0.0011, Train Acc: 0.9932 - Val Loss: 0.0077, Val Acc: 0.9600</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 74/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.69batch/s, loss=0.00232] 
Epoch 74/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.80batch/s, loss=-3.57e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 74/150 - Train Loss: 0.0010, Train Acc: 0.9936 - Val Loss: 0.0079, Val Acc: 0.9586</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 75/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.17batch/s, loss=0.000876]
Epoch 75/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.91batch/s, loss=-4.05e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 75/150 - Train Loss: 0.0009, Train Acc: 0.9948 - Val Loss: 0.0095, Val Acc: 0.9491</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 76/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.69batch/s, loss=6.31e-6] 
Epoch 76/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.40batch/s, loss=-4.46e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 76/150 - Train Loss: 0.0006, Train Acc: 0.9966 - Val Loss: 0.0089, Val Acc: 0.9545</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 77/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.45batch/s, loss=0.00198] 
Epoch 77/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.25batch/s, loss=-4.73e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 77/150 - Train Loss: 0.0006, Train Acc: 0.9962 - Val Loss: 0.0088, Val Acc: 0.9566</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 78/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.00113] 
Epoch 78/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.11batch/s, loss=-6.37e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 78/150 - Train Loss: 0.0005, Train Acc: 0.9967 - Val Loss: 0.0088, Val Acc: 0.9555</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 79/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=0.000288]
Epoch 79/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.86batch/s, loss=-5.78e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 79/150 - Train Loss: 0.0005, Train Acc: 0.9971 - Val Loss: 0.0080, Val Acc: 0.9576</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 80/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.70batch/s, loss=0.00184] 
Epoch 80/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.47batch/s, loss=-7.85e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 80/150 - Train Loss: 0.0005, Train Acc: 0.9974 - Val Loss: 0.0090, Val Acc: 0.9535</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 81/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.41batch/s, loss=0.000388]
Epoch 81/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 29.96batch/s, loss=3.44e-7] </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 81/150 - Train Loss: 0.0009, Train Acc: 0.9952 - Val Loss: 0.0096, Val Acc: 0.9491</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 82/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.28batch/s, loss=0.000401]
Epoch 82/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.90batch/s, loss=-6.53e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 82/150 - Train Loss: 0.0005, Train Acc: 0.9974 - Val Loss: 0.0093, Val Acc: 0.9532</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 83/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.69batch/s, loss=0.000519]
Epoch 83/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.48batch/s, loss=6.79e-6]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 83/150 - Train Loss: 0.0007, Train Acc: 0.9954 - Val Loss: 0.0078, Val Acc: 0.9617</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 84/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.55batch/s, loss=1.21e-5] 
Epoch 84/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.65batch/s, loss=-4.07e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 84/150 - Train Loss: 0.0005, Train Acc: 0.9974 - Val Loss: 0.0073, Val Acc: 0.9630</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 85/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.66batch/s, loss=0.000208]
Epoch 85/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.97batch/s, loss=-5.41e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 85/150 - Train Loss: 0.0002, Train Acc: 0.9985 - Val Loss: 0.0076, Val Acc: 0.9623</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 86/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.56batch/s, loss=0.000375]
Epoch 86/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.83batch/s, loss=-6.07e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 86/150 - Train Loss: 0.0002, Train Acc: 0.9985 - Val Loss: 0.0071, Val Acc: 0.9634</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 87/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.59batch/s, loss=5.34e-5] 
Epoch 87/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.50batch/s, loss=-6.32e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 87/150 - Train Loss: 0.0002, Train Acc: 0.9990 - Val Loss: 0.0071, Val Acc: 0.9650</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 88/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.71batch/s, loss=0.00266] 
Epoch 88/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.23batch/s, loss=-6.98e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 88/150 - Train Loss: 0.0004, Train Acc: 0.9973 - Val Loss: 0.0059, Val Acc: 0.9691</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 89/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.71batch/s, loss=7.01e-5] 
Epoch 89/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.29batch/s, loss=-7.96e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 89/150 - Train Loss: 0.0005, Train Acc: 0.9974 - Val Loss: 0.0073, Val Acc: 0.9627</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 90/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=0.000948]
Epoch 90/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.57batch/s, loss=-7.62e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 90/150 - Train Loss: 0.0003, Train Acc: 0.9982 - Val Loss: 0.0077, Val Acc: 0.9600</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 91/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.63batch/s, loss=5.78e-6] 
Epoch 91/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.20batch/s, loss=-7.87e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 91/150 - Train Loss: 0.0004, Train Acc: 0.9980 - Val Loss: 0.0069, Val Acc: 0.9644</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 92/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.64batch/s, loss=5.95e-6] 
Epoch 92/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.32batch/s, loss=-6.23e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 92/150 - Train Loss: 0.0002, Train Acc: 0.9988 - Val Loss: 0.0071, Val Acc: 0.9617</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 93/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=5.12e-5] 
Epoch 93/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.94batch/s, loss=-5.91e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 93/150 - Train Loss: 0.0003, Train Acc: 0.9981 - Val Loss: 0.0074, Val Acc: 0.9596</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 94/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=0.000534]
Epoch 94/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.36batch/s, loss=-4.64e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 94/150 - Train Loss: 0.0003, Train Acc: 0.9982 - Val Loss: 0.0076, Val Acc: 0.9617</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 95/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.63batch/s, loss=3.7e-5]  
Epoch 95/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.41batch/s, loss=-6.3e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 95/150 - Train Loss: 0.0002, Train Acc: 0.9988 - Val Loss: 0.0093, Val Acc: 0.9542</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 96/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.31batch/s, loss=3.83e-5] 
Epoch 96/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.22batch/s, loss=-4.66e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 96/150 - Train Loss: 0.0004, Train Acc: 0.9981 - Val Loss: 0.0083, Val Acc: 0.9572</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 97/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.62batch/s, loss=9.67e-5] 
Epoch 97/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.03batch/s, loss=-6.76e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 97/150 - Train Loss: 0.0006, Train Acc: 0.9963 - Val Loss: 0.0078, Val Acc: 0.9593</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 98/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.70batch/s, loss=9.24e-7] 
Epoch 98/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.52batch/s, loss=-5.53e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 98/150 - Train Loss: 0.0003, Train Acc: 0.9984 - Val Loss: 0.0081, Val Acc: 0.9589</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 99/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.59batch/s, loss=5.79e-5] 
Epoch 99/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 32.43batch/s, loss=-6.73e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 99/150 - Train Loss: 0.0002, Train Acc: 0.9990 - Val Loss: 0.0080, Val Acc: 0.9610</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 100/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.56batch/s, loss=1.08e-7] 
Epoch 100/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 30.99batch/s, loss=-6.69e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 100/150 - Train Loss: 0.0002, Train Acc: 0.9992 - Val Loss: 0.0073, Val Acc: 0.9647</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 101/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.67batch/s, loss=9.35e-6] 
Epoch 101/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.78batch/s, loss=-5.69e-8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 101/150 - Train Loss: 0.0001, Train Acc: 0.9995 - Val Loss: 0.0074, Val Acc: 0.9613</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 102/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 16.66batch/s, loss=0.00114] 
Epoch 102/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 31.79batch/s, loss=2.98e-7]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 102/150 - Train Loss: 0.0003, Train Acc: 0.9986 - Val Loss: 0.0074, Val Acc: 0.9627
Early stopping triggered!
Best model saved to best_model.pth with val_loss: 0.0049 and val_acc: 0.9746</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
</div>
<div id="cell-39" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:04:04.396167Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:04:04.395813Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:04:04.791475Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:04:04.790383Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:04:04.396136Z&quot;}" data-trusted="true" data-execution_count="52">
<div class="sourceCode cell-code" id="cb241"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb241-1"><a href="#cb241-1" aria-hidden="true" tabindex="-1"></a>plot_history(history)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="uci-har-pytorch_files/figure-html/cell-31-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-40" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:04:07.926863Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:04:07.925987Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:04:08.300527Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:04:08.299613Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:04:07.926829Z&quot;}" data-trusted="true" data-execution_count="53">
<div class="sourceCode cell-code" id="cb242"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb242-1"><a href="#cb242-1" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> AdaptiveRNN(n_timesteps<span class="op">=</span>n_timesteps, n_features<span class="op">=</span>n_features, n_outputs<span class="op">=</span>n_outputs).to(device)</span>
<span id="cb242-2"><a href="#cb242-2" aria-hidden="true" tabindex="-1"></a>best_model.load_state_dict(torch.load(<span class="st">'best_model.pth'</span>, weights_only <span class="op">=</span> <span class="va">True</span>))</span>
<span id="cb242-3"><a href="#cb242-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb242-4"><a href="#cb242-4" aria-hidden="true" tabindex="-1"></a>val_correct <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb242-5"><a href="#cb242-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb242-6"><a href="#cb242-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="bu">input</span>, target <span class="kw">in</span> test_loader:</span>
<span id="cb242-7"><a href="#cb242-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span> <span class="op">=</span> <span class="bu">input</span>.to(device)</span>
<span id="cb242-8"><a href="#cb242-8" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> target.to(device)</span>
<span id="cb242-9"><a href="#cb242-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb242-10"><a href="#cb242-10" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> best_model(<span class="bu">input</span>)</span>
<span id="cb242-11"><a href="#cb242-11" aria-hidden="true" tabindex="-1"></a>        val_correct <span class="op">+=</span> (outputs.argmax(<span class="dv">1</span>) <span class="op">==</span> target.argmax(<span class="dv">1</span>)).<span class="bu">sum</span>().item()</span>
<span id="cb242-12"><a href="#cb242-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"val_acc: </span><span class="sc">{</span>val_correct <span class="op">/</span> <span class="bu">len</span>(test_loader.dataset)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>val_acc: 0.9613165931455717</code></pre>
</div>
</div>
<div id="cell-41" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:04:12.892837Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:04:12.892101Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:04:13.601867Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:04:13.601046Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:04:12.892805Z&quot;}" data-trusted="true" data-execution_count="54">
<div class="sourceCode cell-code" id="cb244"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb244-1"><a href="#cb244-1" aria-hidden="true" tabindex="-1"></a>plot_confusion_matrix(best_model, test_loader, class_names<span class="op">=</span>[<span class="st">"WALKING"</span>, <span class="st">"WALKING_UPSTAIRS"</span>, <span class="st">"WALKING_DOWNSTAIRS"</span>, <span class="st">"SITTING"</span>, <span class="st">"STANDING"</span>, <span class="st">"LAYING"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="uci-har-pytorch_files/figure-html/cell-33-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="tcn-model" class="level3">
<h3 class="anchored" data-anchor-id="tcn-model">TCN Model</h3>
<div id="cell-43" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:04:46.220182Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:04:46.219800Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:04:46.231412Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:04:46.230480Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:04:46.220152Z&quot;}" data-trusted="true" data-execution_count="55">
<div class="sourceCode cell-code" id="cb245"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb245-1"><a href="#cb245-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TCNBlock(nn.Module):</span>
<span id="cb245-2"><a href="#cb245-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, kernel_size, dilation_rate):</span>
<span id="cb245-3"><a href="#cb245-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(TCNBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb245-4"><a href="#cb245-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-5"><a href="#cb245-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv1d(</span>
<span id="cb245-6"><a href="#cb245-6" aria-hidden="true" tabindex="-1"></a>            in_channels,</span>
<span id="cb245-7"><a href="#cb245-7" aria-hidden="true" tabindex="-1"></a>            out_channels,</span>
<span id="cb245-8"><a href="#cb245-8" aria-hidden="true" tabindex="-1"></a>            kernel_size,</span>
<span id="cb245-9"><a href="#cb245-9" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="st">'same'</span>,</span>
<span id="cb245-10"><a href="#cb245-10" aria-hidden="true" tabindex="-1"></a>            dilation<span class="op">=</span>dilation_rate</span>
<span id="cb245-11"><a href="#cb245-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb245-12"><a href="#cb245-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_norm <span class="op">=</span> nn.BatchNorm1d(out_channels)</span>
<span id="cb245-13"><a href="#cb245-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-14"><a href="#cb245-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Residual connection</span></span>
<span id="cb245-15"><a href="#cb245-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> in_channels <span class="op">!=</span> out_channels:</span>
<span id="cb245-16"><a href="#cb245-16" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.shortcut <span class="op">=</span> nn.Conv1d(in_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb245-17"><a href="#cb245-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb245-18"><a href="#cb245-18" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.shortcut <span class="op">=</span> <span class="va">None</span></span>
<span id="cb245-19"><a href="#cb245-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-20"><a href="#cb245-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb245-21"><a href="#cb245-21" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> F.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb245-22"><a href="#cb245-22" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.batch_norm(out)</span>
<span id="cb245-23"><a href="#cb245-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-24"><a href="#cb245-24" aria-hidden="true" tabindex="-1"></a>        shortcut <span class="op">=</span> <span class="va">self</span>.shortcut(x) <span class="cf">if</span> <span class="va">self</span>.shortcut <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> x</span>
<span id="cb245-25"><a href="#cb245-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.relu(out <span class="op">+</span> shortcut)</span>
<span id="cb245-26"><a href="#cb245-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-27"><a href="#cb245-27" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TCNModel(nn.Module):</span>
<span id="cb245-28"><a href="#cb245-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_length, n_features, n_outputs):</span>
<span id="cb245-29"><a href="#cb245-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(TCNModel, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb245-30"><a href="#cb245-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-31"><a href="#cb245-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tcn_blocks <span class="op">=</span> nn.Sequential(</span>
<span id="cb245-32"><a href="#cb245-32" aria-hidden="true" tabindex="-1"></a>            TCNBlock(n_features, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb245-33"><a href="#cb245-33" aria-hidden="true" tabindex="-1"></a>            TCNBlock(<span class="dv">64</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb245-34"><a href="#cb245-34" aria-hidden="true" tabindex="-1"></a>            TCNBlock(<span class="dv">128</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb245-35"><a href="#cb245-35" aria-hidden="true" tabindex="-1"></a>            TCNBlock(<span class="dv">256</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb245-36"><a href="#cb245-36" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb245-37"><a href="#cb245-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-38"><a href="#cb245-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.global_avg_pool <span class="op">=</span> nn.AdaptiveAvgPool1d(<span class="dv">1</span>)</span>
<span id="cb245-39"><a href="#cb245-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">128</span>)</span>
<span id="cb245-40"><a href="#cb245-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>)</span>
<span id="cb245-41"><a href="#cb245-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, n_outputs)</span>
<span id="cb245-42"><a href="#cb245-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-43"><a href="#cb245-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.3</span>)</span>
<span id="cb245-44"><a href="#cb245-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-45"><a href="#cb245-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb245-46"><a href="#cb245-46" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)  <span class="co"># Convert to (batch_size, n_features, n_length)</span></span>
<span id="cb245-47"><a href="#cb245-47" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.tcn_blocks(x)</span>
<span id="cb245-48"><a href="#cb245-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-49"><a href="#cb245-49" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.global_avg_pool(x).squeeze(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># Global average pooling</span></span>
<span id="cb245-50"><a href="#cb245-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-51"><a href="#cb245-51" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb245-52"><a href="#cb245-52" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb245-53"><a href="#cb245-53" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb245-54"><a href="#cb245-54" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb245-55"><a href="#cb245-55" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb245-56"><a href="#cb245-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-57"><a href="#cb245-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.softmax(x, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb245-58"><a href="#cb245-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-59"><a href="#cb245-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Define model parameters</span></span>
<span id="cb245-60"><a href="#cb245-60" aria-hidden="true" tabindex="-1"></a>n_length <span class="op">=</span> <span class="dv">128</span>  </span>
<span id="cb245-61"><a href="#cb245-61" aria-hidden="true" tabindex="-1"></a>n_features <span class="op">=</span> <span class="dv">9</span></span>
<span id="cb245-62"><a href="#cb245-62" aria-hidden="true" tabindex="-1"></a>n_outputs <span class="op">=</span> <span class="dv">6</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-44" class="cell" data-trusted="true">
<div class="sourceCode cell-code" id="cb246"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb246-1"><a href="#cb246-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate model</span></span>
<span id="cb246-2"><a href="#cb246-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> TCNModel(n_length, n_features, n_outputs).to(device)</span>
<span id="cb246-3"><a href="#cb246-3" aria-hidden="true" tabindex="-1"></a>summary(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-45" class="cell" data-trusted="true">
<div class="sourceCode cell-code" id="cb247"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb247-1"><a href="#cb247-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb247-2"><a href="#cb247-2" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max<span class="op">=</span><span class="dv">50</span>, eta_min<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb247-3"><a href="#cb247-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb247-4"><a href="#cb247-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb247-5"><a href="#cb247-5" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> <span class="kw">lambda</span> inputs, targets: <span class="fl">0.9</span><span class="op">*</span>kl_divergence_loss(inputs, targets) <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>nn.CrossEntropyLoss()(inputs.<span class="bu">float</span>(), targets.<span class="bu">float</span>())</span>
<span id="cb247-6"><a href="#cb247-6" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> train_model(model, train_loader, test_loader, optimizer<span class="op">=</span>optimizer, criterion<span class="op">=</span> loss_fn, epochs<span class="op">=</span><span class="dv">150</span>, device<span class="op">=</span>device, early_stopping<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-46" class="cell" data-trusted="true">
<div class="sourceCode cell-code" id="cb248"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb248-1"><a href="#cb248-1" aria-hidden="true" tabindex="-1"></a>plot_history(history)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-47" class="cell" data-trusted="true">
<div class="sourceCode cell-code" id="cb249"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb249-1"><a href="#cb249-1" aria-hidden="true" tabindex="-1"></a>plot_confusion_matrix(model, test_loader, class_names<span class="op">=</span>[<span class="st">"WALKING"</span>, <span class="st">"WALKING_UPSTAIRS"</span>, <span class="st">"WALKING_DOWNSTAIRS"</span>, <span class="st">"SITTING"</span>, <span class="st">"STANDING"</span>, <span class="st">"LAYING"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="tcn-cbam-model" class="level3">
<h3 class="anchored" data-anchor-id="tcn-cbam-model">TCN + CBAM Model</h3>
<div id="cell-49" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:04:49.325455Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:04:49.325107Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:04:49.338758Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:04:49.337770Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:04:49.325426Z&quot;}" data-trusted="true" data-execution_count="56">
<div class="sourceCode cell-code" id="cb250"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb250-1"><a href="#cb250-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ChannelAttention(nn.Module):</span>
<span id="cb250-2"><a href="#cb250-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, reduction_ratio<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb250-3"><a href="#cb250-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ChannelAttention, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb250-4"><a href="#cb250-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.avg_pool <span class="op">=</span> nn.AdaptiveAvgPool1d(<span class="dv">1</span>)</span>
<span id="cb250-5"><a href="#cb250-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_pool <span class="op">=</span> nn.AdaptiveMaxPool1d(<span class="dv">1</span>)</span>
<span id="cb250-6"><a href="#cb250-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Conv1d(in_channels, in_channels <span class="op">//</span> reduction_ratio, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb250-7"><a href="#cb250-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Conv1d(in_channels <span class="op">//</span> reduction_ratio, in_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb250-8"><a href="#cb250-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-9"><a href="#cb250-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb250-10"><a href="#cb250-10" aria-hidden="true" tabindex="-1"></a>        avg_pool <span class="op">=</span> <span class="va">self</span>.avg_pool(x)</span>
<span id="cb250-11"><a href="#cb250-11" aria-hidden="true" tabindex="-1"></a>        max_pool <span class="op">=</span> <span class="va">self</span>.max_pool(x)</span>
<span id="cb250-12"><a href="#cb250-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-13"><a href="#cb250-13" aria-hidden="true" tabindex="-1"></a>        avg_out <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(avg_pool))</span>
<span id="cb250-14"><a href="#cb250-14" aria-hidden="true" tabindex="-1"></a>        avg_out <span class="op">=</span> <span class="va">self</span>.fc2(avg_out)</span>
<span id="cb250-15"><a href="#cb250-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-16"><a href="#cb250-16" aria-hidden="true" tabindex="-1"></a>        max_out <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(max_pool))</span>
<span id="cb250-17"><a href="#cb250-17" aria-hidden="true" tabindex="-1"></a>        max_out <span class="op">=</span> <span class="va">self</span>.fc2(max_out)</span>
<span id="cb250-18"><a href="#cb250-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-19"><a href="#cb250-19" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> avg_out <span class="op">+</span> max_out</span>
<span id="cb250-20"><a href="#cb250-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(out) <span class="op">*</span> x  <span class="co"># Channel attention applied</span></span>
<span id="cb250-21"><a href="#cb250-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-22"><a href="#cb250-22" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SpatialAttention(nn.Module):</span>
<span id="cb250-23"><a href="#cb250-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, kernel_size<span class="op">=</span><span class="dv">7</span>):</span>
<span id="cb250-24"><a href="#cb250-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SpatialAttention, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb250-25"><a href="#cb250-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv <span class="op">=</span> nn.Conv1d(<span class="dv">2</span>, <span class="dv">1</span>, kernel_size<span class="op">=</span>kernel_size, padding<span class="op">=</span>kernel_size<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb250-26"><a href="#cb250-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigmoid <span class="op">=</span> nn.Sigmoid()</span>
<span id="cb250-27"><a href="#cb250-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-28"><a href="#cb250-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb250-29"><a href="#cb250-29" aria-hidden="true" tabindex="-1"></a>        avg_pool <span class="op">=</span> torch.mean(x, dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb250-30"><a href="#cb250-30" aria-hidden="true" tabindex="-1"></a>        max_pool, _ <span class="op">=</span> torch.<span class="bu">max</span>(x, dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb250-31"><a href="#cb250-31" aria-hidden="true" tabindex="-1"></a>        x_cat <span class="op">=</span> torch.cat([avg_pool, max_pool], dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># Concatenate along channel dimension</span></span>
<span id="cb250-32"><a href="#cb250-32" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv(x_cat)</span>
<span id="cb250-33"><a href="#cb250-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.sigmoid(out) <span class="op">*</span> x  <span class="co"># Spatial attention applied</span></span>
<span id="cb250-34"><a href="#cb250-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-35"><a href="#cb250-35" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CBAMBlock(nn.Module):</span>
<span id="cb250-36"><a href="#cb250-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels):</span>
<span id="cb250-37"><a href="#cb250-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(CBAMBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb250-38"><a href="#cb250-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.channel_attention <span class="op">=</span> ChannelAttention(in_channels)</span>
<span id="cb250-39"><a href="#cb250-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.spatial_attention <span class="op">=</span> SpatialAttention()</span>
<span id="cb250-40"><a href="#cb250-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-41"><a href="#cb250-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb250-42"><a href="#cb250-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.channel_attention(x)</span>
<span id="cb250-43"><a href="#cb250-43" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.spatial_attention(x)</span>
<span id="cb250-44"><a href="#cb250-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb250-45"><a href="#cb250-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-46"><a href="#cb250-46" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TCNWithCBAM(nn.Module):</span>
<span id="cb250-47"><a href="#cb250-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_length, n_features, n_outputs):</span>
<span id="cb250-48"><a href="#cb250-48" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(TCNWithCBAM, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb250-49"><a href="#cb250-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-50"><a href="#cb250-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tcn_blocks <span class="op">=</span> nn.Sequential(</span>
<span id="cb250-51"><a href="#cb250-51" aria-hidden="true" tabindex="-1"></a>            TCNBlock(n_features, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb250-52"><a href="#cb250-52" aria-hidden="true" tabindex="-1"></a>            TCNBlock(<span class="dv">64</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb250-53"><a href="#cb250-53" aria-hidden="true" tabindex="-1"></a>            TCNBlock(<span class="dv">128</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb250-54"><a href="#cb250-54" aria-hidden="true" tabindex="-1"></a>            TCNBlock(<span class="dv">256</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb250-55"><a href="#cb250-55" aria-hidden="true" tabindex="-1"></a>        ) <span class="co"># (batch_size, out_channels, n_length)</span></span>
<span id="cb250-56"><a href="#cb250-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-57"><a href="#cb250-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cbam_block <span class="op">=</span> CBAMBlock(<span class="dv">256</span>)</span>
<span id="cb250-58"><a href="#cb250-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-59"><a href="#cb250-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.global_avg_pool <span class="op">=</span> nn.AdaptiveAvgPool1d(<span class="dv">1</span>)</span>
<span id="cb250-60"><a href="#cb250-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>)</span>
<span id="cb250-61"><a href="#cb250-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>)</span>
<span id="cb250-62"><a href="#cb250-62" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, n_outputs)</span>
<span id="cb250-63"><a href="#cb250-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-64"><a href="#cb250-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.5</span>)</span>
<span id="cb250-65"><a href="#cb250-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-66"><a href="#cb250-66" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb250-67"><a href="#cb250-67" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)  <span class="co"># Convert to (batch_size, n_features, n_length)</span></span>
<span id="cb250-68"><a href="#cb250-68" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.tcn_blocks(x)</span>
<span id="cb250-69"><a href="#cb250-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-70"><a href="#cb250-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply CBAM</span></span>
<span id="cb250-71"><a href="#cb250-71" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.cbam_block(x)</span>
<span id="cb250-72"><a href="#cb250-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-73"><a href="#cb250-73" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.global_avg_pool(x).squeeze(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># Global average pooling</span></span>
<span id="cb250-74"><a href="#cb250-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-75"><a href="#cb250-75" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb250-76"><a href="#cb250-76" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb250-77"><a href="#cb250-77" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb250-78"><a href="#cb250-78" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb250-79"><a href="#cb250-79" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb250-80"><a href="#cb250-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-81"><a href="#cb250-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.softmax(x, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb250-82"><a href="#cb250-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-83"><a href="#cb250-83" aria-hidden="true" tabindex="-1"></a><span class="co"># Define model parameters</span></span>
<span id="cb250-84"><a href="#cb250-84" aria-hidden="true" tabindex="-1"></a>n_length <span class="op">=</span> <span class="dv">128</span>  </span>
<span id="cb250-85"><a href="#cb250-85" aria-hidden="true" tabindex="-1"></a>n_features <span class="op">=</span> <span class="dv">9</span></span>
<span id="cb250-86"><a href="#cb250-86" aria-hidden="true" tabindex="-1"></a>n_outputs <span class="op">=</span> <span class="dv">6</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-50" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:04:50.439549Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:04:50.439200Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:04:50.462617Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:04:50.461509Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:04:50.439520Z&quot;}" data-trusted="true" data-execution_count="57">
<div class="sourceCode cell-code" id="cb251"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb251-1"><a href="#cb251-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> TCNWithCBAM(n_length, n_features, n_outputs).to(device)</span>
<span id="cb251-2"><a href="#cb251-2" aria-hidden="true" tabindex="-1"></a>summary(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TCNWithCBAM                              --
├─Sequential: 1-1                        --
│    └─TCNBlock: 2-1                     --
│    │    └─Conv1d: 3-1                  1,792
│    │    └─BatchNorm1d: 3-2             128
│    │    └─Conv1d: 3-3                  640
│    └─TCNBlock: 2-2                     --
│    │    └─Conv1d: 3-4                  24,704
│    │    └─BatchNorm1d: 3-5             256
│    │    └─Conv1d: 3-6                  8,320
│    └─TCNBlock: 2-3                     --
│    │    └─Conv1d: 3-7                  98,560
│    │    └─BatchNorm1d: 3-8             512
│    │    └─Conv1d: 3-9                  33,024
│    └─TCNBlock: 2-4                     --
│    │    └─Conv1d: 3-10                 196,864
│    │    └─BatchNorm1d: 3-11            512
├─CBAMBlock: 1-2                         --
│    └─ChannelAttention: 2-5             --
│    │    └─AdaptiveAvgPool1d: 3-12      --
│    │    └─AdaptiveMaxPool1d: 3-13      --
│    │    └─Conv1d: 3-14                 8,224
│    │    └─Conv1d: 3-15                 8,448
│    └─SpatialAttention: 2-6             --
│    │    └─Conv1d: 3-16                 15
│    │    └─Sigmoid: 3-17                --
├─AdaptiveAvgPool1d: 1-3                 --
├─Linear: 1-4                            32,896
├─Linear: 1-5                            8,256
├─Linear: 1-6                            390
├─Dropout: 1-7                           --
=================================================================
Total params: 423,541
Trainable params: 423,541
Non-trainable params: 0
=================================================================</code></pre>
</div>
</div>
<div id="cell-51" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:04:54.877100Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:04:54.876748Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:08:01.247361Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:08:01.246484Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:04:54.877071Z&quot;}" data-trusted="true" data-execution_count="58">
<div class="sourceCode cell-code" id="cb253"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb253-1"><a href="#cb253-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb253-2"><a href="#cb253-2" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max<span class="op">=</span><span class="dv">50</span>, eta_min<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb253-3"><a href="#cb253-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb253-4"><a href="#cb253-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb253-5"><a href="#cb253-5" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> <span class="kw">lambda</span> inputs, targets: <span class="fl">0.9</span><span class="op">*</span>kl_divergence_loss(inputs, targets) <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>nn.CrossEntropyLoss()(inputs.<span class="bu">float</span>(), targets.<span class="bu">float</span>())</span>
<span id="cb253-6"><a href="#cb253-6" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> train_model(model, train_loader, test_loader, optimizer<span class="op">=</span>optimizer, criterion<span class="op">=</span> loss_fn, epochs<span class="op">=</span><span class="dv">150</span>, device<span class="op">=</span>device, early_stopping<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 14.79batch/s, loss=0.21] 
Epoch 1/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 40.84batch/s, loss=0.196]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/150 - Train Loss: 0.2444, Train Acc: 0.5257 - Val Loss: 0.2193, Val Acc: 0.6895
New best model found at epoch 1 with val_loss: 0.2193</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.42batch/s, loss=0.12] 
Epoch 2/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.88batch/s, loss=0.105]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/150 - Train Loss: 0.1591, Train Acc: 0.8183 - Val Loss: 0.1289, Val Acc: 0.9074
New best model found at epoch 2 with val_loss: 0.1289</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.121]
Epoch 3/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.27batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/150 - Train Loss: 0.1181, Train Acc: 0.9529 - Val Loss: 0.1284, Val Acc: 0.9141
New best model found at epoch 3 with val_loss: 0.1284</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 4/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.61batch/s, loss=0.111]
Epoch 4/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 39.80batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 4/150 - Train Loss: 0.1168, Train Acc: 0.9573 - Val Loss: 0.1287, Val Acc: 0.9138</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 5/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.32batch/s, loss=0.116]
Epoch 5/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.77batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 5/150 - Train Loss: 0.1157, Train Acc: 0.9600 - Val Loss: 0.1277, Val Acc: 0.9175
New best model found at epoch 5 with val_loss: 0.1277</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 6/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.59batch/s, loss=0.114]
Epoch 6/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.53batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 6/150 - Train Loss: 0.1148, Train Acc: 0.9623 - Val Loss: 0.1267, Val Acc: 0.9226
New best model found at epoch 6 with val_loss: 0.1267</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 7/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.65batch/s, loss=0.118]
Epoch 7/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.28batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 7/150 - Train Loss: 0.1148, Train Acc: 0.9633 - Val Loss: 0.1240, Val Acc: 0.9291
New best model found at epoch 7 with val_loss: 0.1240</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 8/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.45batch/s, loss=0.111]
Epoch 8/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.46batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 8/150 - Train Loss: 0.1143, Train Acc: 0.9642 - Val Loss: 0.1269, Val Acc: 0.9158</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 9/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.71batch/s, loss=0.113]
Epoch 9/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.51batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9/150 - Train Loss: 0.1135, Train Acc: 0.9676 - Val Loss: 0.1267, Val Acc: 0.9182</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 10/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.45batch/s, loss=0.111]
Epoch 10/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.84batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 10/150 - Train Loss: 0.1134, Train Acc: 0.9683 - Val Loss: 0.1244, Val Acc: 0.9253</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 11/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.62batch/s, loss=0.11] 
Epoch 11/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.93batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 11/150 - Train Loss: 0.1127, Train Acc: 0.9703 - Val Loss: 0.1244, Val Acc: 0.9257</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 12/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.71batch/s, loss=0.112]
Epoch 12/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.90batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 12/150 - Train Loss: 0.1128, Train Acc: 0.9690 - Val Loss: 0.1231, Val Acc: 0.9311
New best model found at epoch 12 with val_loss: 0.1231</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 13/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.70batch/s, loss=0.115]
Epoch 13/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.99batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 13/150 - Train Loss: 0.1121, Train Acc: 0.9728 - Val Loss: 0.1233, Val Acc: 0.9301</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 14/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.44batch/s, loss=0.11] 
Epoch 14/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.62batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 14/150 - Train Loss: 0.1122, Train Acc: 0.9714 - Val Loss: 0.1213, Val Acc: 0.9328
New best model found at epoch 14 with val_loss: 0.1213</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 15/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.07batch/s, loss=0.12] 
Epoch 15/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.12batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 15/150 - Train Loss: 0.1116, Train Acc: 0.9725 - Val Loss: 0.1223, Val Acc: 0.9325</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 16/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.62batch/s, loss=0.113]
Epoch 16/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.27batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 16/150 - Train Loss: 0.1111, Train Acc: 0.9761 - Val Loss: 0.1219, Val Acc: 0.9328</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 17/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.80batch/s, loss=0.105]
Epoch 17/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.56batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 17/150 - Train Loss: 0.1114, Train Acc: 0.9752 - Val Loss: 0.1243, Val Acc: 0.9253</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 18/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.87batch/s, loss=0.115]
Epoch 18/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.74batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 18/150 - Train Loss: 0.1127, Train Acc: 0.9710 - Val Loss: 0.1213, Val Acc: 0.9386
New best model found at epoch 18 with val_loss: 0.1213</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 19/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 24.21batch/s, loss=0.112]
Epoch 19/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.57batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 19/150 - Train Loss: 0.1110, Train Acc: 0.9762 - Val Loss: 0.1199, Val Acc: 0.9433
New best model found at epoch 19 with val_loss: 0.1199</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 20/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.50batch/s, loss=0.109]
Epoch 20/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.23batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 20/150 - Train Loss: 0.1109, Train Acc: 0.9757 - Val Loss: 0.1193, Val Acc: 0.9447
New best model found at epoch 20 with val_loss: 0.1193</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 21/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.44batch/s, loss=0.112]
Epoch 21/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 39.01batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 21/150 - Train Loss: 0.1112, Train Acc: 0.9754 - Val Loss: 0.1195, Val Acc: 0.9433</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 22/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.106]
Epoch 22/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.77batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 22/150 - Train Loss: 0.1109, Train Acc: 0.9767 - Val Loss: 0.1193, Val Acc: 0.9457
New best model found at epoch 22 with val_loss: 0.1193</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 23/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.58batch/s, loss=0.106]
Epoch 23/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.03batch/s, loss=0.108]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 23/150 - Train Loss: 0.1105, Train Acc: 0.9780 - Val Loss: 0.1230, Val Acc: 0.9304</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 24/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.61batch/s, loss=0.111]
Epoch 24/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.72batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 24/150 - Train Loss: 0.1111, Train Acc: 0.9762 - Val Loss: 0.1226, Val Acc: 0.9348</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 25/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.116]
Epoch 25/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.43batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 25/150 - Train Loss: 0.1103, Train Acc: 0.9776 - Val Loss: 0.1226, Val Acc: 0.9325</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 26/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.57batch/s, loss=0.11] 
Epoch 26/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 41.54batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 26/150 - Train Loss: 0.1104, Train Acc: 0.9784 - Val Loss: 0.1212, Val Acc: 0.9420</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 27/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.112]
Epoch 27/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.58batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 27/150 - Train Loss: 0.1106, Train Acc: 0.9773 - Val Loss: 0.1214, Val Acc: 0.9382</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 28/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.44batch/s, loss=0.111]
Epoch 28/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.04batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 28/150 - Train Loss: 0.1097, Train Acc: 0.9797 - Val Loss: 0.1203, Val Acc: 0.9413</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 29/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.51batch/s, loss=0.112]
Epoch 29/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.16batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 29/150 - Train Loss: 0.1100, Train Acc: 0.9793 - Val Loss: 0.1214, Val Acc: 0.9372</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 30/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.72batch/s, loss=0.107]
Epoch 30/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.91batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 30/150 - Train Loss: 0.1093, Train Acc: 0.9822 - Val Loss: 0.1213, Val Acc: 0.9406</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 31/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.63batch/s, loss=0.113]
Epoch 31/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.96batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 31/150 - Train Loss: 0.1100, Train Acc: 0.9791 - Val Loss: 0.1238, Val Acc: 0.9311</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 32/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.68batch/s, loss=0.108]
Epoch 32/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.42batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 32/150 - Train Loss: 0.1109, Train Acc: 0.9761 - Val Loss: 0.1218, Val Acc: 0.9386</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 33/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.74batch/s, loss=0.111]
Epoch 33/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.45batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 33/150 - Train Loss: 0.1110, Train Acc: 0.9743 - Val Loss: 0.1201, Val Acc: 0.9444</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 34/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.56batch/s, loss=0.11] 
Epoch 34/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 41.81batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 34/150 - Train Loss: 0.1100, Train Acc: 0.9793 - Val Loss: 0.1199, Val Acc: 0.9450</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 35/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.21batch/s, loss=0.109]
Epoch 35/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.67batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 35/150 - Train Loss: 0.1102, Train Acc: 0.9785 - Val Loss: 0.1245, Val Acc: 0.9304</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 36/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.58batch/s, loss=0.111]
Epoch 36/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.97batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 36/150 - Train Loss: 0.1101, Train Acc: 0.9789 - Val Loss: 0.1255, Val Acc: 0.9237</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 37/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.60batch/s, loss=0.108]
Epoch 37/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.92batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 37/150 - Train Loss: 0.1093, Train Acc: 0.9818 - Val Loss: 0.1248, Val Acc: 0.9247</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 38/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.32batch/s, loss=0.113]
Epoch 38/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 38.88batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 38/150 - Train Loss: 0.1085, Train Acc: 0.9856 - Val Loss: 0.1245, Val Acc: 0.9277</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 39/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.32batch/s, loss=0.111]
Epoch 39/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.09batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 39/150 - Train Loss: 0.1096, Train Acc: 0.9808 - Val Loss: 0.1239, Val Acc: 0.9325</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 40/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.62batch/s, loss=0.107]
Epoch 40/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.12batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 40/150 - Train Loss: 0.1095, Train Acc: 0.9816 - Val Loss: 0.1215, Val Acc: 0.9379</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 41/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.41batch/s, loss=0.11] 
Epoch 41/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.98batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 41/150 - Train Loss: 0.1097, Train Acc: 0.9808 - Val Loss: 0.1224, Val Acc: 0.9355</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 42/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.60batch/s, loss=0.108]
Epoch 42/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.04batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 42/150 - Train Loss: 0.1087, Train Acc: 0.9842 - Val Loss: 0.1219, Val Acc: 0.9372</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 43/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.66batch/s, loss=0.11] 
Epoch 43/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.47batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 43/150 - Train Loss: 0.1094, Train Acc: 0.9810 - Val Loss: 0.1191, Val Acc: 0.9460</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 44/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.41batch/s, loss=0.112]
Epoch 44/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 40.24batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 44/150 - Train Loss: 0.1091, Train Acc: 0.9830 - Val Loss: 0.1230, Val Acc: 0.9318</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 45/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.59batch/s, loss=0.108]
Epoch 45/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.91batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 45/150 - Train Loss: 0.1101, Train Acc: 0.9795 - Val Loss: 0.1261, Val Acc: 0.9226</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 46/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.51batch/s, loss=0.111]
Epoch 46/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.66batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 46/150 - Train Loss: 0.1105, Train Acc: 0.9786 - Val Loss: 0.1220, Val Acc: 0.9369</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 47/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.43batch/s, loss=0.107]
Epoch 47/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.64batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 47/150 - Train Loss: 0.1098, Train Acc: 0.9808 - Val Loss: 0.1231, Val Acc: 0.9328</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 48/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.05batch/s, loss=0.113]
Epoch 48/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 41.23batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 48/150 - Train Loss: 0.1086, Train Acc: 0.9852 - Val Loss: 0.1226, Val Acc: 0.9345</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 49/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.111]
Epoch 49/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.23batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 49/150 - Train Loss: 0.1094, Train Acc: 0.9819 - Val Loss: 0.1254, Val Acc: 0.9243</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 50/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.27batch/s, loss=0.105]
Epoch 50/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.58batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 50/150 - Train Loss: 0.1088, Train Acc: 0.9839 - Val Loss: 0.1264, Val Acc: 0.9223</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 51/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.35batch/s, loss=0.106]
Epoch 51/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.91batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 51/150 - Train Loss: 0.1082, Train Acc: 0.9867 - Val Loss: 0.1242, Val Acc: 0.9321</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 52/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.65batch/s, loss=0.112]
Epoch 52/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.80batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 52/150 - Train Loss: 0.1078, Train Acc: 0.9876 - Val Loss: 0.1242, Val Acc: 0.9301</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 53/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.66batch/s, loss=0.106]
Epoch 53/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.47batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 53/150 - Train Loss: 0.1083, Train Acc: 0.9861 - Val Loss: 0.1246, Val Acc: 0.9287</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 54/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.75batch/s, loss=0.109]
Epoch 54/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.11batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 54/150 - Train Loss: 0.1080, Train Acc: 0.9869 - Val Loss: 0.1289, Val Acc: 0.9155</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 55/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.52batch/s, loss=0.108]
Epoch 55/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.83batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 55/150 - Train Loss: 0.1088, Train Acc: 0.9839 - Val Loss: 0.1245, Val Acc: 0.9298</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 56/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.71batch/s, loss=0.104]
Epoch 56/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.46batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 56/150 - Train Loss: 0.1102, Train Acc: 0.9799 - Val Loss: 0.1203, Val Acc: 0.9447</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 57/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.64batch/s, loss=0.106]
Epoch 57/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.05batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 57/150 - Train Loss: 0.1086, Train Acc: 0.9856 - Val Loss: 0.1222, Val Acc: 0.9386</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 58/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.64batch/s, loss=0.111]
Epoch 58/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.76batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 58/150 - Train Loss: 0.1087, Train Acc: 0.9849 - Val Loss: 0.1192, Val Acc: 0.9484
New best model found at epoch 58 with val_loss: 0.1192</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 59/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.67batch/s, loss=0.107]
Epoch 59/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 40.18batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 59/150 - Train Loss: 0.1088, Train Acc: 0.9844 - Val Loss: 0.1195, Val Acc: 0.9474</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 60/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.47batch/s, loss=0.107]
Epoch 60/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.09batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 60/150 - Train Loss: 0.1085, Train Acc: 0.9859 - Val Loss: 0.1204, Val Acc: 0.9454</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 61/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.25batch/s, loss=0.107]
Epoch 61/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.16batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 61/150 - Train Loss: 0.1090, Train Acc: 0.9825 - Val Loss: 0.1201, Val Acc: 0.9454</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 62/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 24.37batch/s, loss=0.108]
Epoch 62/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 45.81batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 62/150 - Train Loss: 0.1089, Train Acc: 0.9838 - Val Loss: 0.1199, Val Acc: 0.9450</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 63/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 24.73batch/s, loss=0.107]
Epoch 63/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 46.01batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 63/150 - Train Loss: 0.1078, Train Acc: 0.9880 - Val Loss: 0.1202, Val Acc: 0.9454</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 64/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 24.46batch/s, loss=0.105]
Epoch 64/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 45.42batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 64/150 - Train Loss: 0.1077, Train Acc: 0.9882 - Val Loss: 0.1213, Val Acc: 0.9403</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 65/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.78batch/s, loss=0.106]
Epoch 65/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.19batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 65/150 - Train Loss: 0.1078, Train Acc: 0.9879 - Val Loss: 0.1196, Val Acc: 0.9477</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 66/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.72batch/s, loss=0.106]
Epoch 66/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.66batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 66/150 - Train Loss: 0.1080, Train Acc: 0.9869 - Val Loss: 0.1207, Val Acc: 0.9437</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 67/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.60batch/s, loss=0.109]
Epoch 67/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 36.15batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 67/150 - Train Loss: 0.1088, Train Acc: 0.9838 - Val Loss: 0.1213, Val Acc: 0.9427</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 68/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.37batch/s, loss=0.113]
Epoch 68/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.27batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 68/150 - Train Loss: 0.1086, Train Acc: 0.9849 - Val Loss: 0.1215, Val Acc: 0.9413</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 69/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.60batch/s, loss=0.111]
Epoch 69/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.16batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 69/150 - Train Loss: 0.1087, Train Acc: 0.9846 - Val Loss: 0.1202, Val Acc: 0.9454</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 70/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.62batch/s, loss=0.106]
Epoch 70/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.65batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 70/150 - Train Loss: 0.1082, Train Acc: 0.9864 - Val Loss: 0.1212, Val Acc: 0.9406</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 71/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.43batch/s, loss=0.111]
Epoch 71/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.99batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 71/150 - Train Loss: 0.1082, Train Acc: 0.9865 - Val Loss: 0.1198, Val Acc: 0.9464</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 72/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.73batch/s, loss=0.108]
Epoch 72/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.62batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 72/150 - Train Loss: 0.1085, Train Acc: 0.9854 - Val Loss: 0.1201, Val Acc: 0.9454</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 73/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.65batch/s, loss=0.111]
Epoch 73/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.35batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 73/150 - Train Loss: 0.1082, Train Acc: 0.9863 - Val Loss: 0.1203, Val Acc: 0.9454</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 74/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.22batch/s, loss=0.109]
Epoch 74/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.24batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 74/150 - Train Loss: 0.1079, Train Acc: 0.9878 - Val Loss: 0.1194, Val Acc: 0.9488</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 75/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.51batch/s, loss=0.118]
Epoch 75/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.95batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 75/150 - Train Loss: 0.1076, Train Acc: 0.9887 - Val Loss: 0.1206, Val Acc: 0.9437</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 76/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.72batch/s, loss=0.107]
Epoch 76/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.78batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 76/150 - Train Loss: 0.1074, Train Acc: 0.9893 - Val Loss: 0.1187, Val Acc: 0.9511
New best model found at epoch 76 with val_loss: 0.1187</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 77/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.58batch/s, loss=0.11] 
Epoch 77/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.15batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 77/150 - Train Loss: 0.1078, Train Acc: 0.9878 - Val Loss: 0.1221, Val Acc: 0.9393</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 78/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.34batch/s, loss=0.104]
Epoch 78/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 38.20batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 78/150 - Train Loss: 0.1082, Train Acc: 0.9865 - Val Loss: 0.1189, Val Acc: 0.9511</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 79/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.105]
Epoch 79/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.62batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 79/150 - Train Loss: 0.1081, Train Acc: 0.9865 - Val Loss: 0.1198, Val Acc: 0.9477</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 80/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.77batch/s, loss=0.105]
Epoch 80/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.91batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 80/150 - Train Loss: 0.1076, Train Acc: 0.9879 - Val Loss: 0.1207, Val Acc: 0.9433</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 81/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.29batch/s, loss=0.11] 
Epoch 81/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.38batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 81/150 - Train Loss: 0.1074, Train Acc: 0.9895 - Val Loss: 0.1196, Val Acc: 0.9477</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 82/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.75batch/s, loss=0.108]
Epoch 82/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.99batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 82/150 - Train Loss: 0.1071, Train Acc: 0.9906 - Val Loss: 0.1197, Val Acc: 0.9474</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 83/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.74batch/s, loss=0.109]
Epoch 83/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 41.70batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 83/150 - Train Loss: 0.1074, Train Acc: 0.9897 - Val Loss: 0.1193, Val Acc: 0.9481</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 84/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.53batch/s, loss=0.109]
Epoch 84/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 40.30batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 84/150 - Train Loss: 0.1079, Train Acc: 0.9878 - Val Loss: 0.1209, Val Acc: 0.9427</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 85/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.52batch/s, loss=0.109]
Epoch 85/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.87batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 85/150 - Train Loss: 0.1075, Train Acc: 0.9897 - Val Loss: 0.1178, Val Acc: 0.9542
New best model found at epoch 85 with val_loss: 0.1178</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 86/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.61batch/s, loss=0.109]
Epoch 86/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.40batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 86/150 - Train Loss: 0.1078, Train Acc: 0.9878 - Val Loss: 0.1193, Val Acc: 0.9488</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 87/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.10batch/s, loss=0.109]
Epoch 87/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.46batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 87/150 - Train Loss: 0.1079, Train Acc: 0.9875 - Val Loss: 0.1188, Val Acc: 0.9501</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 88/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.38batch/s, loss=0.114]
Epoch 88/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.03batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 88/150 - Train Loss: 0.1076, Train Acc: 0.9884 - Val Loss: 0.1197, Val Acc: 0.9471</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 89/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.54batch/s, loss=0.109]
Epoch 89/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.72batch/s, loss=0.105]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 89/150 - Train Loss: 0.1072, Train Acc: 0.9903 - Val Loss: 0.1250, Val Acc: 0.9291</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 90/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.59batch/s, loss=0.107]
Epoch 90/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.44batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 90/150 - Train Loss: 0.1075, Train Acc: 0.9894 - Val Loss: 0.1204, Val Acc: 0.9454</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 91/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.56batch/s, loss=0.106]
Epoch 91/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.44batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 91/150 - Train Loss: 0.1075, Train Acc: 0.9891 - Val Loss: 0.1256, Val Acc: 0.9270</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 92/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.74batch/s, loss=0.106]
Epoch 92/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.47batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 92/150 - Train Loss: 0.1080, Train Acc: 0.9875 - Val Loss: 0.1274, Val Acc: 0.9216</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 93/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.33batch/s, loss=0.113]
Epoch 93/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 44.04batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 93/150 - Train Loss: 0.1091, Train Acc: 0.9830 - Val Loss: 0.1240, Val Acc: 0.9332</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 94/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.94batch/s, loss=0.109]
Epoch 94/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.23batch/s, loss=0.105]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 94/150 - Train Loss: 0.1091, Train Acc: 0.9846 - Val Loss: 0.1217, Val Acc: 0.9396</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 95/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.63batch/s, loss=0.104]
Epoch 95/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.31batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 95/150 - Train Loss: 0.1090, Train Acc: 0.9837 - Val Loss: 0.1198, Val Acc: 0.9464</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 96/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.64batch/s, loss=0.109]
Epoch 96/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.85batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 96/150 - Train Loss: 0.1083, Train Acc: 0.9868 - Val Loss: 0.1199, Val Acc: 0.9457</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 97/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.55batch/s, loss=0.111]
Epoch 97/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.26batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 97/150 - Train Loss: 0.1086, Train Acc: 0.9852 - Val Loss: 0.1198, Val Acc: 0.9464</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 98/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.57batch/s, loss=0.11] 
Epoch 98/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.76batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 98/150 - Train Loss: 0.1086, Train Acc: 0.9854 - Val Loss: 0.1202, Val Acc: 0.9460</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 99/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.70batch/s, loss=0.111]
Epoch 99/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.31batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 99/150 - Train Loss: 0.1080, Train Acc: 0.9876 - Val Loss: 0.1186, Val Acc: 0.9515</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 100/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.80batch/s, loss=0.106]
Epoch 100/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 37.88batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 100/150 - Train Loss: 0.1080, Train Acc: 0.9872 - Val Loss: 0.1193, Val Acc: 0.9484</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 101/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.40batch/s, loss=0.111]
Epoch 101/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.39batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 101/150 - Train Loss: 0.1077, Train Acc: 0.9882 - Val Loss: 0.1179, Val Acc: 0.9542</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 102/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.77batch/s, loss=0.108]
Epoch 102/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 44.20batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 102/150 - Train Loss: 0.1082, Train Acc: 0.9863 - Val Loss: 0.1170, Val Acc: 0.9562
New best model found at epoch 102 with val_loss: 0.1170</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 103/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.81batch/s, loss=0.108]
Epoch 103/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.44batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 103/150 - Train Loss: 0.1073, Train Acc: 0.9894 - Val Loss: 0.1186, Val Acc: 0.9522</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 104/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.82batch/s, loss=0.105]
Epoch 104/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.29batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 104/150 - Train Loss: 0.1071, Train Acc: 0.9901 - Val Loss: 0.1190, Val Acc: 0.9511</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 105/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.50batch/s, loss=0.106]
Epoch 105/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.52batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 105/150 - Train Loss: 0.1069, Train Acc: 0.9916 - Val Loss: 0.1182, Val Acc: 0.9511</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 106/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.72batch/s, loss=0.109]
Epoch 106/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.80batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 106/150 - Train Loss: 0.1072, Train Acc: 0.9902 - Val Loss: 0.1189, Val Acc: 0.9488</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 107/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.12batch/s, loss=0.105]
Epoch 107/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.48batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 107/150 - Train Loss: 0.1069, Train Acc: 0.9910 - Val Loss: 0.1207, Val Acc: 0.9440</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 108/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.37batch/s, loss=0.106]
Epoch 108/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.41batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 108/150 - Train Loss: 0.1072, Train Acc: 0.9895 - Val Loss: 0.1193, Val Acc: 0.9494</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 109/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.73batch/s, loss=0.106]
Epoch 109/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.81batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 109/150 - Train Loss: 0.1072, Train Acc: 0.9903 - Val Loss: 0.1198, Val Acc: 0.9477</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 110/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.62batch/s, loss=0.104]
Epoch 110/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.58batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 110/150 - Train Loss: 0.1072, Train Acc: 0.9899 - Val Loss: 0.1187, Val Acc: 0.9508</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 111/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.58batch/s, loss=0.106]
Epoch 111/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.02batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 111/150 - Train Loss: 0.1071, Train Acc: 0.9901 - Val Loss: 0.1186, Val Acc: 0.9508</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 112/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.70batch/s, loss=0.105]
Epoch 112/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.25batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 112/150 - Train Loss: 0.1076, Train Acc: 0.9880 - Val Loss: 0.1194, Val Acc: 0.9488</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 113/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.63batch/s, loss=0.108]
Epoch 113/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 39.53batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 113/150 - Train Loss: 0.1074, Train Acc: 0.9891 - Val Loss: 0.1191, Val Acc: 0.9494</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 114/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.65batch/s, loss=0.106]
Epoch 114/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.95batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 114/150 - Train Loss: 0.1070, Train Acc: 0.9910 - Val Loss: 0.1200, Val Acc: 0.9467</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 115/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.49batch/s, loss=0.108]
Epoch 115/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 43.32batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 115/150 - Train Loss: 0.1067, Train Acc: 0.9917 - Val Loss: 0.1192, Val Acc: 0.9501</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 116/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.78batch/s, loss=0.111]
Epoch 116/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.87batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 116/150 - Train Loss: 0.1068, Train Acc: 0.9914 - Val Loss: 0.1189, Val Acc: 0.9494</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 117/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 24.25batch/s, loss=0.108]
Epoch 117/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 41.40batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 117/150 - Train Loss: 0.1068, Train Acc: 0.9917 - Val Loss: 0.1196, Val Acc: 0.9474</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 118/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.60batch/s, loss=0.106]
Epoch 118/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.16batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 118/150 - Train Loss: 0.1067, Train Acc: 0.9914 - Val Loss: 0.1188, Val Acc: 0.9505</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 119/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.51batch/s, loss=0.104]
Epoch 119/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.77batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 119/150 - Train Loss: 0.1072, Train Acc: 0.9905 - Val Loss: 0.1201, Val Acc: 0.9450</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 120/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.36batch/s, loss=0.109]
Epoch 120/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.21batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 120/150 - Train Loss: 0.1075, Train Acc: 0.9887 - Val Loss: 0.1210, Val Acc: 0.9433</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 121/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 22.58batch/s, loss=0.106]
Epoch 121/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.15batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 121/150 - Train Loss: 0.1070, Train Acc: 0.9908 - Val Loss: 0.1193, Val Acc: 0.9488</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 122/150 - Training: 100%|██████████| 29/29 [00:01&lt;00:00, 23.64batch/s, loss=0.11] 
Epoch 122/150 - Validation: 100%|██████████| 12/12 [00:00&lt;00:00, 42.32batch/s, loss=0.104]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 122/150 - Train Loss: 0.1069, Train Acc: 0.9914 - Val Loss: 0.1193, Val Acc: 0.9481
Early stopping triggered!
Best model saved to best_model.pth with val_loss: 0.1170 and val_acc: 0.9562</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
</div>
<div id="cell-52" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:08:03.978405Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:08:03.978047Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:08:04.383141Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:08:04.382308Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:08:03.978375Z&quot;}" data-trusted="true" data-execution_count="59">
<div class="sourceCode cell-code" id="cb499"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb499-1"><a href="#cb499-1" aria-hidden="true" tabindex="-1"></a>plot_history(history)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="uci-har-pytorch_files/figure-html/cell-42-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-53" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:08:06.763953Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:08:06.763573Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:08:07.398381Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:08:07.397286Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:08:06.763910Z&quot;}" data-trusted="true" data-execution_count="60">
<div class="sourceCode cell-code" id="cb500"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb500-1"><a href="#cb500-1" aria-hidden="true" tabindex="-1"></a>plot_confusion_matrix(model, test_loader, class_names<span class="op">=</span>[<span class="st">"WALKING"</span>, <span class="st">"WALKING_UPSTAIRS"</span>, <span class="st">"WALKING_DOWNSTAIRS"</span>, <span class="st">"SITTING"</span>, <span class="st">"STANDING"</span>, <span class="st">"LAYING"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="uci-har-pytorch_files/figure-html/cell-43-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="tcn-self-attention" class="level3">
<h3 class="anchored" data-anchor-id="tcn-self-attention">TCN + Self Attention</h3>
<div id="cell-55" class="cell" data-trusted="true">
<div class="sourceCode cell-code" id="cb501"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb501-1"><a href="#cb501-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttention(nn.Module):</span>
<span id="cb501-2"><a href="#cb501-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, d_k<span class="op">=</span><span class="dv">64</span>, d_v<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb501-3"><a href="#cb501-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SelfAttention, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb501-4"><a href="#cb501-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb501-5"><a href="#cb501-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Conv1d(in_channels, d_k, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb501-6"><a href="#cb501-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Conv1d(in_channels, d_k, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb501-7"><a href="#cb501-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Conv1d(in_channels, d_v, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb501-8"><a href="#cb501-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb501-9"><a href="#cb501-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn_fc <span class="op">=</span> nn.Linear(d_v, in_channels)</span>
<span id="cb501-10"><a href="#cb501-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-11"><a href="#cb501-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb501-12"><a href="#cb501-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x: (batch_size, channels, seq_length)</span></span>
<span id="cb501-13"><a href="#cb501-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb501-14"><a href="#cb501-14" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> <span class="va">self</span>.query(x)  <span class="co"># Query: (batch_size, d_k, seq_length)</span></span>
<span id="cb501-15"><a href="#cb501-15" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.key(x)    <span class="co"># Key: (batch_size, d_k, seq_length)</span></span>
<span id="cb501-16"><a href="#cb501-16" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.value(x)  <span class="co"># Value: (batch_size, d_v, seq_length)</span></span>
<span id="cb501-17"><a href="#cb501-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-18"><a href="#cb501-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Scaled dot-product attention</span></span>
<span id="cb501-19"><a href="#cb501-19" aria-hidden="true" tabindex="-1"></a>        attn_scores <span class="op">=</span> torch.bmm(Q.transpose(<span class="dv">1</span>, <span class="dv">2</span>), K)  <span class="co"># (batch_size, seq_length, seq_length)</span></span>
<span id="cb501-20"><a href="#cb501-20" aria-hidden="true" tabindex="-1"></a>        attn_scores <span class="op">=</span> attn_scores <span class="op">/</span> (Q.size(<span class="op">-</span><span class="dv">1</span>) <span class="op">**</span> <span class="fl">0.5</span>)  <span class="co"># Scaling</span></span>
<span id="cb501-21"><a href="#cb501-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb501-22"><a href="#cb501-22" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> F.softmax(attn_scores, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (batch_size, seq_length, seq_length)</span></span>
<span id="cb501-23"><a href="#cb501-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb501-24"><a href="#cb501-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attention output</span></span>
<span id="cb501-25"><a href="#cb501-25" aria-hidden="true" tabindex="-1"></a>        attn_output <span class="op">=</span> torch.bmm(attn_weights, V.transpose(<span class="dv">1</span>, <span class="dv">2</span>))  <span class="co"># (batch_size, seq_length, d_v)</span></span>
<span id="cb501-26"><a href="#cb501-26" aria-hidden="true" tabindex="-1"></a>        attn_output <span class="op">=</span> attn_output.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># (batch_size, d_v, seq_length)</span></span>
<span id="cb501-27"><a href="#cb501-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-28"><a href="#cb501-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine with input</span></span>
<span id="cb501-29"><a href="#cb501-29" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.attn_fc(attn_output)</span>
<span id="cb501-30"><a href="#cb501-30" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb501-31"><a href="#cb501-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output <span class="op">+</span> x  <span class="co"># Add residual connection</span></span>
<span id="cb501-32"><a href="#cb501-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-33"><a href="#cb501-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-34"><a href="#cb501-34" aria-hidden="true" tabindex="-1"></a><span class="co"># TCN + Self-Attention Model</span></span>
<span id="cb501-35"><a href="#cb501-35" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TCNWithAttention(nn.Module):</span>
<span id="cb501-36"><a href="#cb501-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_length, n_features, n_outputs):</span>
<span id="cb501-37"><a href="#cb501-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(TCNWithAttention, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb501-38"><a href="#cb501-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-39"><a href="#cb501-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># TCN Blocks</span></span>
<span id="cb501-40"><a href="#cb501-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tcn_blocks <span class="op">=</span> nn.Sequential(</span>
<span id="cb501-41"><a href="#cb501-41" aria-hidden="true" tabindex="-1"></a>            TCNBlock(n_features, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb501-42"><a href="#cb501-42" aria-hidden="true" tabindex="-1"></a>            TCNBlock(<span class="dv">64</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb501-43"><a href="#cb501-43" aria-hidden="true" tabindex="-1"></a>            TCNBlock(<span class="dv">128</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb501-44"><a href="#cb501-44" aria-hidden="true" tabindex="-1"></a>            TCNBlock(<span class="dv">256</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb501-45"><a href="#cb501-45" aria-hidden="true" tabindex="-1"></a>        ) <span class="co"># (batch_size, out_channels, n_length)</span></span>
<span id="cb501-46"><a href="#cb501-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-47"><a href="#cb501-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Self-Attention Layer</span></span>
<span id="cb501-48"><a href="#cb501-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attention <span class="op">=</span> SelfAttention(in_channels<span class="op">=</span><span class="dv">256</span>, d_k<span class="op">=</span><span class="dv">128</span>, d_v<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb501-49"><a href="#cb501-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-50"><a href="#cb501-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fully connected layers after TCN and Attention</span></span>
<span id="cb501-51"><a href="#cb501-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.global_avg_pool <span class="op">=</span> nn.AdaptiveAvgPool1d(<span class="dv">1</span>)</span>
<span id="cb501-52"><a href="#cb501-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>)</span>
<span id="cb501-53"><a href="#cb501-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>)</span>
<span id="cb501-54"><a href="#cb501-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, n_outputs)</span>
<span id="cb501-55"><a href="#cb501-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-56"><a href="#cb501-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.5</span>)</span>
<span id="cb501-57"><a href="#cb501-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_norm1 <span class="op">=</span> nn.BatchNorm1d(<span class="dv">128</span>)</span>
<span id="cb501-58"><a href="#cb501-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_norm2 <span class="op">=</span> nn.BatchNorm1d(<span class="dv">64</span>)</span>
<span id="cb501-59"><a href="#cb501-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-60"><a href="#cb501-60" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb501-61"><a href="#cb501-61" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)  <span class="co"># Convert to (batch_size, n_features, n_length)</span></span>
<span id="cb501-62"><a href="#cb501-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb501-63"><a href="#cb501-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># TCN blocks for feature extraction</span></span>
<span id="cb501-64"><a href="#cb501-64" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.tcn_blocks(x)</span>
<span id="cb501-65"><a href="#cb501-65" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb501-66"><a href="#cb501-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-67"><a href="#cb501-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply Self-Attention</span></span>
<span id="cb501-68"><a href="#cb501-68" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.self_attention(x)</span>
<span id="cb501-69"><a href="#cb501-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-70"><a href="#cb501-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global average pooling</span></span>
<span id="cb501-71"><a href="#cb501-71" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.global_avg_pool(x).squeeze(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># (batch_size, channels)</span></span>
<span id="cb501-72"><a href="#cb501-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-73"><a href="#cb501-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fully connected layers for classification</span></span>
<span id="cb501-74"><a href="#cb501-74" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb501-75"><a href="#cb501-75" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.batch_norm1(x)</span>
<span id="cb501-76"><a href="#cb501-76" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb501-77"><a href="#cb501-77" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb501-78"><a href="#cb501-78" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.batch_norm2(x)</span>
<span id="cb501-79"><a href="#cb501-79" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb501-80"><a href="#cb501-80" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb501-81"><a href="#cb501-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-82"><a href="#cb501-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.softmax(x, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb501-83"><a href="#cb501-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-84"><a href="#cb501-84" aria-hidden="true" tabindex="-1"></a><span class="co"># Model parameters</span></span>
<span id="cb501-85"><a href="#cb501-85" aria-hidden="true" tabindex="-1"></a>n_length <span class="op">=</span> <span class="dv">128</span>  </span>
<span id="cb501-86"><a href="#cb501-86" aria-hidden="true" tabindex="-1"></a>n_features <span class="op">=</span> <span class="dv">9</span></span>
<span id="cb501-87"><a href="#cb501-87" aria-hidden="true" tabindex="-1"></a>n_outputs <span class="op">=</span> <span class="dv">6</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-56" class="cell" data-trusted="true">
<div class="sourceCode cell-code" id="cb502"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb502-1"><a href="#cb502-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> TCNWithAttention(n_length, n_features, n_outputs).to(device)</span>
<span id="cb502-2"><a href="#cb502-2" aria-hidden="true" tabindex="-1"></a>summary(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-57" class="cell" data-trusted="true">
<div class="sourceCode cell-code" id="cb503"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb503-1"><a href="#cb503-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb503-2"><a href="#cb503-2" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max<span class="op">=</span><span class="dv">50</span>, eta_min<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb503-3"><a href="#cb503-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb503-4"><a href="#cb503-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb503-5"><a href="#cb503-5" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> <span class="kw">lambda</span> inputs, targets: <span class="fl">0.9</span><span class="op">*</span>kl_divergence_loss(inputs, targets) <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>nn.CrossEntropyLoss()(inputs.<span class="bu">float</span>(), targets.<span class="bu">float</span>())</span>
<span id="cb503-6"><a href="#cb503-6" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> train_model(model, train_loader, test_loader, optimizer<span class="op">=</span>optimizer, criterion<span class="op">=</span> loss_fn, epochs<span class="op">=</span><span class="dv">150</span>, device<span class="op">=</span>device, early_stopping<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-58" class="cell" data-trusted="true">
<div class="sourceCode cell-code" id="cb504"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb504-1"><a href="#cb504-1" aria-hidden="true" tabindex="-1"></a>plot_history(history)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-59" class="cell" data-trusted="true">
<div class="sourceCode cell-code" id="cb505"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb505-1"><a href="#cb505-1" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> TCNWithAttention(n_length, n_features, n_outputs).to(device)</span>
<span id="cb505-2"><a href="#cb505-2" aria-hidden="true" tabindex="-1"></a>best_model.load_state_dict(torch.load(<span class="st">'best_model.pth'</span>, weights_only <span class="op">=</span> <span class="va">True</span>))</span>
<span id="cb505-3"><a href="#cb505-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb505-4"><a href="#cb505-4" aria-hidden="true" tabindex="-1"></a>val_correct <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb505-5"><a href="#cb505-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb505-6"><a href="#cb505-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="bu">input</span>, target <span class="kw">in</span> test_loader:</span>
<span id="cb505-7"><a href="#cb505-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span> <span class="op">=</span> <span class="bu">input</span>.to(device)</span>
<span id="cb505-8"><a href="#cb505-8" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> target.to(device)</span>
<span id="cb505-9"><a href="#cb505-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb505-10"><a href="#cb505-10" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> best_model(<span class="bu">input</span>)</span>
<span id="cb505-11"><a href="#cb505-11" aria-hidden="true" tabindex="-1"></a>        val_correct <span class="op">+=</span> (outputs.argmax(<span class="dv">1</span>) <span class="op">==</span> target.argmax(<span class="dv">1</span>)).<span class="bu">sum</span>().item()</span>
<span id="cb505-12"><a href="#cb505-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"val_acc: </span><span class="sc">{</span>val_correct <span class="op">/</span> <span class="bu">len</span>(test_loader.dataset)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-60" class="cell" data-trusted="true">
<div class="sourceCode cell-code" id="cb506"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb506-1"><a href="#cb506-1" aria-hidden="true" tabindex="-1"></a>plot_confusion_matrix(best_model, test_loader, class_names<span class="op">=</span>[<span class="st">"WALKING"</span>, <span class="st">"WALKING_UPSTAIRS"</span>, <span class="st">"WALKING_DOWNSTAIRS"</span>, <span class="st">"SITTING"</span>, <span class="st">"STANDING"</span>, <span class="st">"LAYING"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="ensemble-of-models" class="level3">
<h3 class="anchored" data-anchor-id="ensemble-of-models">Ensemble of Models</h3>
<div id="cell-62" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:08:28.419342Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:08:28.418990Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:08:28.425473Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:08:28.424572Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:08:28.419312Z&quot;}" data-trusted="true" data-execution_count="62">
<div class="sourceCode cell-code" id="cb507"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb507-1"><a href="#cb507-1" aria-hidden="true" tabindex="-1"></a>best_model, model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>(AdaptiveRNN(
   (lstm): LSTM(9, 256, batch_first=True, bidirectional=True)
   (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
   (batch_norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
   (attention_dense): Linear(in_features=512, out_features=1, bias=True)
   (fc1): Linear(in_features=512, out_features=128, bias=True)
   (fc2): Linear(in_features=128, out_features=64, bias=True)
   (fc3): Linear(in_features=64, out_features=6, bias=True)
   (dropout): Dropout(p=0.5, inplace=False)
 ),
 TCNWithCBAM(
   (tcn_blocks): Sequential(
     (0): TCNBlock(
       (conv1): Conv1d(9, 64, kernel_size=(3,), stride=(1,), padding=same)
       (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
       (shortcut): Conv1d(9, 64, kernel_size=(1,), stride=(1,))
     )
     (1): TCNBlock(
       (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))
       (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
       (shortcut): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
     )
     (2): TCNBlock(
       (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=same, dilation=(4,))
       (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
       (shortcut): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
     )
     (3): TCNBlock(
       (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same, dilation=(8,))
       (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
     )
   )
   (cbam_block): CBAMBlock(
     (channel_attention): ChannelAttention(
       (avg_pool): AdaptiveAvgPool1d(output_size=1)
       (max_pool): AdaptiveMaxPool1d(output_size=1)
       (fc1): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
       (fc2): Conv1d(32, 256, kernel_size=(1,), stride=(1,))
     )
     (spatial_attention): SpatialAttention(
       (conv): Conv1d(2, 1, kernel_size=(7,), stride=(1,), padding=(3,))
       (sigmoid): Sigmoid()
     )
   )
   (global_avg_pool): AdaptiveAvgPool1d(output_size=1)
   (fc1): Linear(in_features=256, out_features=128, bias=True)
   (fc2): Linear(in_features=128, out_features=64, bias=True)
   (fc3): Linear(in_features=64, out_features=6, bias=True)
   (dropout): Dropout(p=0.5, inplace=False)
 ))</code></pre>
</div>
</div>
<div id="cell-63" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:11:50.058008Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:11:50.057135Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:11:50.064396Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:11:50.063409Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:11:50.057974Z&quot;}" data-trusted="true" data-execution_count="70">
<div class="sourceCode cell-code" id="cb509"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb509-1"><a href="#cb509-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_ensemble_accuracy(models, test_loader, num_classes, device<span class="op">=</span><span class="st">'cuda'</span>):</span>
<span id="cb509-2"><a href="#cb509-2" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb509-3"><a href="#cb509-3" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb509-4"><a href="#cb509-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb509-5"><a href="#cb509-5" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> [model.to(device).<span class="bu">eval</span>() <span class="cf">for</span> model <span class="kw">in</span> models]  <span class="co"># Ensure all models are in eval mode</span></span>
<span id="cb509-6"><a href="#cb509-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb509-7"><a href="#cb509-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb509-8"><a href="#cb509-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> inputs, targets <span class="kw">in</span> test_loader:</span>
<span id="cb509-9"><a href="#cb509-9" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Move data to the appropriate device</span></span>
<span id="cb509-10"><a href="#cb509-10" aria-hidden="true" tabindex="-1"></a>            inputs, targets <span class="op">=</span> inputs.to(device), targets.to(device)</span>
<span id="cb509-11"><a href="#cb509-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb509-12"><a href="#cb509-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get ensemble predictions</span></span>
<span id="cb509-13"><a href="#cb509-13" aria-hidden="true" tabindex="-1"></a>            avg_probs <span class="op">=</span> torch.zeros((inputs.size(<span class="dv">0</span>), num_classes)).to(device)</span>
<span id="cb509-14"><a href="#cb509-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> model <span class="kw">in</span> models:</span>
<span id="cb509-15"><a href="#cb509-15" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> model(inputs)</span>
<span id="cb509-16"><a href="#cb509-16" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb509-17"><a href="#cb509-17" aria-hidden="true" tabindex="-1"></a>                avg_probs <span class="op">+=</span> outputs</span>
<span id="cb509-18"><a href="#cb509-18" aria-hidden="true" tabindex="-1"></a>            avg_probs <span class="op">/=</span> <span class="bu">len</span>(models)</span>
<span id="cb509-19"><a href="#cb509-19" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> avg_probs.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb509-20"><a href="#cb509-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb509-21"><a href="#cb509-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update accuracy</span></span>
<span id="cb509-22"><a href="#cb509-22" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (preds <span class="op">==</span> targets.argmax(<span class="dv">1</span>)).<span class="bu">sum</span>().item()</span>
<span id="cb509-23"><a href="#cb509-23" aria-hidden="true" tabindex="-1"></a>            total <span class="op">+=</span> targets.size(<span class="dv">0</span>)</span>
<span id="cb509-24"><a href="#cb509-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb509-25"><a href="#cb509-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate overall accuracy</span></span>
<span id="cb509-26"><a href="#cb509-26" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> correct <span class="op">/</span> total</span>
<span id="cb509-27"><a href="#cb509-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Ensemble Test Accuracy: </span><span class="sc">{</span>accuracy <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb509-28"><a href="#cb509-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> accuracy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-64" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:12:21.025217Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:12:21.024882Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:12:21.395343Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:12:21.394533Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:12:21.025191Z&quot;}" data-trusted="true" data-execution_count="72">
<div class="sourceCode cell-code" id="cb510"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb510-1"><a href="#cb510-1" aria-hidden="true" tabindex="-1"></a>calculate_ensemble_accuracy([best_model], test_loader, num_classes<span class="op">=</span><span class="dv">6</span>, device<span class="op">=</span>device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ensemble Test Accuracy: 96.06%</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="72">
<pre><code>0.9606379368849678</code></pre>
</div>
</div>
<div id="cell-65" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T06:12:30.456313Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T06:12:30.455467Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T06:12:30.723633Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T06:12:30.722697Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T06:12:30.456279Z&quot;}" data-trusted="true" data-execution_count="73">
<div class="sourceCode cell-code" id="cb513"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb513-1"><a href="#cb513-1" aria-hidden="true" tabindex="-1"></a>calculate_ensemble_accuracy([model], test_loader, num_classes<span class="op">=</span><span class="dv">6</span>, device<span class="op">=</span>device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ensemble Test Accuracy: 94.77%</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>0.9477434679334917</code></pre>
</div>
</div>
</section>
</section>
<section id="log" class="level2">
<h2 class="anchored" data-anchor-id="log">Log</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
</colgroup>
<tbody>
<tr class="odd">
<td>RNN (180k) + Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9606; Val Acc: 0.9192<br>
TCN (56k) + KL Div + Adam + StepLR -&gt; Train Acc: 0.9698; Val Acc: 0.9355<br>
TCN (56k) + KL Div + Adam + ConsineAnnealineLR -&gt; Train Acc: 0.9921; Val Acc: 0.9382<br>
TCN (250k) + KL Div + Adam + ConsineAnnealingLR -&gt; Train Acc: 0.9871; Val Acc: 0.9484<br>
TCN (50k) + CBAM + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9891; Val Acc: 0.9450<br>
TCN (400k) + CBAM + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9970; Val Acc: 0.9545<br>
</td>
</tr>
<tr class="even">
<td>All below this use Early Stopping + Retraining</td>
</tr>
</tbody>
</table>
<p>Normalized + TCN (400k) + CBAM + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9931; Val Acc: 0.9498<br>
Normalized + Noise + TCN (400k) + CBAM + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9962; Val Acc: 0.9511<br>
Normalized + Noise + TCN (284k) + Self Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9977; Val Acc: 0.9427<br>
Normalized + Noise + TCN (539k) + Self Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9944; Val Acc: 0.9569<br>
Normalized + Noise + TCN (Dropouts(p=0.5) + BatchNorm) (539k) + Self Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9932; Val Acc: 0.9623<br>
Normalized + Noise + TCN (Dropouts(p=0.7) + BatchNorm) (539k) + Self Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9950; Val Acc: 0.9545<br>
Model model with loss func <code>0.8*kl_div + 0.2*label_smoothing(0.1)</code> -&gt; Train Acc: 0.9875; Val Acc: 0.9559<br>
Normalized + Noise + RNN (180k) + Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9929; Val Acc: 0.9545<br>
Normalized + Noise + RNN (622k) + Attention + KL Div + Adam + CosineAnnealingLR -&gt; Train Acc: 0.9963; Val Acc: 0.9746</p>
<div id="cell-67" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:43:48.108472Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:43:48.107689Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:43:48.114679Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:43:48.113784Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:43:48.108434Z&quot;}" data-trusted="true" data-execution_count="44">
<div class="sourceCode cell-code" id="cb516"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb516-1"><a href="#cb516-1" aria-hidden="true" tabindex="-1"></a>torch.cuda.memory_summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>'|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |  22754 KiB |    783 MiB |  12921 GiB |  12921 GiB |\n|       from large pool |  17792 KiB |    778 MiB |  12853 GiB |  12853 GiB |\n|       from small pool |   4962 KiB |      8 MiB |     68 GiB |     68 GiB |\n|---------------------------------------------------------------------------|\n| Active memory         |  22754 KiB |    783 MiB |  12921 GiB |  12921 GiB |\n|       from large pool |  17792 KiB |    778 MiB |  12853 GiB |  12853 GiB |\n|       from small pool |   4962 KiB |      8 MiB |     68 GiB |     68 GiB |\n|---------------------------------------------------------------------------|\n| Requested memory      |  22743 KiB |    782 MiB |  12907 GiB |  12907 GiB |\n|       from large pool |  17792 KiB |    776 MiB |  12839 GiB |  12839 GiB |\n|       from small pool |   4951 KiB |      8 MiB |     68 GiB |     68 GiB |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |   1224 MiB |   1224 MiB |   1224 MiB |      0 B   |\n|       from large pool |   1214 MiB |   1214 MiB |   1214 MiB |      0 B   |\n|       from small pool |     10 MiB |     10 MiB |     10 MiB |      0 B   |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |   5918 KiB | 552547 KiB |   5176 GiB |   5176 GiB |\n|       from large pool |   2688 KiB | 548479 KiB |   5103 GiB |   5103 GiB |\n|       from small pool |   3230 KiB |   5388 KiB |     73 GiB |     73 GiB |\n|---------------------------------------------------------------------------|\n| Allocations           |     112    |     136    |     817 K  |     816 K  |\n|       from large pool |       3    |      11    |     128 K  |     128 K  |\n|       from small pool |     109    |     133    |     688 K  |     688 K  |\n|---------------------------------------------------------------------------|\n| Active allocs         |     112    |     136    |     817 K  |     816 K  |\n|       from large pool |       3    |      11    |     128 K  |     128 K  |\n|       from small pool |     109    |     133    |     688 K  |     688 K  |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |      10    |      10    |      10    |       0    |\n|       from large pool |       5    |       5    |       5    |       0    |\n|       from small pool |       5    |       5    |       5    |       0    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |      21    |      25    |  360638    |  360617    |\n|       from large pool |       2    |       8    |   74884    |   74882    |\n|       from small pool |      19    |      21    |  285754    |  285735    |\n|---------------------------------------------------------------------------|\n| Oversize allocations  |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\n|===========================================================================|\n'</code></pre>
</div>
</div>
<div id="cell-68" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-12-13T05:43:49.005170Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-12-13T05:43:49.004848Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-12-13T05:43:49.024093Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-12-13T05:43:49.023207Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-12-13T05:43:49.005140Z&quot;}" data-trusted="true" data-execution_count="45">
<div class="sourceCode cell-code" id="cb518"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb518-1"><a href="#cb518-1" aria-hidden="true" tabindex="-1"></a>torch.cuda.empty_cache()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/RISHIT7\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>